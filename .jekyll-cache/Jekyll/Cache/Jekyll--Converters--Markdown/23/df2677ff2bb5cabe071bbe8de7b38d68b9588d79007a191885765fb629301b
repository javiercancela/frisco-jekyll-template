I"óñ<p>Empezamos con los algoritmos. Vamos a implementar un perceptr√≥n en Python, y entrenarlo para que sepa clasificar las diferentes especies de flores en Iris. El libro comienza mostrando el <a href="https://es.wikipedia.org/wiki/Neurona_de_McCulloch-Pitts">modelo de neurona de McCulloch-Pitts</a>:</p>

<div style="text-align:center">
    <figure>
        <img alt="En este modelo se emite una se√±al de salida si el resultado de aplicar una funci√≥n a la suma de las entradas supera cierto umbral" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_01.png" />
        <figcaption>En este modelo se emite una se√±al de salida si el resultado de aplicar una funci√≥n a la suma de las entradas supera cierto umbral</figcaption>
    </figure>
</div>

<p>Tomando este modelo como base, Frank Rosenblatt dise√±√≥ un dispositivo electr√≥nico con capacidad para aprender. El algoritmo usado por ese dispositivo se conoce como Perceptr√≥n.</p>

<h2 id="perceptr√≥n">Perceptr√≥n</h2>

<p>El <a href="https://es.wikipedia.org/wiki/Perceptr%C3%B3n">perceptr√≥n</a> es un algoritmo de aprendizaje supervisado para realizar tareas de clasificaci√≥n binaria, es decir, para determinar si una muestra pertenece o no a una clase.</p>

<div style="background-color: #EEEEEE; padding: 1em">
El libro asume cierta familiaridad trabajando con vectores y matrices. Para refrescar conceptos recomiendo revisar los <a href="https://es.wikipedia.org/wiki/%C3%81lgebra_lineal#Enlaces_externos">enlaces externos referenciados en la p√°gina de la Wikipedia dedicada al √Ålgebra Lineal</a>. Tambi√©n tiene buena pinta [el curso de la Khan Academy](https://es.khanacademy.org/math/linear-algebra). <br />
En ingl√©s hay varios libros gratuitos disponibles, como [este](https://www.math.ucdavis.edu/~linear/linear-guest.pdf) y [este](http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf).
</div>

<div style="text-align:center">
    <figure>
        <a href="https://commons.wikimedia.org/wiki/File:Perceptr%C3%B3n_5_unidades.svg#/media/File:Perceptr%C3%B3n_5_unidades.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/1200px-Perceptr%C3%B3n_5_unidades.svg.png" alt="Perceptr√≥n 5 unidades.svg" /></a>
        <figcaption>
            De <a href="//commons.wikimedia.org/w/index.php?title=User:Alejandro.cartas&amp;action=edit&amp;redlink=1" class="new" title="User:Alejandro.cartas (page does not exist)">Alejandro Cartas</a> - <span class="int-own-work" lang="es">Trabajo propio</span>, <a href="http://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=41534843">Enlace</a>
        </figcaption>
    </figure>
</div>

<p>Como podemos ver en la figura anterior, la se√±al de entrada est√° compuesta por un array \(\bf x\) al que se aplica una serie de pesos definidos por un array \(\bf w\). Es decir, cada elemento \(x_i\) se multiplica por \(w_i\), y se suma el resultado de cada uno de estos productos. De una manera m√°s formal, definimos</p>

\[x = \begin{bmatrix}
x_1 \cr
\vdots \cr
x_m \cr
\end{bmatrix}, w = \begin{bmatrix}
w_1 \cr
\vdots \cr
w_m \cr
\end{bmatrix}
\tag{2.1}\]

<p>Y la entrada resultante \(z\) ser√° la suma de cada entrada ponderada:</p>

\[z = \sum{w_ix_i} = w_1x_1 + \cdots + w_mx_m
\tag{2.2}\]

<p>Ya tenemos la entrada \(z\) a la funci√≥n de activaci√≥n \(\phi(z)\). Para nuestro caso vamos a usar como funci√≥n de activaci√≥n la <a href="https://es.wikipedia.org/wiki/Funci%C3%B3n_escal%C3%B3n_de_Heaviside">funci√≥n escal√≥n</a>. Esta funci√≥n se define de la siguiente forma:</p>

\[\phi(z) = \begin{cases}
\text{1 si } z \ge \theta \cr
\text{-1 en otro caso}
\end{cases}
\tag{2.3}\]

<p>Ese valor \(\theta\) es el umbral que tiene que superar \(z\) para disparar la se√±al. Para simplificar los c√°lculos, definimos un peso 0 con \(w_0 = -\theta\) y \(x_0 = 1\), de esta forma podemos escribir:</p>

\[z = w_0x_0 + w_1x_1 + \cdots + w_mx_m
\tag{2.4}
\label{2.4}\]

<p>y</p>

\[\phi(z) = \begin{cases}
\text{1 si } z \ge 0 \cr
\text{-1 en otro caso}
\end{cases}
\tag{2.5}
\label{2.5}\]

<p>El valor de \(z\) se puede escribir tambi√©n como \(z = \mathbf{w}^T\bf x\). El super√≠ndice T indica que la matriz (en este caso un array, que tratamos como una matriz de 1 columna y n filas) est√° traspuesta, o lo que es lo mismo, se han convertidos sus columnas en filas. Por lo tanto:</p>

\[\mathbf{w}^T\bf x = \begin{bmatrix} w_0 &amp; w_1 &amp; \cdots &amp; w_m \end{bmatrix} \begin{bmatrix}
x_1 \cr
\vdots \cr
x_m \cr
\end{bmatrix}

\tag{2.6}\]

<p>Esto nos permite usar la operaci√≥n de producto escalar entre matrices. En una operaci√≥n entre matrices, si la primera tiene dimensiones \(n \times m\), la segunda debe tener \(m \times p\), dando como resultado una matriz \(n \times p\). En nuestro caso, la dimensiones ser√≠an \(1 \times n\) y \(n \times 1\), dando como resultado una matriz \(1 \times 1\), es decir, un escalar. Por ello esta operaci√≥n es realmente un producto escalar entre vectores, cuyo resultado es la ya conocida expresi√≥n \eqref{2.4}</p>

<h2 id="entrenando-el-modelo">Entrenando el modelo</h2>

<p>Pretendemos modificar los valores de los pesos (\(\mathbf{w}\)) de forma que las salidas del preceptr√≥n se vayan acercando a los valores esperados. Para ello se inicializan los pesos a cero, o a un valor muy peque√±o, se obtiene la salida, y se actualizan los pesos.</p>

<p>¬øC√≥mo se actualizan los pesos? Sum√°ndoles una cantidad (positiva o negativa) que depende de la entrada, de la diferencia entre la salida real y la salida esperada, y de la tasa de aprendizaje (\(\eta\)), que es una constante que vale entre 0 y 1:</p>

\[\Delta w_j = \eta(y^{(i)} - \hat y^{(i)})x_j^{(i)}
\tag{2.7}\]

<p>En esta f√≥rmula \(y^{(i)}\) es la clase a la que pertenece la muestra (i), \(\hat y^{(i)}\) es la clase predicha por el perceptr√≥n, y \(x_j^{(i)}\) es la entrada de la caracter√≠stica j para la muestra (i). Todo esto se multiplica para calcular la variaci√≥n en la caracter√≠stica j: \(\Delta w_j\). Con cada muestra se recalculan los pesos:</p>

\[w_j := w_j + \Delta w_j
\tag{2.8}\]

<h3 id="condiciones-de-convergencia">Condiciones de convergencia</h3>

<p>Este proceso iterativo puede converger o no. Para que el perceptr√≥n clasifique correctamente las muestras , el valor de \(w_j\) debe ir acerc√°ndose cada vez m√°s a un valor final, o lo que es lo mismo, \(\Delta w_j\) debe tender a cero.</p>

<p>Para que se de esta situaci√≥n deben cumplirse dos requisitos. En primer lugar, la tasa de aprendizaje debe ser suficientemente peque√±a. En general, cuanto m√°s grande sea \(\eta\) m√°s r√°pido ser√° el aprendizaje, pero pasado cierto valor el proceso de aprendizaje nunca converger√°. En segundo lugar, las dos clases deben ser linealmente separables. Es decir, si representamos los elementos de ambas clases en un diagrama de 2 dimensiones (suponeniendo que las muestras tienen dos caracter√≠sticas), debe existir una l√≠nea que separe a todos los de una clase a un lado, y a todos los de la otra clase al otro lado.</p>

<div style="text-align:center">
    <figure>
        <img alt="Ejemplos con muestras de dos clases. S√≥lo el primero es linearmente separable." src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_03.png" />
        <figcaption>Ejemplos con muestras de dos clases. S√≥lo el primero es linearmente separable.</figcaption>
    </figure>
</div>

<h2 id="perceptr√≥n-en-python">Perceptr√≥n en Python</h2>

<p>El c√≥digo del cap√≠tulo 2 (y de los dem√°s cap√≠tulos) est√°, como comentamos, <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb">subido a GitHub</a> en formato <a href="http://jupyter.org/">Jupyter</a>. Jupyter (originalmente iPython) es una aplicaci√≥n web que permite crear y compartir documentos que contengan c√≥digo, gr√°ficas, animaciones‚Ä¶ de un forma muy sencilla. Jupyter viene incluido en Anaconda, que es el entorno Python que recomendamos en <a href="/2017/03/12/python-machine-learning-i/">una entrada anterior</a>. Tambi√©n se puede ver el c√≥digo completo de cada cap√≠tulo como archivo Python <a href="https://github.com/rasbt/python-machine-learning-book/tree/master/code/optional-py-scripts">aqu√≠</a>.</p>

<p>El c√≥digo comienza declarando una clase <code class="language-plaintext highlighter-rouge">Perceptron</code> en la que se definen en constructor y tres m√©todos:</p>
<pre class="line-numbers">
  <code class="language-python">
    import numpy as np

    class Perceptron(object):
      def __init__(self, eta=0.01, n_iter=10):
        self.eta = eta
        self.n_iter = n_iter

      def fit(self, X, y):
        self.w_ = np.zeros(1 + X.shape[1])
        self.errors_ = []

        for _ in range(self.n_iter):
          errors = 0
          for xi, target in zip(X, y):
            update = self.eta * (target - self.predict(xi))
            self.w_[1:] += update * xi
            self.w_[0] += update
            errors += int(update != 0.0)
          self.errors_.append(errors)
        return self

      def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

      def predict(self, X):
        return np.where(self.net_input(X) &gt;= 0.0, 1, -1)
  </code>
</pre>

<p>Inicializamos el perceptr√≥n con dos par√°metros: <code class="language-plaintext highlighter-rouge">eta</code>, que es la tasa de aprendizaje, y <code class="language-plaintext highlighter-rouge">n_iter</code>, que es el n√∫mero de veces que vamos a recorrer el conjunto de datos de entrenamiento. A cada una de estas veces se les llama ‚Äú√©pocas‚Äù.</p>

<p>Una vez instanciado el perceptr√≥n llamaremos al m√©todo <code class="language-plaintext highlighter-rouge">fit</code> para entrenar nuestro modelo. Se le pasan dos par√°metros: una matriz \(\bf X\), que contiene una fila por cada muestra y una columna por cada caracter√≠stica, y que constituye nuestro conjunto de datos de entrenamiento; y un array \(\bf y\), que contiene el resultado objetivo para cada muestra.</p>

<p>Lo primero que hace el m√©todo <code class="language-plaintext highlighter-rouge">fit</code> es inicializar los pesos y los errores:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="bp">self</span><span class="p">.</span><span class="n">w_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="bp">self</span><span class="p">.</span><span class="n">errors_</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>Los pesos se inicializan a un array con el mismo n√∫mero de elementos que muestras tiene la matriz \(\bf X\) m√°s uno. Ese m√°s uno es debido a que vamos a almacenar tambi√©n el par√°metro \(w_0\) corresondiente al umbral \(\theta\) que pasamos a la izquiera en la equaci√≥n \eqref{2.4}. Como estamos inicializando todo a cero consideramos el umbral inicial de activaci√≥n como cero.</p>

<p>A continuaci√≥n comienza un precose que se ejecutar√° <code class="language-plaintext highlighter-rouge">n_iter</code> veces. En el se itera sobre los datos proporcionados mediante esta instrucci√≥n:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Lo que va a hacer la instrucci√≥n <code class="language-plaintext highlighter-rouge">zip</code> es devolver un elemento por cada lista que se le pase de par√°metro. As√≠, si \(\bf X\) es una matriz de 100 filas (muestras) y 2 columnas (caracter√≠sticas), e \(\bf y\) es un array de 100 elementos (las clases reales de cada muestra), la instrucci√≥n anterior iterar√° para cada muestra almacenando en <code class="language-plaintext highlighter-rouge">xi</code> las caracter√≠sticas y en <code class="language-plaintext highlighter-rouge">target</code> su clase real.</p>

<p>Con esos datos vamos a calcular los nuevos pesos. La f√≥rmula era la siguiente:</p>

\[\Delta w_j = \eta(y^{(i)} - \hat y^{(i)})x_j^{(i)}
w_j := w_j + \Delta w_j\]

<p>En nuestro caso hacemos esto:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xi</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="n">update</span> <span class="o">*</span> <span class="n">xi</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">update</span>
    <span class="n">errors</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">update</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></div>

<p>Calculamos primero el factor \(\eta(y^{(i)} - \hat y^{(i)})\), y despu√©s lo multiplicamos por \(\bf{x^{(i)}}\) para sum√°rselo al array de pesos, salvo el peso 0. Al ser tanto ‚Äòself.w_[1:]‚Äô como ‚Äòxi‚Äô arrays con un elemento por cada caracter√≠stica, estamos aplicando la correcci√≥n a todos los pesos. En el peso \(w_0\) no multiplicamos por \(\bf{x_0^{(i)}}\), ya que este por definici√≥n vale uno. Finalmente acumulamos el error de cada muestra, y al final de cada √©poca lo a√±adimos a un array. Esto nos permitir√° ver la evoluci√≥n del error por √©pocas.</p>

<p>Veamos ahora c√≥mo calcular el t√©rmino \(\hat y^{(i)}\), que en c√≥digo se obtiene llamando a <code class="language-plaintext highlighter-rouge">self.predict(xi)</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">net_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xi</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xi</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">net_input</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Lo primero que hace <code class="language-plaintext highlighter-rouge">predict</code> con <code class="language-plaintext highlighter-rouge">xi</code> es usarlo para invocar <code class="language-plaintext highlighter-rouge">net_input</code>. En esta funci√≥n usamos <code class="language-plaintext highlighter-rouge">np.dot</code> para realizar un producto escalar entre <code class="language-plaintext highlighter-rouge">xi</code> y <code class="language-plaintext highlighter-rouge">self.w_[1:]</code>. Es decir, suponiendo una muestra con dos caracter√≠sticas, y por tanto dos entradas y dos pesos, <code class="language-plaintext highlighter-rouge">np.dot(xi, self.w_[1:])</code> equivale a \(w_1x_1 + w_2x_2\). A esto le sumamos \(w_0\) (ya que \(x_0\) es igual a uno), y tenemnos que <code class="language-plaintext highlighter-rouge">net_input</code> devuelve el valor \(z\), tal como se define en \eqref{2.4}. La √∫ltima parte, <code class="language-plaintext highlighter-rouge">np.where(self.net_input(xi) &gt;= 0.0, 1, -1)</code> devuelve 1 si \(z\) es mayor o igual que 0, y -1 en otro caso. En resumen, el m√©todo <code class="language-plaintext highlighter-rouge">predict</code> es la funci√≥n \(\phi(z)\) (\eqref{2.5}).</p>

<h3 id="perceptr√≥n-con-el-conjunto-de-datos-iris">Perceptr√≥n con el conjunto de datos Iris</h3>

<p>Toda la l√≥gica del perceptr√≥n est√° implementada en la clase anterior. Vamos a utilizarla con el conjunto de datos Iris ya mencionado anteriormente. El ejemplo del libro utilizan la librer√≠a <code class="language-plaintext highlighter-rouge">pandas</code>, especializada en an√°lisis de datos y estructuras, para cargar los datos de Iris. Estos datos se almacenan en un tipo de datos de la librer√≠a llamado <a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe">DataFrame</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://archive.ics.uci.edu/ml/'</span>
                 <span class="s">'machine-learning-databases/iris/iris.data'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">tail</span><span class="p">())</span>                
</code></pre></div></div>

<p>El archivo de datos lo estamos descargando de internet, pero tambi√©n est√° disponible en el c√≥digo de ejemplo para usar en local.</p>

<p>El c√≥digo de ejemplo imprime las √∫ltimas l√≠neas de la estructura de datos cargada:</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>145</strong></td>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <td><strong>146</strong></td>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <td><strong>147</strong></td>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <td><strong>148</strong></td>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <td><strong>149</strong></td>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Iris-virginica</td>
    </tr>
  </tbody>
</table>

<p>A continuaci√≥n preprocesamos los datos, para dejarlos en el formato adecuado para trabajar con ellos. Usaremos las librer√≠as <code class="language-plaintext highlighter-rouge">numpy</code>, para tratar los datos, y <code class="language-plaintext highlighter-rouge">matplotlib</code>, para dibujarlos en una gr√°fica</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">4</span><span class="p">].</span><span class="n">values</span>
</code></pre></div></div>

<p>En primer lugar hemos seleccionado los 100 primeros registros del DataFrame, pero cogiendo s√≥lo la quinta columna (la 4 por comenzar el √≠ndice en 0). Esto nos devuelve un array con 100 valores, cada uno de los cuales es el nombre de la especie. Es decir, cogemos las etiquetas de la clase de cada muestra.</p>

<p>En el conjunto de datos Iris los datos est√°n ordenados de tal forma que los 50 primeros son Iris-setosa, y los 50 siguientes Iris-versicolor. De esta forma limitarnos a los 100 primeros registros nos permite realizar una clasificaci√≥n binaria.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="s">'Iris-setosa'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>El siguiente paso consiste en sustituir las etiquetas de texto por valores num√©ricos. El valor objetivo pasar√° a ser -1 para las Iris-setosa y 1 para las Iris-versicolor. Almacenamos los datos en un array \(\bf y\) de 100 elementos.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]].</span><span class="n">values</span>
</code></pre></div></div>

<p>De las cuatro caracter√≠sticas posibles nos vamos a quedar con dos: la longitud del s√©palo y la longitud del p√©talo. Esto nos permitir√° dibujar los datos en una gr√°fica de dos dimensiones. Al igual que hicimos con el valor objetivo, cogemos los cien primeros valores del conjunto de datos, qued√°ndonos con la primera y la tercera columna. Esto nos devuelve una matriz \(\bf X \in \mathbb R^{100x2}\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'setosa'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'versicolor'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'sepal length [cm]'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'petal length [cm]'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Dibujamos los datos almacenados. Marcamos con una ‚Äòo‚Äô roja las Iris-setosa (las 50 primeras), y con una ‚Äòx‚Äô azul las Iris-versicolor. El resultado es este:</p>

<div style="text-align:center">
    <figure>
        <img alt="Datos de entrenamiento" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_06.png" />
        <figcaption>Datos de entrenamiento</figcaption>
    </figure>
</div>

<p>Con los datos listos podemos realizar el entrenamiento.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppn</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ppn</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Instanciamos el perceptr√≥n con un \(\eta = 0.1\) y 10 √©pocas. Para entrenar  basta con llamar al m√©todo <code class="language-plaintext highlighter-rouge">fit</code> pasando \(\bf X\)  e \(\bf y\) como par√°metros.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ppn</span><span class="p">.</span><span class="n">errors_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ppn</span><span class="p">.</span><span class="n">errors_</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Number of updates'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1"># plt.savefig('./perceptron_1.png', dpi=300)
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Una vez entrenado el modelo, pintamos en un gr√°fico los errores encontrados en cada generaci√≥n. En  el eje X pintamos las √©pocas (la longitud del array de errores), y en el Y mostramos el n√∫mero de errores.</p>

<div style="text-align:center">
    <figure>
        <img alt="Errores por √©poca" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_07.png" />
        <figcaption>Errores por √©poca</figcaption>
    </figure>
</div>

<p>A partir de la 6¬™ √©poca el modelo ya no comete errores. Ahora vamos a dibujar los datos de entrenamiento, pero marcando las zonas que el modelo ya entrenado considera como de una clase u otra.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>

<span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span>
    <span class="n">markers</span> <span class="o">=</span> <span class="p">(</span><span class="s">'s'</span><span class="p">,</span> <span class="s">'x'</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="s">'^'</span><span class="p">,</span> <span class="s">'v'</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">(</span><span class="s">'red'</span><span class="p">,</span> <span class="s">'blue'</span><span class="p">,</span> <span class="s">'lightgreen'</span><span class="p">,</span> <span class="s">'gray'</span><span class="p">,</span> <span class="s">'cyan'</span><span class="p">)</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>
</code></pre></div></div>

<p>Definimos una nueva funci√≥n que nos permita dibujar en distintos colores las zonas que ser√≠an de Iris-setosa e Iris-versicolor si una muestra tuviese unas caracter√≠sticas que estuviesen en ese punto.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span>
    <span class="c1"># ... Continuaci√≥n
</span>    <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span>
                           <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx1</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="p">.</span><span class="n">ravel</span><span class="p">()]).</span><span class="n">T</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx1</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">xx1</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">xx2</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">xx2</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>

    <span class="c1"># plot class samples
</span>    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">cl</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span>
                    <span class="n">marker</span><span class="o">=</span><span class="n">markers</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">cl</span><span class="p">)</span>
</code></pre></div></div>
<p>Definimos los valores m√≠nimo y m√°ximo del s√©palo (<code class="language-plaintext highlighter-rouge">x1_min</code> y <code class="language-plaintext highlighter-rouge">x1_max</code>) y del p√©talo (<code class="language-plaintext highlighter-rouge">x2_min</code> y <code class="language-plaintext highlighter-rouge">x2_max</code>), restando uno al m√≠nimo y sum√°ndolo al m√°ximo para tener un rango con margen. La funci√≥n <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html"><code class="language-plaintext highlighter-rouge">np.arange</code></a> devuelve una lista de valores que van del primer par√°metro al segundo en intervalos marcados por el tercer par√°metro. Las dos llamadas a <code class="language-plaintext highlighter-rouge">np.arange</code> devolver√°n un array de valores separados por 0.02. En el primer caso van del 3.30 al 7.98, en el segundo del 0 al 6.08.  Con ambos arrays construimos una matriz que pasamos al predictor, el cual nos devolver√° el valor predicho para todos los puntos del plano definido.</p>

<div style="text-align:center">
    <figure>
        <img alt="Regiones predichas para cada clase" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_08.png" />
        <figcaption>Regiones predichas para cada clase</figcaption>
    </figure>
</div>
:ET