<!DOCTYPE html>

<html lang="en">
	<head>
		<script type="text/javascript">
		    var host = "www.javiercancela.com";
		    if ((host == window.location.host) && (window.location.protocol != "https:"))
		        window.location.protocol = "https";
		</script>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="generator" content="Jekyll v4.2.0">

		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300|Rubik:300">
		<link rel="stylesheet" href="/css/screen.css">
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
		<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
		<link rel="manifest" href="/manifest.json">
		<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
		<meta name="theme-color" content="#ffffff">

		<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>PML T2 - ADALINE | Página de Javier Cancela</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="PML T2 - ADALINE" />
<meta name="author" content="Javier Cancela" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka" />
<meta property="og:description" content="Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka" />
<link rel="canonical" href="https://www.javiercancela.com/2017/04/02/python-machine-learning-iii/" />
<meta property="og:url" content="https://www.javiercancela.com/2017/04/02/python-machine-learning-iii/" />
<meta property="og:site_name" content="Página de Javier Cancela" />
<meta property="og:image" content="https://www.javiercancela.com/images/pml/pml.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-04-02T03:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://www.javiercancela.com/images/pml/pml.jpg" />
<meta property="twitter:title" content="PML T2 - ADALINE" />
<script type="application/ld+json">
{"dateModified":"2017-04-02T03:00:00+02:00","datePublished":"2017-04-02T03:00:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.javiercancela.com/2017/04/02/python-machine-learning-iii/"},"@type":"BlogPosting","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://www.javiercancela.com/siteicon.png"},"name":"Javier Cancela"},"url":"https://www.javiercancela.com/2017/04/02/python-machine-learning-iii/","author":{"@type":"Person","name":"Javier Cancela"},"image":"https://www.javiercancela.com/images/pml/pml.jpg","description":"Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka","headline":"PML T2 - ADALINE","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


		
			<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-58458863-1', 'auto');
  ga('send', 'pageview');

</script>
		
	</head>

	<body>
		<header id="mainHeader">
			<div class="container">
				<div class="company-name">
					<a href="/">
						<span class="dark-logo"><img width="104" height="38" src="/images/logo/dark.png" alt="dark frisco logo"></span>
						<span class="light-logo"><img width="104" height="38" src="/images/logo/light.png" alt="light frisco logo"></span>
					</a>
				</div>
				<nav>
	<a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
	
		
		

		
		<a href="/" class="" >Blog</a>
	
		
		

		
		<a href="/archivo/" class="" >Archivo</a>
	
		
		

		
		<a href="/sobre-mi/" class="" >Sobre mí</a>
	
</nav>

			</div>
		</header>

		<section class="hero" style="background-image: url(/images/pml/pml.jpg)">
	<div class="inner-hero text-container">
		<div class="hero-text-container">
			<h1>PML T2 - ADALINE</h1>
			<p class="subtext">Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka</p>
		</div>
	</div>
</section>

<section>
	<div class="blog-post text-container">
		<p class="post-details">
	
		
		<span class="blog-filter">
			<a href="/category/python/">Python</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/libros/">Libros</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/machine-learning/">Machine Learning</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/inteligencia-artificial/">Inteligencia Artificial</a>
		</span>
	
	
	
	
	
	
	
	<span class="post-date">02 de abril de 2017</span>
</p>


		<div class="post-content">
			<p>El perceptrón visto en el post anterior es una red neuronal de una sola capa. Otro ejemplo de este tipo de redes es la llamada <strong>neurona lineal adaptativa</strong>, en inglés <em>ADAptative LIneal NEuron</em>, o <a href="https://es.wikipedia.org/wiki/Adaline"><em>ADALINE</em></a>. Nos dice el libro que la importancia de este algoritmo se debe a que muestra con claridad la definición y minimización de las funciones de costos (<em>cost functions</em>).</p>

<h2 id="función-de-costos">Función de costos</h2>
<p><strong>Función de costos</strong> es un término que procede de la economía, y hace referencia a la función que expresa los costes de producción en términos de la cantidad producida. En el campo de las redes neuronales la función de costos devuelve un número que determina indica lo bien que el algoritmo genera las salidas correctas para las muestras de entrenamiento. Este número es mejor cuanto más bajo, por eso se busca minimizar el valor de la función. Esta minimización de la función de costos supone la base de muchas técnicas de clasificación.</p>

<p>La diferencia básica entre Adaline y el perceptrón está en la función de activación, que pasa de ser una función escalón a ser una función lineal.</p>

<div style="text-align:center">
    <figure>
        <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_09.png"><img alt="Actualizamos los pesos con una función lineal de la entrada" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_09.png" /></a>
        <figcaption>Actualizamos los pesos con una función lineal de la entrada</figcaption>
    </figure>
</div>

<p>En concreto, la función lineal de activación de Adaline es la función identidad \(\phi(z) = z\), con lo que \(\phi(\mathbf{w}^T\bf x) = \mathbf{w}^T\bf x\). Como vemos en la gráfica superior, se sigue usando un cuantizador para clasificar la muestra, aunque no se use ya para corregir los pesos.</p>

<h2 id="minimización-de-funciones-de-costos">Minimización de funciones de costos</h2>

<p>Igual que en el caso del perceptrón, la actualización de pesos se realiza calculando \(w_j := w_j + \eta(y^{(i)} - \hat y^{(i)})x_j^{(i)}\). Para el perceptrón tanto \(y^{(i)}\) como \(\hat y^{(i)}\) son etiquetas de clases con solo dos posibles valores. En el caso de Adaline se usan valores continuos.</p>

<p><a name="1"></a></p>

<p>La idea de la función de costos es definir una función que se pueda optimizar durante el proceso de aprendizaje. Para ello utilizamos la suma de los errores al cuadrado (<em>Sum of Squared Errors</em>, o <em>SSE</em>, también llamado en inglés <a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares"><em>Residual Sum of Squareds</em></a>):</p>

\[J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2,\]

<p>donde \(\frac{1}{2}\) se incluye para simplificar cálculos posteriores. Como buscamos el \(\mathbf{w}\) que minimiza el valor de esa función, multiplicarla por una constante no afecta al resultado. Como siempre, \(y^{(i)}\) es el resultado esperado para la muestra \(i\). \(\phi(z)_{A}^{(i)}\) es la salida de la función de activación (\(A\)) para la muestra \(i\).</p>

<p>Esta función es diferenciable y convexa, lo que nos permite calcular un mínimo. Como nuestra función de costos sólo depende de los pesos \(\mathbf{w}\), minimizarla nos dará el peso que tenga una menor diferencia entre la salida esperada y la obtenida (\(y^{(i)} - \phi(z)_{A}^{(i)}\)). Para calcular el mínimo se usa un algoritmo llamado <a href="http://alejandrosanchezyali.blogspot.co.uk/2016/01/algoritmo-del-gradiente-descendente-y.html">gradiente descendente</a>.</p>

<p>Nuestro objetivo, como siempre, es ajustar el peso \(\mathbf{w}\) en incrementos \(\Delta \mathbf{w}\):</p>

\[\mathbf{w} := \mathbf{w} + \Delta \mathbf{w}.\]

<p>Para calcular \(\Delta \mathbf{w}\) vamos a necesitar conocer el gradiente de nuestra función de costos: \(\nabla J(\mathbf{w})\). <a href="https://es.wikipedia.org/wiki/Gradiente">Gradiente</a> es un término matemático que hace referencia al vector que indica la dirección en la que un campo varía con más rapidez. Para una función de \(k\) variables el gradiente es el vector de \(k\) dimensiones que marca la dirección en la que la función se incrementa más rápidamente.</p>

<div style="text-align:center">
    <figure>
        <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_10.png"><img alt="Usamos el gradiente en cada punto para calcular los decrementos que nos lleven al mínimo" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_10.png" /></a>
        <figcaption>Usamos el gradiente en cada punto para calcular los decrementos que nos lleven al mínimo</figcaption>
    </figure>
</div>

<p>Nosotros vamos a usar el gradiente para buscar el mínimo de \(J(\mathbf{w})\), así que en vez de sumarlo lo restaremos (de ahí lo de gradiente descendente). Además usaremos nuestra tasa de aprendizaje para modular esta iteración:</p>

\[\Delta \mathbf{w} = - \eta \nabla J(\mathbf{w}).\]

<p>Dado que</p>

\[J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2,\]

<p>tendremos que cada uno de los pesos se actualizará con este incremento:</p>

\[\Delta w_{j} := \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x_{j}^{(i)}.\]

<p>Para una explicación del cálculo de este gradiente podemos leer <a href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/linear-gradient-derivative/">este artículo del propio Sebastian Raschka</a>.</p>

<h2 id="adaline-en-python">Adaline en Python</h2>

<p>Como en la entrada anterior, el código está <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb">sacado de GitHub</a>.</p>

<p>La implementación de Adaline es muy similar a la del Perceptrón. La única diferencia está en el método <code class="language-plaintext highlighter-rouge">fit</code>, donde los pesos se actualizan con el gradiente descendente</p>

<pre class="line-numbers">
  <code class="language-python">
	class AdalineGD(object):
	    def __init__(self, eta=0.01, n_iter=50):
	        self.eta = eta
	        self.n_iter = n_iter

	    def fit(self, X, y):
	        self.w_ = np.zeros(1 + X.shape[1])
	        self.cost_ = []

	        for i in range(self.n_iter):
	            output = self.activation(X)
	            errors = (y - output)
	            self.w_[1:] += self.eta * X.T.dot(errors)
	            self.w_[0] += self.eta * errors.sum()
	            cost = (errors**2).sum() / 2.0
	            self.cost_.append(cost)
	        return self

	    def net_input(self, X):
	        return np.dot(X, self.w_[1:]) + self.w_[0]

	    def activation(self, X):
	        return self.net_input(X)

	    def predict(self, X):
	        return np.where(self.activation(X) &gt;= 0.0, 1, -1)
  </code>
</pre>

<p>Recordemos cómo funciona el método <code class="language-plaintext highlighter-rouge">fit</code>. Se le pasan dos parámetros: una matriz \(\bf X\), que contiene una fila por cada muestra y una columna por cada característica, y que constituye nuestro conjunto de datos de entrenamiento; y un array \(\bf y\), que contiene el resultado objetivo para cada muestra.</p>

<p>Lo primero que hace el método <code class="language-plaintext highlighter-rouge">fit</code> es inicializar los pesos y los costos:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">w_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="bp">self</span><span class="p">.</span><span class="n">cost_</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>
<p>Después iteramos el proceso tantas veces como épocas se hayan definido. La entrada neta (la función <code class="language-plaintext highlighter-rouge">net_input</code>) es el producto escalar de los atributos de las muestras y sus pesos. Para la salida utilizamos una función <code class="language-plaintext highlighter-rouge">activation</code> que es igual a la entrada neta. La existencia de este método se justifica para hacer el código más general, y poder reutilizarlo con otros algoritmos. En nuestro caso, esta función es la función identidad, por lo que podríamos haber usado directamente <code class="language-plaintext highlighter-rouge">net_input</code>. El error será la difencia entre la salida esperada, <code class="language-plaintext highlighter-rouge">y</code>, y la real, <code class="language-plaintext highlighter-rouge">output</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">activation</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p>Si recordamos las entradas anteriores, la salida esperada <code class="language-plaintext highlighter-rouge">y</code> es un array con las clases reales de 100 muestras, las 50 primeras con valor -1 (Iris-setosa), y las 50 siguientes con valor 1 (Iris-versicolor). El valor de output es la salida de la función identidad, pero en este caso vamos a considerar todas las muestras a la vez, con lo que tendremos un array de 100 elementos en vez de un escalar. El método <code class="language-plaintext highlighter-rouge">net_input</code> calcula el producto escalar de la matriz de muestras <code class="language-plaintext highlighter-rouge">X</code> por el array de pesos <code class="language-plaintext highlighter-rouge">w_</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">net_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

</code></pre></div></div>

<h3 id="cálculo-de-errores">Cálculo de errores</h3>

<p>El resultado es un array de errores (<code class="language-plaintext highlighter-rouge">errors</code>) de 100 elementos, uno para cada muestra. Con ellos se actualizan los pesos:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">errors</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div></div>

<p>En la primera línea se aplica un incremento <code class="language-plaintext highlighter-rouge">self.eta * X.T.dot(errors)</code> al array de pesos \(\mathbf{w}\), salvo al elemento \(w_0\) (peso asociado a una entrada ficticia \(x_0 = 1\) para simplificar los cálculos). Recordando la fórmula de incremento de pesos vista más arriba:</p>

\[\Delta w_{j} := \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x_{j}^{(i)},\]

<p>tenemos que los elementos \(error^{(i)}\) del array <code class="language-plaintext highlighter-rouge">errors</code> se corresponden con \(error^{(i)} = y^{(i)} - \phi(z)_{A}^{(i)}\). Por lo tanto, el producto escalar entre la traspuesta de la matriz de atributos de cada muestra <code class="language-plaintext highlighter-rouge">X</code> y el array de errores <code class="language-plaintext highlighter-rouge">errors</code> (<code class="language-plaintext highlighter-rouge">X.T.dot(errors)</code>) se corresponden con el sumatorio \(\sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x_{j}^{(i)}\).</p>

<p>Para el caso del elemento \(w_0\), al ser el valor de los atributos de esa muestra ficticia igual a uno, basta con sumar los errores para obtener el incremento de peso (también podríamos haber añadido una fila con valores unitarios a la matriz <code class="language-plaintext highlighter-rouge">X</code>).</p>

<p>Aunque el algoritmo no usa la función de costos directamente, ya que el ajuste de pesos se realiza mediante el gradiente descendente que acabamos de aplicar y que se explica más arriba, vamos a almacenar también el valor de la función para dibujar después su evolución.</p>

<h3 id="cálculo-de-costos">Cálculo de costos</h3>

<p>Recordemos la función de costos:</p>

\[J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2.\]

<p>El programa almacena un arrary <code class="language-plaintext highlighter-rouge">errors = (y - output)</code> que se corresponde con \(y^{(i)} - \phi(z)_{A}^{(i)}\), así que para calcular en valor de la función sólo tenemos que sumar el cuadrado de los valores del array y dividir entre dos. Esto nos dará el valor calculado por la función de costos para los pesos actuales. Como estamos iterando mediante el gradiente descendente vamos a almacenar los resultados de la función obtenidos para cada conjunto de pesos:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">errors</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.0</span>
<span class="bp">self</span><span class="p">.</span><span class="n">cost_</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</code></pre></div></div>

<p>Una vez definida la clase, el libro pasa a utilizarla con dos tasas de aprendizaje distintas. Esta tasa va a determinar la convergencia del algoritmo, y encontrar el valor que nos de la mejor convergencia posible requiere muchas pruebas. Vamos a probar con \(\eta = 0.1\) y \(\eta = 0.0001\):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">ada1</span> <span class="o">=</span> <span class="n">AdalineGD</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ada1</span><span class="p">.</span><span class="n">cost_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ada1</span><span class="p">.</span><span class="n">cost_</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'log(Sum-squared-error)'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Adaline - Learning rate 0.01'</span><span class="p">)</span>

<span class="n">ada2</span> <span class="o">=</span> <span class="n">AdalineGD</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ada2</span><span class="p">.</span><span class="n">cost_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ada2</span><span class="p">.</span><span class="n">cost_</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Sum-squared-error'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Adaline - Learning rate 0.0001'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Pintamos una gráfica con cada tasa, representado la evolución del valor de la función de costos para cada iteración (época). Para la tasa de 0.1 representamos en realidad el logaritmo del valor, ya que el valor real diverge exponencialmente. En cambio, podemos ver como la tasa de 0.0001 converge, pero lentamente.</p>

<div style="text-align:center">
    <figure>
        <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_11.png"><img alt="Una tasa de aprendizaje muy alta provoca que el algoritmo no converja, una tasa muy baja provoca que converja lentamente" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_11.png" /></a>
        <figcaption>Una tasa de aprendizaje muy alta provoca que el algoritmo no converja, una tasa muy baja provoca que converja lentamente</figcaption>
    </figure>
</div>

<p>Llevado al ejemplo visual del gradiente descendente, vemos que la tasa de aprendizaje influye en la longitud del “paso” que se da hacia el mínimo. Si este paso es muy grande el algoritmo “se sale”:</p>

<div style="text-align:center">
    <figure>
        <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_12.png"><img alt="Visualización gráfica de los pasos dados con el algoritmo" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_12.png" /></a>
        <figcaption>Visualización gráfica de los pasos dados con el algoritmo</figcaption>
    </figure>
</div>

<p>El libro acaba esta parte con una referencia al escalado de características que se verá en el tema 3, y que nosotros dejaremos hasta entonces.</p>

<p>En la próxima entrada veremos cómo modificar este algoritmo para adaptarlo a grandes grupos de datos.</p>



			<div class="blog-navigation">
				
					<a class="prev" href="/2017/03/26/materia-oscura-blake-crouch/">&laquo; Materia oscura</a>
				
				
					<a class="next" href="/2017/04/09/python-machine-learning-iv/">PML T2 - Gradiente descendente estocástico &raquo;</a>
				
			</div>


			
				<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    
    var disqus_config = function () {
        this.page.url = "http://www.javiercancela.com/2017/04/02/python-machine-learning-iii/";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "/2017/04/02/python-machine-learning-iii"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        
        s.src = '//javiercancela.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
			
		</div>
	</div>
</section>



		<footer>
			<div class="container">
				<p class="editor-link"><a href="cloudcannon:collections/_data/footer.yml" class="btn"><strong>&#9998;</strong> Edit footer</a></p>
				<ul class="footer-left-links">
					
						<li>
							<a  href="/" >
								
								Blog
							</a>
						</li>
					
						<li>
							<a  href="/archivo/" >
								
								Archivo
							</a>
						</li>
					
						<li>
							<a  href="/sobre-mi/" >
								
								Sobre mí
							</a>
						</li>
					
				</ul>
				<ul class="footer-right-links">
					
						<li>
							<a target="_blank" href="https://twitter.com/javier_cancela_" class="twitter-icon">
								
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M22.46,6C21.69,6.35 20.86,6.58 20,6.69C20.88,6.16 21.56,5.32 21.88,4.31C21.05,4.81 20.13,5.16 19.16,5.36C18.37,4.5 17.26,4 16,4C13.65,4 11.73,5.92 11.73,8.29C11.73,8.63 11.77,8.96 11.84,9.27C8.28,9.09 5.11,7.38 3,4.79C2.63,5.42 2.42,6.16 2.42,6.94C2.42,8.43 3.17,9.75 4.33,10.5C3.62,10.5 2.96,10.3 2.38,10C2.38,10 2.38,10 2.38,10.03C2.38,12.11 3.86,13.85 5.82,14.24C5.46,14.34 5.08,14.39 4.69,14.39C4.42,14.39 4.15,14.36 3.89,14.31C4.43,16 6,17.26 7.89,17.29C6.43,18.45 4.58,19.13 2.56,19.13C2.22,19.13 1.88,19.11 1.54,19.07C3.44,20.29 5.7,21 8.12,21C16,21 20.33,14.46 20.33,8.79C20.33,8.6 20.33,8.42 20.32,8.23C21.16,7.63 21.88,6.87 22.46,6Z" /></svg>
	
								
							</a>
						</li>
					
						<li>
							<a target="_blank" href="https://www.linkedin.com/in/javiercancela" class="linkedin-icon">
								
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M19,19H16V13.7A1.5,1.5 0 0,0 14.5,12.2A1.5,1.5 0 0,0 13,13.7V19H10V10H13V11.2C13.5,10.36 14.59,9.8 15.5,9.8A3.5,3.5 0 0,1 19,13.3M6.5,8.31C5.5,8.31 4.69,7.5 4.69,6.5A1.81,1.81 0 0,1 6.5,4.69C7.5,4.69 8.31,5.5 8.31,6.5A1.81,1.81 0 0,1 6.5,8.31M8,19H5V10H8M20,2H4C2.89,2 2,2.89 2,4V20A2,2 0 0,0 4,22H20A2,2 0 0,0 22,20V4C22,2.89 21.1,2 20,2Z" /></svg>
	
								
							</a>
						</li>
					
						<li>
							<a  href="/feed.xml" class="rss-icon">
								
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"/><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
	
								
							</a>
						</li>
					
				</ul>
				<p class="copyright">
					<a href="https://cloudcannon.com/">
						Template by CloudCannon
					</a>
				</p>
			</div>
		</footer>

		<script src="//ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js"></script>
		<script src="/js/main.js"></script>
		<script src="/js/prism.js"></script>
		<script id="dsq-count-scr" src="//javiercancela.disqus.com/count.js" async></script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		<script type="text/javascript">
			MathJax.Hub.Config({
				config: ["MMLorHTML.js"],
				extensions: ["tex2jax.js"],
				jax: ["input/TeX"],
				displayAlign: "center",
				tex2jax: {
					inlineMath: [ ['$','$'], ["\\(","\\)"] ],
					displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
					processEscapes: false
				},
				TeX: {
					extensions: ["AMSmath.js", "AMSsymbols.js"],
					TagSide: "right",
					TagIndent: ".8em",
					MultLineWidth: "85%",
					equationNumbers: {
						autoNumber: "AMS",
					},
					unicode: {
						fonts: "STIXGeneral,'Arial Unicode MS'"
					}
				},
				showProcessingMessages: false
			});
		</script>		
	</body>
</html>
