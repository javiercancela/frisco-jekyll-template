<!DOCTYPE html>

<html lang="en">
	<head>
		<script type="text/javascript">
		    var host = "www.javiercancela.com";
		    if ((host == window.location.host) && (window.location.protocol != "https:"))
		        window.location.protocol = "https";
		</script>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="generator" content="Jekyll v3.8.6">

		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300|Rubik:300">
		<link rel="stylesheet" href="/css/screen.css">
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
		<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
		<link rel="manifest" href="/manifest.json">
		<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
		<meta name="theme-color" content="#ffffff">

		<!-- Begin Jekyll SEO tag v2.1.0 -->
<title>PML T2 - Gradiente descendente estocástico - Página de Javier Cancela</title>
<meta property="og:title" content="PML T2 - Gradiente descendente estocástico" />
<meta name="description" content="Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka" />
<meta property="og:description" content="Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka" />
<link rel="canonical" href="https://www.javiercancela.com/2017/04/09/python-machine-learning-iv/" />
<meta property="og:url" content="https://www.javiercancela.com/2017/04/09/python-machine-learning-iv/" />
<meta property="og:site_name" content="Página de Javier Cancela" />
<meta property="og:image" content="https://www.javiercancela.com/images/pml/pml.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-04-09T03:00:00+02:00" />
<script type="application/ld+json">
{"@context": "http://schema.org",
"@type": "BlogPosting",
"headline": "PML T2 - Gradiente descendente estocástico",
"image": "https://www.javiercancela.com/images/pml/pml.jpg",
"datePublished": "2017-04-09T03:00:00+02:00",
"description": "Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka",
"publisher": {"@type": "Organization",
"logo": {"@type": "ImageObject",
"url": "https://www.javiercancela.com/siteicon.png"}},
"url": "https://www.javiercancela.com/2017/04/09/python-machine-learning-iv/"}</script>
<!-- End Jekyll SEO tag -->


		
	</head>

	<body>
		<header id="mainHeader">
			<div class="container">
				<div class="company-name">
					<a href="/">
						<span class="dark-logo"><img width="104" height="38" src="/images/logo/dark.png" alt="dark frisco logo"></span>
						<span class="light-logo"><img width="104" height="38" src="/images/logo/light.png" alt="light frisco logo"></span>
					</a>
				</div>
				<nav>
	<a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
	
		
		

		
		<a href="/" class="" >Blog</a>
	
		
		

		
		<a href="/archivo/" class="" >Archivo</a>
	
		
		

		
		<a href="/sobre-mi/" class="" >Sobre mí</a>
	
</nav>

			</div>
		</header>

		<section class="hero" style="background-image: url(/images/pml/pml.jpg)">
	<div class="inner-hero text-container">
		<div class="hero-text-container">
			<h1>PML T2 - Gradiente descendente estocástico</h1>
			<p class="subtext">Python Machine Learning - Tema 2 - Notas sobre el libro de Sebastian Raschka</p>
		</div>
	</div>
</section>

<section>
	<div class="blog-post text-container">
		<p class="post-details">
	
		
		<span class="blog-filter">
			<a href="/category/python/">Python</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/libros/">Libros</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/machine-learning/">Machine Learning</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/inteligencia-artificial/">Inteligencia Artificial</a>
		</span>
	
	
	
	
	
	
	
	<span class="post-date">09 de abril de 2017</span>
</p>


		<div class="post-content">
			<p>El siguiente punto del libro nos habla de cómo modificar este algoritmo para adaptarlo a conjuntos con millones de datos. El punto clave está en que la forma del algoritmo que hemos analizado reevalúa todo el conjunto de datos cada vez que se realiza una iteración o época. Si tenemos 1 millón de muestras, por cada época vamos a tener que calcular:
<script type="math/tex">\Delta \mathbf{w} = \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}</script></p>

<p>Es decir, para cada época tenemos que realizar un millón de veces:</p>

<ul>
  <li>el cálculo de la función de costos</li>
  <li>el cálculo de la resta con la clase real</li>
  <li>la multiplicación del resultado por la entrada de la muestra</li>
</ul>

<p>y después sumar el millón de resultados.</p>

<h3 id="optimizando-el-proceso">Optimizando el proceso</h3>

<p>El gradiente descendente estocástico utiliza otra aproximación. En vez de recalcular los pesos usando todas las muestras, utilizamos sólo la muestra de la entrada para el cálculo:
<script type="math/tex">\Delta \mathbf{w} = \eta \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}</script></p>

<p>Este cambio tiene dos consecuencias fundamentales Por un lado, los pesos se actualizan con más frecuencia, así que la convergencia suele ser más rápida. Por otro lado, usar una sola muestra da menos precisión a la actualización, así que se introduce más ruido en los pesos. Esto no es necesariamente malo, ya que facilita salir de mínimos locales que no se corresponden con la minimización de nuestra función de costos. Otra ventaja de este algoritmo es que resulta adecuado para el aprendizaje <em>on line</em>, es decir, aquel en el que el modelo tiene que adaptarse rápidamente a cambios en los datos.</p>

<p>Una variante mencionada en el libro pero no implementada en el código consiste en sustituir una tasa de aprendizaje fija <script type="math/tex">\eta</script> por una que disminuya con el tiempo:</p>

<script type="math/tex; mode=display">\frac{c_1}{[\textit {número de iteraciones}] + c_2}</script>

<h3 id="código-python">Código Python</h3>

<p>Vamos a adaptar la clase Adaline anterior a este nuevo algoritmo:</p>

<pre class="line-numbers">
  <code class="language-python">
  from numpy.random import seed

  class AdalineSGD(object):
      def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
          self.eta = eta
          self.n_iter = n_iter
          self.w_initialized = False
          self.shuffle = shuffle
          if random_state:
              seed(random_state)
          
      def fit(self, X, y):
          self._initialize_weights(X.shape[1])
          self.cost_ = []
          for i in range(self.n_iter):
              if self.shuffle:
                  X, y = self._shuffle(X, y)
              cost = []
              for xi, target in zip(X, y):
                  cost.append(self._update_weights(xi, target))
              avg_cost = sum(cost) / len(y)
              self.cost_.append(avg_cost)
          return self

      def partial_fit(self, X, y):
          if not self.w_initialized:
              self._initialize_weights(X.shape[1])
          if y.ravel().shape[0] &gt; 1:
              for xi, target in zip(X, y):
                  self._update_weights(xi, target)
          else:
              self._update_weights(X, y)
          return self

      def _shuffle(self, X, y):
          r = np.random.permutation(len(y))
          return X[r], y[r]
      
      def _initialize_weights(self, m):
          self.w_ = np.zeros(1 + m)
          self.w_initialized = True
          
      def _update_weights(self, xi, target):
          output = self.net_input(xi)
          error = (target - output)
          self.w_[1:] += self.eta * xi.dot(error)
          self.w_[0] += self.eta * error
          cost = 0.5 * error**2
          return cost
      
      def net_input(self, X):
          return np.dot(X, self.w_[1:]) + self.w_[0]

      def activation(self, X):
          return self.net_input(X)

      def predict(self, X):
          return np.where(self.activation(X) &gt;= 0.0, 1, -1)
  </code>
</pre>

<p>En primer lugar, hemos añadido un método <code class="highlighter-rouge">_shuffle</code> para mezclar los datos de la entrada antes de cada iteración. El propósito de este paso es evitar ciclos al minimizar la función de costos, ya que este algoritmo es sensible a este problema.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">r</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">r</span><span class="p">]</span>
</code></pre></div></div>

<p>El constructor incluye parámetros para indicar si queremos mezclar al azar los datos de entrada y para inicializar una semilla para esta mezcla.</p>

<p>El método <code class="highlighter-rouge">fit</code> es el que se encarga del entrenamiento. Invocamos primero al método que inicializa los pesos a cero. Este método existe para marcar los pesos como inicializados, información que necesitamos para el método de entrenamiento parcial <code class="highlighter-rouge">parcial_fit</code>.</p>

<p>Dentro del método <code class="highlighter-rouge">fit</code> el código que se itera en cada época comienza comprobando si hay que barajar los datos de entrada, e inicializa el costo que evaluará la función de costos:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shuffle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>A continuación iteramos los datos. La variable <code class="highlighter-rouge">X</code> es una matriz que contiene una fila por cada muestra, y una columna por cada característica. La variable <code class="highlighter-rouge">y</code> es un array que contiene un elemento, la clase real, para cada muestra de <code class="highlighter-rouge">X</code>. El método <code class="highlighter-rouge">zip</code> nos devolverá un elemento de cada variable (<em>zip</em> en inglés significa, entre otras cosas, cremallera), por lo que en cada iteración del <code class="highlighter-rouge">for</code> en <code class="highlighter-rouge">xi</code> y en <code class="highlighter-rouge">target</code> se almacenarán las características y la clase real de cada muestra.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">cost</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_update_weights</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>

</code></pre></div></div>

<p>Para cada muestra, actualizamos los pesos en el método <code class="highlighter-rouge">_update_weights</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net_input</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">xi</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">error</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">error</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">cost</span>

</code></pre></div></div>

<p>Para la salida real seguimos utilizando la función identidad, con lo que la <code class="highlighter-rouge">output</code> es igual a la entrada. La diferencia en este nuevo algoritmo está en cómo ajustamos los pesos. El array de pesos se ajusta con el producto del error con la muestra procesada, no con toda la matriz de entradas. Lógicamente, el costo calculado se obtendrá del error de esta muestra al cuadrado. El costo lo añadimos al array de costos para representarlo posteriormente.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">avg_cost</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">cost_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_cost</span><span class="p">)</span>
</code></pre></div></div>

<p>Para finalizar el método <code class="highlighter-rouge">fit</code> calculamos la media de los costos de cada muestra, y la añadimos al array de costos.</p>

<h3 id="resultados">Resultados</h3>

<p>Ahora toca usar esta clase. La instanciamos y entrenamos el modelo (el libro usa, en este ejemplo y en el de la entrada anterior, un conjunto de datos escalado para mejorar el rendimiento, y que nosotros revisaremos en el tema 3):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ada</span> <span class="o">=</span> <span class="n">AdalineSGD</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ada</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Pintamos los resultados, primero mostrando la clasificación de las muestras y después el costo por época.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">ada</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Adaline - Gradiente descendiente estocástico'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'longitud sépalo'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'longitud pétalo'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ada</span><span class="o">.</span><span class="n">cost_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ada</span><span class="o">.</span><span class="n">cost_</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Épocas'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Costo medio'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>Los datos obtenidos son estos (los resultados son distintos a los obtenidos en el libro, ya que nosotros no hemos estandarizado los valores de las muestras):</p>

<div style="text-align:center">
    <figure>
        <img alt="Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados" src="/images/pml/2_adaline.png" />
        <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_14.png"><img alt="Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_14.png" /></a>
        <figcaption>Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados</figcaption>
    </figure>
</div>

<p>Nuestra convergencia es mucha más rápida, pero también muy variable, ya que a partir de la sexta época el costo empieza a oscilar.</p>


			<div class="blog-navigation">
				
					<a class="prev" href="/2017/04/02/python-machine-learning-iii/">&laquo; PML T2 - ADALINE</a>
				
				
					<a class="next" href="/2017/04/16/pausa-por-vacaciones/">Pausa por vacaciones &raquo;</a>
				
			</div>


			
				<div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */
    
    var disqus_config = function () {
        this.page.url = "http://www.javiercancela.com/2017/04/09/python-machine-learning-iv/";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "/2017/04/09/python-machine-learning-iv"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() {  // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        
        s.src = '//javiercancela.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
			
		</div>
	</div>
</section>



		<footer>
			<div class="container">
				<p class="editor-link"><a href="cloudcannon:collections/_data/footer.yml" class="btn"><strong>&#9998;</strong> Edit footer</a></p>
				<ul class="footer-left-links">
					
						<li>
							<a  href="/" >
								
								Blog
							</a>
						</li>
					
						<li>
							<a  href="/archivo/" >
								
								Archivo
							</a>
						</li>
					
						<li>
							<a  href="/sobre-mi/" >
								
								Sobre mí
							</a>
						</li>
					
				</ul>
				<ul class="footer-right-links">
					
						<li>
							<a target="_blank" href="https://twitter.com/javier_cancela_" class="twitter-icon">
								
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M22.46,6C21.69,6.35 20.86,6.58 20,6.69C20.88,6.16 21.56,5.32 21.88,4.31C21.05,4.81 20.13,5.16 19.16,5.36C18.37,4.5 17.26,4 16,4C13.65,4 11.73,5.92 11.73,8.29C11.73,8.63 11.77,8.96 11.84,9.27C8.28,9.09 5.11,7.38 3,4.79C2.63,5.42 2.42,6.16 2.42,6.94C2.42,8.43 3.17,9.75 4.33,10.5C3.62,10.5 2.96,10.3 2.38,10C2.38,10 2.38,10 2.38,10.03C2.38,12.11 3.86,13.85 5.82,14.24C5.46,14.34 5.08,14.39 4.69,14.39C4.42,14.39 4.15,14.36 3.89,14.31C4.43,16 6,17.26 7.89,17.29C6.43,18.45 4.58,19.13 2.56,19.13C2.22,19.13 1.88,19.11 1.54,19.07C3.44,20.29 5.7,21 8.12,21C16,21 20.33,14.46 20.33,8.79C20.33,8.6 20.33,8.42 20.32,8.23C21.16,7.63 21.88,6.87 22.46,6Z" /></svg>
	
								
							</a>
						</li>
					
						<li>
							<a target="_blank" href="https://www.linkedin.com/in/javiercancela" class="linkedin-icon">
								
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M19,19H16V13.7A1.5,1.5 0 0,0 14.5,12.2A1.5,1.5 0 0,0 13,13.7V19H10V10H13V11.2C13.5,10.36 14.59,9.8 15.5,9.8A3.5,3.5 0 0,1 19,13.3M6.5,8.31C5.5,8.31 4.69,7.5 4.69,6.5A1.81,1.81 0 0,1 6.5,4.69C7.5,4.69 8.31,5.5 8.31,6.5A1.81,1.81 0 0,1 6.5,8.31M8,19H5V10H8M20,2H4C2.89,2 2,2.89 2,4V20A2,2 0 0,0 4,22H20A2,2 0 0,0 22,20V4C22,2.89 21.1,2 20,2Z" /></svg>
	
								
							</a>
						</li>
					
						<li>
							<a  href="/feed.xml" class="rss-icon">
								
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"/><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
	
								
							</a>
						</li>
					
				</ul>
				<p class="copyright">
					<a href="https://cloudcannon.com/">
						Template by CloudCannon
					</a>
				</p>
			</div>
		</footer>

		<script src="//ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js"></script>
		<script src="/js/main.js"></script>
		<script src="/js/prism.js"></script>
		<script id="dsq-count-scr" src="//javiercancela.disqus.com/count.js" async></script>
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		<script type="text/javascript">
			MathJax.Hub.Config({
				config: ["MMLorHTML.js"],
				extensions: ["tex2jax.js"],
				jax: ["input/TeX"],
				displayAlign: "center",
				tex2jax: {
					inlineMath: [ ['$','$'], ["\\(","\\)"] ],
					displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
					processEscapes: false
				},
				TeX: {
					extensions: ["AMSmath.js", "AMSsymbols.js"],
					TagSide: "right",
					TagIndent: ".8em",
					MultLineWidth: "85%",
					equationNumbers: {
						autoNumber: "AMS",
					},
					unicode: {
						fonts: "STIXGeneral,'Arial Unicode MS'"
					}
				},
				showProcessingMessages: false
			});
		</script>		
	</body>
</html>
