<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="https://www.javiercancela.com/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.8.6">Jekyll</generator><link href="https://www.javiercancela.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.javiercancela.com/" rel="alternate" type="text/html" /><updated>2020-01-12T19:08:57+01:00</updated><id>https://www.javiercancela.com//</id><title type="html">Página de Javier Cancela</title><subtitle>Página personal de Javier Cancela</subtitle><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><entry><title type="html">Varios</title><link href="https://www.javiercancela.com/2018/03/12/varios/" rel="alternate" type="text/html" title="Varios" /><published>2018-03-12T21:00:00+01:00</published><updated>2018-03-12T21:00:00+01:00</updated><id>https://www.javiercancela.com/2018/03/12/varios</id><content type="html" xml:base="https://www.javiercancela.com/2018/03/12/varios/">&lt;p&gt;Algunas notas rápidas sobre Google Maps. Y sobre otras cosas.&lt;/p&gt;

&lt;h2 id=&quot;api-de-google-maps&quot;&gt;API de Google Maps&lt;/h2&gt;

&lt;p&gt;Una de las cosas que te da la experiencia es una cierta intuición sobre cómo de difícil es hacer algo que no hayas hecho nunca antes. Y resulta muy molesto cuando esa intuición falla.&lt;/p&gt;

&lt;p&gt;Necesitaba una prueba de concepto de una animación del mapa del mundo con javascript. Mi idea era hacerlo con la api de Google Maps, que sólo he utilizado para cosas muy básicas. El planteamiento era el siguiente: una animación de la tierra tal como se ve en Google Earth, que muestre, por ejemplo, Europa con unos indicadores encima; pasado un tiempo la animación hace zoom sobre una ciudad mostrando otros indicadores, tras lo cual se aleja para cambiar de continente y bla bla bla. &lt;em&gt;Piece of cake&lt;/em&gt;, que dicen los ingleses.&lt;/p&gt;

&lt;p&gt;Empiezo a buscar y me encuentro con que &lt;a href=&quot;https://developers.google.com/earth/&quot;&gt;la api de Google Earth ya no está soportada&lt;/a&gt; (ups!), pero Google Maps tiene un modo 3D nativo que imita el efecto (¡bien!). Así que cojo mi &lt;a href=&quot;https://developers.google.com/maps/documentation/javascript/get-api-key&quot;&gt;&lt;em&gt;key&lt;/em&gt; de Google Maps&lt;/a&gt; y empiezo a probar. “No debería llevar más de media hora”, me digo.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;El mundo según Google Maps. Pero no según su API&quot; src=&quot;/images/google-earth.jpg&quot; /&gt;
        &lt;figcaption&gt;El mundo según Google Maps. Pero no según su API&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Pues no se puede. Resulta que el modo 3D de Google Maps no está soportado en mapas embebidos dentro de aplicaciones. Y tampoco es fácil verlo, porque no he encontrado referencias a ello en la docuemtación. Probablemente se pueda en alguna versión futura, pero de momento no.&lt;/p&gt;

&lt;p&gt;De hecho, la versión para incrustar está aún más limitada. Después de asumir que no iba a mostrar el mapa del mundo, me puse a jugar con las animaciones en 2D, buscando transiciones entre puntos y niveles de zoom lo más fluidas posible. Lo que descubrí fue que no es posible hacerlas tan fluidas como las que se ven en la página de Google Maps. La única explicación que encontré al respecto es &lt;a href=&quot;http://stackoverflow.com/a/34154254&quot;&gt;esta respuesta stackoverflow&lt;/a&gt;. La web de Google Maps utiliza un objeto &lt;em&gt;canvas&lt;/em&gt; para pintar una versión vectorizada del mapa, mientras que la API trae un conjunto de imágenes para cada nivel de zoom. No sé si es correcto, pero es consistente con lo que se observa en las pruebas.&lt;/p&gt;

&lt;p&gt;Hay alternativas a la API de Google Maps: &lt;a href=&quot;https://www.mapbox.com/mapbox-gl-js/api/&quot;&gt;mapbox&lt;/a&gt; (de pago) o  &lt;a href=&quot;https://cesiumjs.org/&quot;&gt;cesiumjs&lt;/a&gt; (gratuita). Esta última tiene varios orígenes de datos cartográficos posibles, entre ellos los mapas de Bing (Microsoft).&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;El mundo visto a través de Cesium y Bing&quot; src=&quot;/images/cesiumjs.jpg&quot; /&gt;
        &lt;figcaption&gt;El mundo visto a través de Cesium y Bing&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Algunas notas rápidas sobre Google Maps. Y sobre otras cosas.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/varios.jpg" /></entry><entry><title type="html">Deep learning - Introducción</title><link href="https://www.javiercancela.com/2018/03/12/deep-learning-01/" rel="alternate" type="text/html" title="Deep learning - Introducción" /><published>2018-03-12T21:00:00+01:00</published><updated>2018-03-12T21:00:00+01:00</updated><id>https://www.javiercancela.com/2018/03/12/deep-learning-01</id><content type="html" xml:base="https://www.javiercancela.com/2018/03/12/deep-learning-01/">&lt;p&gt;Notas sobre el capítulo 1 del libro “Deep learning”&lt;/p&gt;

&lt;h1 id=&quot;capítulo-1---introducción&quot;&gt;Capítulo 1 - Introducción&lt;/h1&gt;

&lt;h2 id=&quot;a-quién-va-destinado-este-libro&quot;&gt;A quién va destinado este libro&lt;/h2&gt;
&lt;p&gt;Este es un libro destinado a dos tipos distintos de lector:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;El lector universitario, de grado o máster (&lt;em&gt;undergraduate or graduate&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;El profesional de la informática que quiere incorporar técnicas de &lt;em&gt;deep learning&lt;/em&gt; en sus proyectos&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Los autores asumen que el lector posee&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;conocimientos básicos de programación&lt;/li&gt;
  &lt;li&gt;una compresión básica de los factores que afectan al rendimiento de los ordenadores&lt;/li&gt;
  &lt;li&gt;conocimientos sobre la teoría de la complejidad computacional&lt;/li&gt;
  &lt;li&gt;conocimientos básicos de cálculo&lt;/li&gt;
  &lt;li&gt;terminología sobre teoría de grafos&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;El libro se divide en tres partes. La parte I presenta los conceptos básicos del &lt;em&gt;machine learning&lt;/em&gt;, así como las herramientas matemáticas asociadas. La parte II describe los algoritmos más comunes. La parte III describe ideas especulativas, aquello que los autores consideran el futuro de esta disciplina.&lt;/p&gt;

&lt;p&gt;Fig 1.6&lt;/p&gt;

&lt;h2 id=&quot;inteligencia-artifical&quot;&gt;Inteligencia artifical&lt;/h2&gt;
&lt;p&gt;El libro comienza hablando sobre los problemas que constituyen el verdadero desafío de la inteligencia artificial hoy en día: las tareas que la gente realiza con facilidad pero cuya descripción formal es compleja, como reconocer rostros en imágenes. &lt;em&gt;Deep learning&lt;/em&gt; es una solución a estos problemas. Por una parte acumula conocimiento a través de la experiencia, evitando así que se tenga que definir formalmente todo el conocimiento necesario. Por otro lado, define conceptos en varias capas jerárquicas, de forma que cada concepto se define a partir de varios conceptos más simples. Es esta idea de construir conceptos complejos sobre muchas capas de conceptos más simples, de forma que las estructuras de estas redes neuronales tienen mucha “profundidad”, la que da el nombre de &lt;em&gt;deep learning&lt;/em&gt; a esta disciplina.&lt;/p&gt;

&lt;p&gt;El origen de esta aproximación a la inteligencia artificial está en las limitaciones de los sistemas que necesitaban que el conocimiento se incorporase de forma explícita. Para eliminar estas limitaciones se empieza a diseñar sistemas capaces de distinguir patrones en datos. Esta capacidad consituye en nucleo del &lt;em&gt;machine learning&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;representaciones-y-características&quot;&gt;Representaciones y características&lt;/h2&gt;
&lt;p&gt;Una consecuencia de este enfoque centrado en los datos es que los algoritmos de &lt;em&gt;machine learning&lt;/em&gt; dependen mucho de la representación de los datos utilizada. El libro nos muestra un ejemplo muy visual:&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Figura 1.1 - El mismo conjunto de datos representado en coordenadas cartesianas (izq.) y polares (dcha.)&quot; src=&quot;/images/DL/Fig1.1.jpg&quot; /&gt;
        &lt;figcaption&gt;Figura 1.1 - El mismo conjunto de datos representado en coordenadas cartesianas (izq.) y polares (dcha.)&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;Si el objetivo fuese separar con una línea recta los datos de una categoría (triángulos verdes) de los de la otra (círculos azules), lo tendríamos fácil al representar los datos con coordenadas polares, e imposible al hacerlo con coordenadas cartesianas.&lt;/p&gt;

&lt;p&gt;Otro concepto básico que nos presentan es el de &lt;strong&gt;característica&lt;/strong&gt; (&lt;em&gt;feature&lt;/em&gt; en inglés, lo he visto también traducido al castellano como &lt;strong&gt;atributo&lt;/strong&gt;). Una característica es una propiedad medible del fenómeno que se está representando en los datos. Un ejemplo que nos presentan es el de la identificación de una persona a partir de su voz. El sonido emitido permite estimar el tamaño del tracto vocal; este tamaño constituye una &lt;strong&gt;característica&lt;/strong&gt; que sirve de pista sobre el sexo y la edad de la persona que habla.&lt;/p&gt;

&lt;p&gt;La definición de característica da paso a la introducción del concepto &lt;em&gt;representation learning&lt;/em&gt; (aprendizaje de representaciones, aunque creo que la expresión se utiliza también en pedagogía con otro sentido distinto). Determinar qué características debe extraer nuestro algoritmo es, habitualmente, una tarea muy compleja (pensemos en cómo indicarle que detecte coches en fotos, haciéndolo en términos del valor de cada pixel). El &lt;em&gt;representation learning&lt;/em&gt; busca que el ordenador descubra tanto la relación entre la representación y el resultado (si tiene cuatro ruedas es un coche) como la representación en sí (el ordenador no va a saber qué es una rueda, pero sí va a aprender que ciertos valores de pixels en zonas próximas aparecen dos, tres o cuatro veces en las fotos que contienen coches).&lt;/p&gt;

&lt;h2 id=&quot;deep-learning&quot;&gt;Deep learning&lt;/h2&gt;
&lt;p&gt;Para presentarnos una definición de &lt;em&gt;Deep learning&lt;/em&gt; la introducción nos habla primero de los &lt;strong&gt;factores de variación&lt;/strong&gt;. Esta expresión se refiere a todos los factores que afectan al valor de los datos que tratamos. Y algunos de estos factores no serán relevantes para las características que queremos determinar en nuestro algoritmo de aprendizaje. En el ejemplo de las imágenes de coches, el color depende de la cantidad de luz, y su forma del ángulo en el que lo vemos. Necesitamos separar estos factores para poder determinar el color y la forma del coche. Sin embargo, en algunos casos estos factores son muy difíciles de aislar mediante &lt;em&gt;representation learning&lt;/em&gt;. &lt;em&gt;Deep learning&lt;/em&gt; resuelve el problema con representaciones que se expresan en términos de otras representaciones más simples.&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Figura 1.2 - A partir de los pixeles individuales el algoritmo identifica bordes, a partir de los cuales identifica contornos, a partir de los que identifica partes de objetos, para finalmente identificar los objetos completos&quot; src=&quot;/images/DL/Fig1.2.jpg&quot; /&gt;
        &lt;figcaption&gt;Figura 1.2 - A partir de los pixeles individuales el algoritmo identifica bordes, a partir de los cuales identifica contornos, a partir de los que identifica partes de objetos, para finalmente identificar los objetos completos&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Otra forma de ver los algoritmos &lt;em&gt;deep learning&lt;/em&gt; consiste en pensar en cada capa como en la memoria de un ordenador tras ejecutar una serie de instrucciones en paralelo.&lt;/p&gt;

&lt;p&gt;Por lo tanto, es posible pensar en la profundidad de un algoritmo &lt;em&gt;deep learning&lt;/em&gt; de dos maneras distintas: la profundidad podría ser el número de instrucciones secuenciales a ejecutar, pero también podría ser la longitud del grafo que relaciona cada concepto con los conceptos más simples en los que se basa.&lt;/p&gt;

&lt;p&gt;Fig 1.4 y 1.5&lt;/p&gt;

&lt;h2 id=&quot;historia&quot;&gt;Historia&lt;/h2&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Notas sobre el capítulo 1 del libro “Deep learning”</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/deep-learning.jpg" /></entry><entry><title type="html">Deep learning - Prólogo</title><link href="https://www.javiercancela.com/2018/02/26/deep-learning-00/" rel="alternate" type="text/html" title="Deep learning - Prólogo" /><published>2018-02-26T21:00:00+01:00</published><updated>2018-02-26T21:00:00+01:00</updated><id>https://www.javiercancela.com/2018/02/26/deep-learning-00</id><content type="html" xml:base="https://www.javiercancela.com/2018/02/26/deep-learning-00/">&lt;p&gt;En la asignatura de inteligencia artificial que se impartía en mi facultad (de Física) a mediados de los noventa, el grueso del temario se dedicaba a estudiar &lt;a href=&quot;https://es.wikipedia.org/wiki/Sistema_experto&quot;&gt;sistemas expertos&lt;/a&gt;, pese a que estos ya empezaban a perder prestigio en el ámbito académico. El concepto de &lt;a href=&quot;https://es.wikipedia.org/wiki/Red_neuronal_artificial&quot;&gt;red neuronal&lt;/a&gt; también aparecía, aunque creo que de pasada. Leyendo a toro pasado sobre aquella época resulta claro que esta disciplina atravesaba un período de crisis.&lt;/p&gt;

&lt;p&gt;Han pasado más de veinte años, y el mundo de la inteligencia artificial de hoy no puede ser más distinto: una disciplina de moda, que atrae dinero de la empresa privada en cantidades increíbles, y que aparece en los medios de comunciación asociada a historias de éxito como la &lt;a href=&quot;https://es.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol&quot;&gt;derrota de uno de los mejores jugadores de Go de la historia&lt;/a&gt; o &lt;a href=&quot;http://voltaico.lavozdegalicia.es/2017/01/google-translate-mejora/&quot;&gt;la impresionante mejora del servicio de traducción de Google&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Las inteligencias artificiales leen código ofuscado para evadirse&quot; src=&quot;/images/ofuscado.jpg&quot; /&gt;
        &lt;figcaption&gt;Las inteligencias artificiales leen código ofuscado para evadirse&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;deep-learning&quot;&gt;&lt;em&gt;Deep learning&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Detrás de este resurgir no hay un cambio de paradigma ni un descubrimiento revolucionario. La diferencia con el pasado es más cualitativa que cuantitativa: los ordenadores de hoy en día son mucho más rápidos, y existen muchos más datos con los que entrenar a los algoritmos utilizados. Todos estos grandes avances están basados en los principos sobre redes neuronales definidos en los años cincuenta, reencarnados en la actualidad en la disciplina conocida como &lt;em&gt;machine learning&lt;/em&gt;, y su hija aventajada, &lt;em&gt;deep learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Machine learning&lt;/em&gt; (&lt;a href=&quot;https://es.wikipedia.org/wiki/Aprendizaje_autom%C3%A1tico&quot;&gt;Aprendizaje automático&lt;/a&gt; es una de las traducciones al español, yo voy a mantener el término en inglés) es una rama de la inteligencia artificial que estudia la capacidad de los ordenadores para aprender. Existen varias definiciones formales de qué significa aprender en este contexto, pero podrímos decir que un ordenador &lt;em&gt;aprende&lt;/em&gt; cuando realiza una tarea mejor cuanto más veces la realiza, es decir, según va adquiriendo experiencia.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deep learning&lt;/em&gt; (&lt;a href=&quot;https://es.wikipedia.org/wiki/Aprendizaje_profundo&quot;&gt;Aprendizaje profundo&lt;/a&gt; parece ser el término más habitual en español) es, a su vez, una rama del &lt;em&gt;machine learning&lt;/em&gt;. Lo que caracteriza al &lt;em&gt;machine learning&lt;/em&gt; es el tipo de algoritmos que utiliza: jerárquicos, no lineales y con múltiples capas.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;En las profundidades del aprendizaje&quot; src=&quot;/images/profundo.jpg&quot; /&gt;
        &lt;figcaption&gt;En las profundidades del aprendizaje&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;El objeto de esta serie de entradas es dejar por escrito algunas ideas básicas de uno de los libros que estoy leyendo: &lt;a href=&quot;https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618&quot;&gt;Deep Learning&lt;/a&gt;, de Ian Goodfellow, Yoshua Bengio y Aaron Courville. Este libro está destinado a estudiantes universitarios y no es una lectura ligera, pero me decidí a leerlo porque tiene muy buenas críticas.&lt;/p&gt;

&lt;p&gt;Bueno, también me decidí a leerlo porque está disponible de forma gratuita &lt;a href=&quot;http://www.deeplearningbook.org/&quot;&gt;en esta web&lt;/a&gt;. Esta web incluye también &lt;a href=&quot;http://www.deeplearningbook.org/lecture_slides.html&quot;&gt;algunas diapositivas&lt;/a&gt; para exponer cada tema, orientadas a profesores.&lt;/p&gt;

&lt;p&gt;Mi plan es dedicar una entrada a cada tema, haciendo un pequeño resumen del mismo. Es posible que alguno de los temas requiera más de una entrada.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">En la asignatura de inteligencia artificial que se impartía en mi facultad (de Física) a mediados de los noventa, el grueso del temario se dedicaba a estudiar sistemas expertos, pese a que estos ya empezaban a perder prestigio en el ámbito académico. El concepto de red neuronal también aparecía, aunque creo que de pasada. Leyendo a toro pasado sobre aquella época resulta claro que esta disciplina atravesaba un período de crisis.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/deep-learning.jpg" /></entry><entry><title type="html">PML T3 - Algoritmo con regresión logística</title><link href="https://www.javiercancela.com/2017/05/28/python-machine-learning-viii/" rel="alternate" type="text/html" title="PML T3 - Algoritmo con regresión logística" /><published>2017-05-28T03:00:00+02:00</published><updated>2017-05-28T03:00:00+02:00</updated><id>https://www.javiercancela.com/2017/05/28/python-machine-learning-viii</id><content type="html" xml:base="https://www.javiercancela.com/2017/05/28/python-machine-learning-viii/">&lt;p&gt;A partir del concepto de &lt;a href=&quot;/2017/05/21/python-machine-learning-vii/&quot;&gt;regresión logística&lt;/a&gt; 
podemos ya definir el siguiente algoritmo.&lt;/p&gt;

&lt;h2 id=&quot;el-modelo-para-la-regresión-logística&quot;&gt;El modelo para la regresión logística&lt;/h2&gt;

&lt;p&gt;El libro traza un paralelo con el modelo &lt;a href=&quot;/2017/04/02/python-machine-learning-iii/&quot;&gt;ADALINE&lt;/a&gt;, donde la función indentidad (&lt;script type=&quot;math/tex&quot;&gt;\phi(z)=z&lt;/script&gt;) es sustituida por la función logística (sigmoidea) en nuestro modelo:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;El algoritmo de ADALINE con la función identidad sustituida por la función logística o sigmoidea&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch03/images/03_03.png&quot; /&gt;
        &lt;figcaption&gt;El algoritmo de ADALINE con la función identidad sustituida por la función logística o sigmoidea&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cálculo-de-los-pesos&quot;&gt;Cálculo de los pesos&lt;/h3&gt;

&lt;p&gt;Cuando hablamos de ADALINE definimos la &lt;a href=&quot;/2017/04/02/python-machine-learning-iii/#1&quot;&gt;función de costos de la suma de los errores cuadrados&lt;/a&gt; para minimizarla:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2.&lt;/script&gt;

&lt;p&gt;Para obtener una función de costos que minimizar, el autor empieza definiendo la siguiente función:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\mathbf{w}) = P(\mathbf{y} | \mathbf{x}; \mathbf{w}) = \prod_{i=1}^{n} P \big( y^{(i)} | x^{(i)}; \mathbf{w} \big) =  \prod_{i=1}^{n} \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}},&lt;/script&gt;

&lt;p&gt;donde &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; es la clase (&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; o &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;) de la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, y &lt;script type=&quot;math/tex&quot;&gt;\phi \big(z^{(i)} \big)&lt;/script&gt; es la probabilidad de que la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; pertenezca a la clase &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. Como vimos en la entrada anterior, la clase predicha &lt;script type=&quot;math/tex&quot;&gt;\hat y&lt;/script&gt; será &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; si la probabilidad de es mayor o igual a &lt;script type=&quot;math/tex&quot;&gt;0.5&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Por tanto, la expresión&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}}&lt;/script&gt;

&lt;p&gt;indica la probabilidad de que la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; pertenezca a la clase real (&lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt;), ya que si &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} = 1&lt;/script&gt;, la expresión se simplifica a &lt;script type=&quot;math/tex&quot;&gt;\phi \big(z^{(i)} \big)&lt;/script&gt;, y si &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} = 0&lt;/script&gt; nos queda&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}},&lt;/script&gt;

&lt;p&gt;que es la probabilidad de que la muestra sea de clase distinta de &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, es decir, la probabilidad de que se de clase &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Entonces, la función &lt;script type=&quot;math/tex&quot;&gt;L(\mathbf{w})&lt;/script&gt; es el producto de las probabilidades de que cada muestra sea de su clase real, en función de los pesos &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt; establecidos. Si maximizamos esa función, estaremos maximizando la calidad de la predicción para el conjunto de muestras.&lt;/p&gt;

&lt;h3 id=&quot;simplificando-los-cálculos&quot;&gt;Simplificando los cálculos&lt;/h3&gt;

&lt;p&gt;El siguiente paso que da el libro es calcular el logaritmo de &lt;script type=&quot;math/tex&quot;&gt;L(\mathbf{w})&lt;/script&gt;. El propósito de esta función es doble. Por un lado, evita un problema potencial si las probabilidades son lo suficientemente pequeñas para que su producto provoque un desbordamiento. El logaritmo convierte los productos en sumas, con lo que el riesgo desaparece. Por otro lado, el cálculo de la derivada se simplifica mucho por el mismo motivo.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\mathbf{w}) = \log L(\mathbf{w}) = \log \Bigg[ \prod_{i=1}^{n} \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}}  \Bigg]&lt;/script&gt;

&lt;p&gt;Dado que &lt;script type=&quot;math/tex&quot;&gt;\log (xy) = \log x + \log y&lt;/script&gt;, el logaritmo del productorio se convierte en un sumatorio de logaritmos:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\mathbf{w}) = \sum_{i=1}^{n} \Bigg[ \log \bigg( \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}} \bigg) \Bigg].&lt;/script&gt;

&lt;p&gt;Ahora el sumatorio contiene también el logaritmo de un producto, por lo que podemos convertirlo en:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\mathbf{w}) = \sum_{i=1}^{n} \Bigg[ \log \bigg( \phi \big(z^{(i)} \big) \bigg) ^ {y^{(i)}} + \log \bigg( 1 - \phi \big( z^{(i)} \big) \bigg)^{1-y^{(i)}}  \Bigg].&lt;/script&gt;

&lt;p&gt;Finalmente podemos simplificar usar la propiedad &lt;script type=&quot;math/tex&quot;&gt;\log (x^y) = y \log x&lt;/script&gt; para convertir la expresión en:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\mathbf{w}) = \sum_{i=1}^{n} \Bigg[ y^{(i)} \log \bigg( \phi \big(z^{(i)} \big) \bigg) + \bigg(1-y^{(i)} \bigg) \log \bigg( 1 - \phi \big( z^{(i)} \big) \bigg) \Bigg].&lt;/script&gt;

&lt;p&gt;En la siguiente entrada veremos cómo convertir esta expresión en una función de costos.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">A partir del concepto de regresión logística 
podemos ya definir el siguiente algoritmo.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">PML T3 - Regresión logística</title><link href="https://www.javiercancela.com/2017/05/21/python-machine-learning-vii/" rel="alternate" type="text/html" title="PML T3 - Regresión logística" /><published>2017-05-21T03:00:00+02:00</published><updated>2017-05-21T03:00:00+02:00</updated><id>https://www.javiercancela.com/2017/05/21/python-machine-learning-vii</id><content type="html" xml:base="https://www.javiercancela.com/2017/05/21/python-machine-learning-vii/">&lt;p&gt;Seguimos con algunos conceptos estadísticos útiles para estudiar el modelo de regresión logística.&lt;/p&gt;

&lt;h3 id=&quot;predicción-de-categorías&quot;&gt;Predicción de categorías&lt;/h3&gt;

&lt;p&gt;La principal característica de la regresión logística es que la variable respuesta es una categoría, no un valor continuo. Siguiendo con el ejemplo de la entrada anterior, la regresión logística nos permitiría relacionar las horas dedicadas al estudio con la categoría final de la nota: suspenso o aprobado. Este tipo de regresión logística, con dos posibles categorías para el resultado, se llama regresión logística binaria.&lt;/p&gt;

&lt;p&gt;La regresión logística estima la probabilidad de que una característica esté presente en la muestra para los valores de las variables predictoras dadas. Estos valores pueden ser continuos, discretos, categóricos o una mezcla de varios. En el caso de que queramos estimar la probabilidad de que una persona desarrolle cáncer de pulmón (y por lo tanto predecir, por ejemplo, si lo va a desarrollar en los próximos 5 años), podríamos utilizar como factores predictores el tiempo que lleva fumando (una variable continua), la cantidad de cigarrillos (una variable discreta), y si tiene o no antecedentes familiares (una categoría con dos posibles valores).&lt;/p&gt;

&lt;p&gt;Todos esos factores se incorporarían al modelo de la siguiente forma. Sea &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; la variable respuesta. Para una persona (muestra) &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, existen dos posibles valores:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i = 1 \rightarrow \text{  la persona desarrollará la enfermedad}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y_i = 0 \rightarrow \text{  la persona no desarrollará la enfermedad}&lt;/script&gt;

&lt;p&gt;El conjunto de variables predictoras lo modelaremos a través de &lt;script type=&quot;math/tex&quot;&gt;X = (X_1, X_2, ..., X_k)&lt;/script&gt;, donde &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; es el valor observado para la persona (muestra) &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. Para simplificar el modelo, nos centraremos en una única variable predictora &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;. El modelo intentará encontrar la probabilidad de que la persona desarrolle la enfermedad (&lt;script type=&quot;math/tex&quot;&gt;Y_i=1&lt;/script&gt;) para un valor predictor concreto (&lt;script type=&quot;math/tex&quot;&gt;X_i=x_i&lt;/script&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_i=Pr(Y_i=1|X_i=x_i) \text{  (1)}&lt;/script&gt;

&lt;h3 id=&quot;la-función-logística&quot;&gt;La función logística&lt;/h3&gt;

&lt;p&gt;La &lt;a href=&quot;https://es.wikipedia.org/wiki/Funci%C3%B3n_log%C3%ADstica&quot;&gt;función logística&lt;/a&gt; es la curva definida por la siguiente ecuación:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)=\dfrac{L}{1+e^{-k(x-x_0)}}&lt;/script&gt;

&lt;p&gt;Un caso especial de esta función es la &lt;a href=&quot;https://es.wikipedia.org/wiki/Funci%C3%B3n_sigmoide&quot;&gt;función sigmoidea&lt;/a&gt;, donde &lt;script type=&quot;math/tex&quot;&gt;L=1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;k=1&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;x_0=0&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)=\dfrac{1}{1+e^{-x}} \text{  (2)}&lt;/script&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Función sigmoidea sacada de la wikipedia&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png&quot; /&gt;
        &lt;figcaption&gt;Función sigmoidea sacada de la wikipedia&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Este tipo de función tiene múltiples usos en distintas disciplinas científicas. En nuestro caso, nos interesa porque los valores de &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; tienen a &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; o &lt;script type=&quot;math/tex&quot;&gt;-L&lt;/script&gt; según &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; tiene a &lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt; o &lt;script type=&quot;math/tex&quot;&gt;-\infty&lt;/script&gt;. Si &lt;script type=&quot;math/tex&quot;&gt;L=1&lt;/script&gt;, se puede interpretar que esta funcion asigna una probabilidad a un suceso en función de &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. En el caso de una red neuronal, la entrada neta al sistema no será &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, sino &lt;script type=&quot;math/tex&quot;&gt;z=w_0x_0+w_1x_1&lt;/script&gt; (para el caso de una única variable predictora), que podemos simplificar como &lt;script type=&quot;math/tex&quot;&gt;\beta_0 + \beta_1x&lt;/script&gt;. Por tanto, de &lt;script type=&quot;math/tex&quot;&gt;\text{(1)}&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;\text{(2)}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr(Y_i=1|Z_i=z_i)=\phi(z)=\dfrac{1}{1+e^{-z}}&lt;/script&gt;

&lt;p&gt;y multiplicando arriba y abajo por &lt;script type=&quot;math/tex&quot;&gt;e^z&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(z)=\dfrac{e^z}{1+e^z}&lt;/script&gt;

&lt;p&gt;Como los valores de &lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt; van a variar entre 0 y 1, podemos establecer como regla para categorizar muestras que la categoría predicha &lt;script type=&quot;math/tex&quot;&gt;\hat y&lt;/script&gt; valdrá:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat y = \begin{cases}
\text{1 si } \phi(z) \ge 0.5 \cr
\text{-1 en otro caso}
\end{cases}&lt;/script&gt;

&lt;p&gt;Utilizaremos estas ideas en la siguiente entrada para plantear el algoritmo de regresión lineal.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Seguimos con algunos conceptos estadísticos útiles para estudiar el modelo de regresión logística.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">PML T3 - Regresión lineal</title><link href="https://www.javiercancela.com/2017/05/14/python-machine-learning-vi/" rel="alternate" type="text/html" title="PML T3 - Regresión lineal" /><published>2017-05-14T03:00:00+02:00</published><updated>2017-05-14T03:00:00+02:00</updated><id>https://www.javiercancela.com/2017/05/14/python-machine-learning-vi</id><content type="html" xml:base="https://www.javiercancela.com/2017/05/14/python-machine-learning-vi/">&lt;p&gt;La regresión logística es un modelo de clasificación que permite superar algunos de los problemas del perceptrón, como su incapacidad para converger si las clases no son linearmente separables. Para estudiarlo, empezaremos con un pequeño recordatorio de los conceptos estadísticos involucrados.&lt;/p&gt;

&lt;h2 id=&quot;análisis-por-regresión&quot;&gt;Análisis por regresión&lt;/h2&gt;

&lt;p&gt;El análisis por regresión es un proceso que permite estimar la relación entre dos o más variables. En este proceso se parte de una o más variables conocidas (variables independientes o predictoras) y se busca construir un modelo que prediga el comportamiento de otra variable desconocida, llamada variable dependiente. Típicamente se busca calcular el valor medio de la variable dependiente para unos valores fijos de las variables independientes.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Un ejemplo de regresión lineal sacado de la wikipedia&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg&quot; /&gt;
        &lt;figcaption&gt;Un ejemplo de regresión lineal sacado de la wikipedia&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;regresión-lineal&quot;&gt;Regresión lineal&lt;/h3&gt;

&lt;p&gt;Dependiendo de la naturaleza de las variables involucradas se pueden llevar a cabo análisis por regresión de distintos tipos. Uno de los más habituales es la &lt;a href=&quot;https://es.wikipedia.org/wiki/Regresión_lineal&quot;&gt;regresión lineal&lt;/a&gt;. En este tipo de análisis se busca una relación lineal entre la variables:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y} = \mathbf{X}\beta + \epsilon&lt;/script&gt;

&lt;p&gt;Un ejemplo de regresión lineal sería el siguiente: imaginemos un curso universitario que tiene un examen final puntuado de 0 a 10. Para ese examen tenemos la lista de notas sacadas por los alumnos en los últimos años, así como el número de horas que cada alumno empleó preparando la asignatura:&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura&quot; src=&quot;/images/pml/3_reg_lineal_1.png&quot; /&gt;
        &lt;figcaption&gt;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;En este caso tenemos dos variables: las horas empleadas en el estudio (el predictor), y la nota conseguida en el alcance (la respuesta). Llamamos predictor a la primera variable porque la regresión lineal nos permitirá calcular una función (lineal) que prediga la nota de un estudiante a partir de las horas estudiadas, con un cierto margen de error. Este ejemplo constituye además una regresión lineal simple, ya que sólo tiene una variable predictora.&lt;/p&gt;

&lt;p&gt;El propósito de la regresión lineal es obtener una línea recta que, en cada punto del eje &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; (para cada valor del predictor) tenga un valor medio sobre los datos reales de la muestra. Como es lógico, esto no es posible para cada conjunto de datos. Existen una serie de condiciones que los datos deben cumplir para que esto sea posible:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;La media de las respuestas debe ser una función lineal de la variable predictora&lt;/li&gt;
  &lt;li&gt;Los errores de cada muestra (la cantidad que cada muestra se desvía sobre esta media) deben ser independientes&lt;/li&gt;
  &lt;li&gt;Los errores de las muestras con un mismo predictor deben seguir una distribución normal&lt;/li&gt;
  &lt;li&gt;Los errores de las muestras de cada predictor deben tener la misma varianza&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Calcular la regresión lineal consiste en calcular los valores &lt;script type=&quot;math/tex&quot;&gt;b_0&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; tales que&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_i=b_0+b_1x_i&lt;/script&gt;

&lt;p&gt;donde &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; es el predictor de la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; es el valor de la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, y &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; es el valor esperado (según la regresión calculada) de la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. El cálculo de &lt;script type=&quot;math/tex&quot;&gt;b_0&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; se realiza minimizando el error de cada error &lt;script type=&quot;math/tex&quot;&gt;e_i=y_i-\hat{y}_i&lt;/script&gt;. Una forma de conseguir esto, como ya vimos anteriormente, es minimizar la suma del cuadrado de los errores. Es decir, minimizamos&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q=\sum_{i=1}^{n}(y_i-(b_0+b_1x_i))^2,&lt;/script&gt;

&lt;p&gt;lo que nos da&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_0=\bar{y}-b_1\bar{x}&lt;/script&gt;

&lt;p&gt;y&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_1=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}.&lt;/script&gt;

&lt;p&gt;En estas ecuaciones &lt;script type=&quot;math/tex&quot;&gt;\bar{y}&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; representan la media de los valores &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;, respectivamente.&lt;/p&gt;

&lt;p&gt;Para nuestro ejemplo, la recta resultante es&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_i=2.37+0.39x_i&lt;/script&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura, y regresión calculada&quot; src=&quot;/images/pml/3_reg_lineal_2.png&quot; /&gt;
        &lt;figcaption&gt;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura, y regresión calculada&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">La regresión logística es un modelo de clasificación que permite superar algunos de los problemas del perceptrón, como su incapacidad para converger si las clases no son linearmente separables. Para estudiarlo, empezaremos con un pequeño recordatorio de los conceptos estadísticos involucrados.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Notas rápidas de la semana</title><link href="https://www.javiercancela.com/2017/05/13/autenticacion-en-dos-pasos-y-otras-cosas/" rel="alternate" type="text/html" title="Notas rápidas de la semana" /><published>2017-05-13T03:00:00+02:00</published><updated>2017-05-13T03:00:00+02:00</updated><id>https://www.javiercancela.com/2017/05/13/autenticacion-en-dos-pasos-y-otras-cosas</id><content type="html" xml:base="https://www.javiercancela.com/2017/05/13/autenticacion-en-dos-pasos-y-otras-cosas/">&lt;p&gt;Algunas notas rápidas sobre cosas que he visto esta semana.&lt;/p&gt;

&lt;h2 id=&quot;nueva-autenticación-en-dos-pasos-de-google&quot;&gt;Nueva autenticación en dos pasos de Google&lt;/h2&gt;

&lt;p&gt;La autenticación en dos pasos con las cuentas de Google solicita en el primer paso usuario y contraseña, y después envía al móvil un código que se debe introducir en el segundo paso.&lt;/p&gt;

&lt;p&gt;Esta mañana se me ha activado la versión simplificada del mecanismo. Tras preguntarme, durante el login en el ordenador, si quería probarlo, el segundo paso se ha convertido en una simple pregunta en el móvil:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Sin necesidad de mensajes SMS&quot; src=&quot;https://2.bp.blogspot.com/-qFPA-JLCm9E/V5ujKPWvyzI/AAAAAAAAEyw/riCGK9YWu8UUGYl-9NElLIBPCidKTTJcACLcB/s1600/Prompt.png&quot; /&gt;
        &lt;figcaption&gt;Sin necesidad de mensajes SMS&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Nada de SMSs ni códigos adicionales: simplemente pulsando un botón en el móvil verificamos que el login es legítimo. No sé si es más seguro, pero desde luego es más cómodo.&lt;/p&gt;

&lt;h2 id=&quot;mashup&quot;&gt;¿&lt;em&gt;Mashup&lt;/em&gt;?&lt;/h2&gt;

&lt;p&gt;Buscando información sobre la api de Google Maps me vino a la cabeza la palabra &lt;a href=&quot;https://en.wikipedia.org/wiki/Mashup_(web_application_hybrid)&quot;&gt;&lt;em&gt;mashup&lt;/em&gt;&lt;/a&gt;, que se utilizaba hace unos años para describir las aplicaciones que mezclan en una aplicación datos obtenidos de otras aplicaciones distintas. El término se usó especialmente con aplicaciones que usaban Google Maps para mostrar todo tipo de información geolocalizada.&lt;/p&gt;

&lt;p&gt;Pero mi sensación era que este término dejó de usarse de forma bastante repentina. De hecho, creo que hace años que no me la encuentro es este contexto. Y con motivo, a juzgar por los datos que muestra Google Trends:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Evolución del término mashup en los últimos años&quot; src=&quot;/images/mashup-trend.jpg&quot; /&gt;
        &lt;figcaption&gt;Evolución del término mashup en los últimos años&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;zips-con-javascript-en-gmail&quot;&gt;Zips con javascript en GMail&lt;/h2&gt;

&lt;p&gt;Google no permite descargar desde GMail una zip que contenga javascript. El servidor detecta que el contenido puede ser peligroso y no permite descargarlo.&lt;/p&gt;

&lt;p&gt;Sin embargo, esta limitación parece sólo de la aplicación web. Usando un cliente de correo electrónico como Thunderbird (que abrí para esto por primera vez desde que instalé Ubuntu) el archivo se descarga sin problemas. Eso sí, conectaros vía IMAP si no queréis descargaros todo el correo que tengáis almacenado en el inbox de GMail (en mi caso, todo el personal correo desde el 2008)&lt;/p&gt;

&lt;h2 id=&quot;visual-studio-code&quot;&gt;Visual Studio Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.sublimetext.com/3&quot;&gt;Sublime Text 3&lt;/a&gt; es un editor estupendo: potente, flexible, multiplataforma y extremadamente rápido. Es superior a las otras opciones que he probado para desarrollo front y Python, creación de documentos Markdown, y gestión de archivos de texto en general.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Visual Studio Code, bueno, bonito y barato&quot; src=&quot;/images/vsc.png&quot; /&gt;
        &lt;figcaption&gt;Visual Studio Code, bueno, bonito y barato&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Sorprendentemente, la mejora alternativa viene de la mano de Microsoft: &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Visual Studio Code&lt;/a&gt;. Gratuito, &lt;em&gt;open source&lt;/em&gt;, multiplataforma y rápido. Pese al nombre  se parece más a Sublime Text que al &lt;a href=&quot;https://www.visualstudio.com/es/vs/whatsnew&quot;&gt;Visual Studio tradicional&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Está orientado al desarrollo desde el inicio, así que incluye de serie depurador para diversos lenguajes, autocompletado, integración con Git… Tiene también un sistema extensiones que permite adaptarlo a casi cualquier necesidad.&lt;/p&gt;

&lt;p&gt;Destaca como entorno de desarrollo &lt;em&gt;front&lt;/em&gt;, con depuración nativa de javascript, integración con Chrome, &lt;em&gt;linting&lt;/em&gt; para diversos sabores de javascript, … Pero también es una gran herramienta para Python, Go y &lt;em&gt;scripting&lt;/em&gt; en general. Totalmente recomendable.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Algunas notas rápidas sobre cosas que he visto esta semana.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/varios.jpg" /></entry><entry><title type="html">PML T3 - Introducción a Scikit-learn</title><link href="https://www.javiercancela.com/2017/04/23/python-machine-learning-v/" rel="alternate" type="text/html" title="PML T3 - Introducción a Scikit-learn" /><published>2017-04-23T03:00:00+02:00</published><updated>2017-04-23T03:00:00+02:00</updated><id>https://www.javiercancela.com/2017/04/23/python-machine-learning-v</id><content type="html" xml:base="https://www.javiercancela.com/2017/04/23/python-machine-learning-v/">&lt;p&gt;El propósito del tema 3 es revisar algoritmos comunes de aprendizaje máquina. Las diferencias existentes entre estos algoritmos hacen que cada uno sea más adecuado para unas tareas que para otras. En unos casos el criterio de elección será la naturaleza de nuestros datos (número de características, ruido, separabilidad lineal,…), mientras que en otros nos basaremos en la direfencia de rendimientos.&lt;/p&gt;

&lt;p&gt;Como paso previo el libro realiza una presentación de &lt;a href=&quot;http://scikit-learn.org/stable/index.html&quot;&gt;&lt;em&gt;scikit-learn&lt;/em&gt;&lt;/a&gt;, una librería para Python que incluye varios de los algoritmos más usados en &lt;em&gt;machine learning&lt;/em&gt;. El código de esta librería es abierto y &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn&quot;&gt;está disponible en GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;perceptrón-con-scikit-learn&quot;&gt;Perceptrón con &lt;em&gt;scikit-learn&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Para empezar con la librería, el libro vuelve al Perceptrón con el conjunto de datos Iris. La librería &lt;em&gt;scikit-learn&lt;/em&gt; incluye varios conjuntos de datos, así que usaremos el conjunto Iris incluido en la librería. Dividiremos el conjunto en un grupo de datos para el entrenamiento y otro para la validación.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Cargamos los datos de la librería y hacemos un &lt;em&gt;slice&lt;/em&gt; de dos de las columnas (dos caracterísiticas) para cargar los datos en la matriz &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;. La librería nos permite acceder directamente a las clases reales de cada muestra a través de &lt;code class=&quot;highlighter-rouge&quot;&gt;iris.target&lt;/code&gt;, donde los valores para &lt;em&gt;Iris-Setosa&lt;/em&gt;, &lt;em&gt;Iris-Versicolor&lt;/em&gt; e &lt;em&gt;Iris-Virginica&lt;/em&gt; están almacenados como 0, 1 y 2.&lt;/p&gt;

&lt;p&gt;El método &lt;code class=&quot;highlighter-rouge&quot;&gt;train_test_split&lt;/code&gt; reparte el 30% de los datos (&lt;code class=&quot;highlighter-rouge&quot;&gt;test_size=0.3&lt;/code&gt;) para las pruebas, de forma aleatoria (al fijar un &lt;em&gt;seed&lt;/em&gt; a través de &lt;code class=&quot;highlighter-rouge&quot;&gt;random_state=0&lt;/code&gt; garantizamos que el reparto pseudoaleatorio sea el mismo en cada ejecución). El resultado será una matriz &lt;code class=&quot;highlighter-rouge&quot;&gt;X_test&lt;/code&gt; con 45 elementos, y un array &lt;code class=&quot;highlighter-rouge&quot;&gt;y_test&lt;/code&gt; con las 45 clases correspondientes. &lt;code class=&quot;highlighter-rouge&quot;&gt;X_train&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;y_train&lt;/code&gt; contendrán el resto de los datos para el entrenamiento.&lt;/p&gt;

&lt;h3 id=&quot;escalado-de-características&quot;&gt;Escalado de características&lt;/h3&gt;

&lt;p&gt;Como mencionamos en la entrada anterior, el escalado de características es una técnica básica para la optimización del proceso de entrenamiento. El libro utiliza la clase &lt;code class=&quot;highlighter-rouge&quot;&gt;StandardScaler&lt;/code&gt; para ilustrar esta técnica:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La clase &lt;code class=&quot;highlighter-rouge&quot;&gt;StandardScaler&lt;/code&gt; contiene un método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; que permite estimar los parámetros necesarios para la estandarización. A partir de los datos de entrenamiento proporcionados, el método estimará la media (&lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;) y la &lt;a href=&quot;https://es.wikipedia.org/wiki/Desviaci%C3%B3n_t%C3%ADpica&quot;&gt;desviación estándar&lt;/a&gt; (&lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;). Con ellas el método &lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt; generará nuestros nuevos conjuntos de datos de entrenamiento y de pruebas estandarizados. Tras la estandarización, las características pasan a tener valores positivos y negativos, siguiendo una distribución casi normal centrada en el cero:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.7101884052506424&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.6373128028016599&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5192836530366176&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.4487217993375945&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;la-clase-perceptron-con-one-vs-rest&quot;&gt;La clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; con &lt;em&gt;one-vs.-rest&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Una diferencia de la clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; de &lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt; con la implementada en el capítulo 2 del libro es que la que vamos a ver ahora soporta clasificaciones multiclase. En el capítulo 2 restringimos nuestro conjunto de datos a muestras de solo dos clases, ya que el algoritmo sólo era capaz de distinguir si una muestra pertenecía a una clase o no. Si la función de activación superaba un umbral se asignaba una clase, si no, se asignaba la otra.&lt;/p&gt;

&lt;p&gt;Esta función de activación es inherente al Perceptrón. Sin embargo, aun con ella es posible realizar clasificaciones multiclase. Para ello se utiliza una estrategia llamada ‘Uno contra los demás’ (&lt;em&gt;one-vs.-rest&lt;/em&gt;, &lt;em&gt;OvR&lt;/em&gt;). En esta estrategia se entrena un clasificador para cada clase. Es decir, en nuestro conjunto de datos con tres variantes de Iris, en vez de usar el Perceptrón para entrenar un clasificador que nos diga si una muestra es &lt;em&gt;Iris-setosa&lt;/em&gt; o &lt;em&gt;Iris-versicolor&lt;/em&gt;, entrenaremos un clasificador que nos diga si una muestra es &lt;em&gt;Iris-setosa&lt;/em&gt; o no, otro que nos diga si una muestra es &lt;em&gt;Iris-versicolor&lt;/em&gt; o no, y otro que haga lo mismo con &lt;em&gt;Iris-virginica&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;La clasificación se realiza evaluando cada muestra con cada clasificador. Estos nos dirán si la muestra pertenece a su clase o no. Para poder resolver ambigüedades (una muestra que esté en más de una clase, o que no esté en ninguna), los clasificadores no devuelven una etiqueta, sino una puntuación de confianza que indica cómo de seguro está el clasificador de que una muestra pertenece a la clase evaluada. Así, cuando queremos predecir la clase de una muestra nueva nos quedamos con la clase a la que corresponde la puntuación de confianza más alta.&lt;/p&gt;

&lt;p&gt;El código Python para hacer esto es muy simple:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; admite muchos parámetros en el constructor. Los parámetros &lt;code class=&quot;highlighter-rouge&quot;&gt;n_iter&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;eta0&lt;/code&gt; establecen el número de épocas y la tasa de aprendizaje, como ya vimos en entradas anteriores. Por defecto, la clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; baraja los datos de entrenamiento después de cada época. Para ello usa un generador de números pseudoaleatorios, cuya semilla se puede definir con el parámetro &lt;code class=&quot;highlighter-rouge&quot;&gt;random_state&lt;/code&gt;. Al fijar este parámetro provocamos que los números aleatorios generados sean siempre los mismos en cada ejecución del código, de forma que siempre que los clasificadores obtenidos sean siempre los mismos.&lt;/p&gt;

&lt;p&gt;El &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb&quot;&gt;código del libro&lt;/a&gt; prosigue mostrando los resultados del entrenamiento: de los datos de prueba se clasifican incorrectamente 4 muestras de 45, es decir, un porcentaje de acierto del 91%:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Misclassified samples: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Imprime: Misclassified samples: 4
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Accuracy: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Imprime: Accuracy: 0.91
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para finalizar este apartado, el autor modifica el método &lt;code class=&quot;highlighter-rouge&quot;&gt;plot_decision_regions&lt;/code&gt;, visto en entradas anteriores, para que resalte los datos de prueba con círculos. El código se puede ver &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb&quot;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/images/03_01.png&quot;&gt;
        &lt;img alt=&quot;No es posible realizar una separación perfecta de los tres conjuntos de datos&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch03/images/03_01.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Como se ven en la gráfica, no es posible realizar una separación perfecta de los tres conjuntos de datos (no se puede dibujar una recta que separe todos los círculos verdes de todas las cruces azules)&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">El propósito del tema 3 es revisar algoritmos comunes de aprendizaje máquina. Las diferencias existentes entre estos algoritmos hacen que cada uno sea más adecuado para unas tareas que para otras. En unos casos el criterio de elección será la naturaleza de nuestros datos (número de características, ruido, separabilidad lineal,…), mientras que en otros nos basaremos en la direfencia de rendimientos.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Pausa por vacaciones</title><link href="https://www.javiercancela.com/2017/04/16/pausa-por-vacaciones/" rel="alternate" type="text/html" title="Pausa por vacaciones" /><published>2017-04-16T03:00:00+02:00</published><updated>2017-04-16T03:00:00+02:00</updated><id>https://www.javiercancela.com/2017/04/16/pausa-por-vacaciones</id><content type="html" xml:base="https://www.javiercancela.com/2017/04/16/pausa-por-vacaciones/">&lt;p&gt;Esta semana no hay entrada por vacaciones. Volvemos la semana que viene.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Esta semana no hay entrada por vacaciones. Volvemos la semana que viene.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/vacaciones.jpg" /></entry><entry><title type="html">PML T2 - Gradiente descendente estocástico</title><link href="https://www.javiercancela.com/2017/04/09/python-machine-learning-iv/" rel="alternate" type="text/html" title="PML T2 - Gradiente descendente estocástico" /><published>2017-04-09T03:00:00+02:00</published><updated>2017-04-09T03:00:00+02:00</updated><id>https://www.javiercancela.com/2017/04/09/python-machine-learning-iv</id><content type="html" xml:base="https://www.javiercancela.com/2017/04/09/python-machine-learning-iv/">&lt;p&gt;El siguiente punto del libro nos habla de cómo modificar este algoritmo para adaptarlo a conjuntos con millones de datos. El punto clave está en que la forma del algoritmo que hemos analizado reevalúa todo el conjunto de datos cada vez que se realiza una iteración o época. Si tenemos 1 millón de muestras, por cada época vamos a tener que calcular:
&lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{w} = \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Es decir, para cada época tenemos que realizar un millón de veces:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;el cálculo de la función de costos&lt;/li&gt;
  &lt;li&gt;el cálculo de la resta con la clase real&lt;/li&gt;
  &lt;li&gt;la multiplicación del resultado por la entrada de la muestra&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;y después sumar el millón de resultados.&lt;/p&gt;

&lt;h3 id=&quot;optimizando-el-proceso&quot;&gt;Optimizando el proceso&lt;/h3&gt;

&lt;p&gt;El gradiente descendente estocástico utiliza otra aproximación. En vez de recalcular los pesos usando todas las muestras, utilizamos sólo la muestra de la entrada para el cálculo:
&lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{w} = \eta \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Este cambio tiene dos consecuencias fundamentales Por un lado, los pesos se actualizan con más frecuencia, así que la convergencia suele ser más rápida. Por otro lado, usar una sola muestra da menos precisión a la actualización, así que se introduce más ruido en los pesos. Esto no es necesariamente malo, ya que facilita salir de mínimos locales que no se corresponden con la minimización de nuestra función de costos. Otra ventaja de este algoritmo es que resulta adecuado para el aprendizaje &lt;em&gt;on line&lt;/em&gt;, es decir, aquel en el que el modelo tiene que adaptarse rápidamente a cambios en los datos.&lt;/p&gt;

&lt;p&gt;Una variante mencionada en el libro pero no implementada en el código consiste en sustituir una tasa de aprendizaje fija &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; por una que disminuya con el tiempo:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{c_1}{[\textit {número de iteraciones}] + c_2}&lt;/script&gt;

&lt;h3 id=&quot;código-python&quot;&gt;Código Python&lt;/h3&gt;

&lt;p&gt;Vamos a adaptar la clase Adaline anterior a este nuevo algoritmo:&lt;/p&gt;

&lt;pre class=&quot;line-numbers&quot;&gt;
  &lt;code class=&quot;language-python&quot;&gt;
  from numpy.random import seed

  class AdalineSGD(object):
      def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
          self.eta = eta
          self.n_iter = n_iter
          self.w_initialized = False
          self.shuffle = shuffle
          if random_state:
              seed(random_state)
          
      def fit(self, X, y):
          self._initialize_weights(X.shape[1])
          self.cost_ = []
          for i in range(self.n_iter):
              if self.shuffle:
                  X, y = self._shuffle(X, y)
              cost = []
              for xi, target in zip(X, y):
                  cost.append(self._update_weights(xi, target))
              avg_cost = sum(cost) / len(y)
              self.cost_.append(avg_cost)
          return self

      def partial_fit(self, X, y):
          if not self.w_initialized:
              self._initialize_weights(X.shape[1])
          if y.ravel().shape[0] &amp;gt; 1:
              for xi, target in zip(X, y):
                  self._update_weights(xi, target)
          else:
              self._update_weights(X, y)
          return self

      def _shuffle(self, X, y):
          r = np.random.permutation(len(y))
          return X[r], y[r]
      
      def _initialize_weights(self, m):
          self.w_ = np.zeros(1 + m)
          self.w_initialized = True
          
      def _update_weights(self, xi, target):
          output = self.net_input(xi)
          error = (target - output)
          self.w_[1:] += self.eta * xi.dot(error)
          self.w_[0] += self.eta * error
          cost = 0.5 * error**2
          return cost
      
      def net_input(self, X):
          return np.dot(X, self.w_[1:]) + self.w_[0]

      def activation(self, X):
          return self.net_input(X)

      def predict(self, X):
          return np.where(self.activation(X) &amp;gt;= 0.0, 1, -1)
  &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;En primer lugar, hemos añadido un método &lt;code class=&quot;highlighter-rouge&quot;&gt;_shuffle&lt;/code&gt; para mezclar los datos de la entrada antes de cada iteración. El propósito de este paso es evitar ciclos al minimizar la función de costos, ya que este algoritmo es sensible a este problema.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;El constructor incluye parámetros para indicar si queremos mezclar al azar los datos de entrada y para inicializar una semilla para esta mezcla.&lt;/p&gt;

&lt;p&gt;El método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; es el que se encarga del entrenamiento. Invocamos primero al método que inicializa los pesos a cero. Este método existe para marcar los pesos como inicializados, información que necesitamos para el método de entrenamiento parcial &lt;code class=&quot;highlighter-rouge&quot;&gt;parcial_fit&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Dentro del método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; el código que se itera en cada época comienza comprobando si hay que barajar los datos de entrada, e inicializa el costo que evaluará la función de costos:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A continuación iteramos los datos. La variable &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; es una matriz que contiene una fila por cada muestra, y una columna por cada característica. La variable &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; es un array que contiene un elemento, la clase real, para cada muestra de &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;. El método &lt;code class=&quot;highlighter-rouge&quot;&gt;zip&lt;/code&gt; nos devolverá un elemento de cada variable (&lt;em&gt;zip&lt;/em&gt; en inglés significa, entre otras cosas, cremallera), por lo que en cada iteración del &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; en &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; y en &lt;code class=&quot;highlighter-rouge&quot;&gt;target&lt;/code&gt; se almacenarán las características y la clase real de cada muestra.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para cada muestra, actualizamos los pesos en el método &lt;code class=&quot;highlighter-rouge&quot;&gt;_update_weights&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_update_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para la salida real seguimos utilizando la función identidad, con lo que la &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt; es igual a la entrada. La diferencia en este nuevo algoritmo está en cómo ajustamos los pesos. El array de pesos se ajusta con el producto del error con la muestra procesada, no con toda la matriz de entradas. Lógicamente, el costo calculado se obtendrá del error de esta muestra al cuadrado. El costo lo añadimos al array de costos para representarlo posteriormente.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;avg_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para finalizar el método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; calculamos la media de los costos de cada muestra, y la añadimos al array de costos.&lt;/p&gt;

&lt;h3 id=&quot;resultados&quot;&gt;Resultados&lt;/h3&gt;

&lt;p&gt;Ahora toca usar esta clase. La instanciamos y entrenamos el modelo (el libro usa, en este ejemplo y en el de la entrada anterior, un conjunto de datos escalado para mejorar el rendimiento, y que nosotros revisaremos en el tema 3):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdalineSGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pintamos los resultados, primero mostrando la clasificación de las muestras y después el costo por época.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_decision_regions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adaline - Gradiente descendiente estocástico'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'longitud sépalo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'longitud pétalo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Épocas'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Costo medio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Los datos obtenidos son estos (los resultados son distintos a los obtenidos en el libro, ya que nosotros no hemos estandarizado los valores de las muestras):&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados&quot; src=&quot;/images/pml/2_adaline.png&quot; /&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_14.png&quot;&gt;&lt;img alt=&quot;Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_14.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Nuestra convergencia es mucha más rápida, pero también muy variable, ya que a partir de la sexta época el costo empieza a oscilar.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">El siguiente punto del libro nos habla de cómo modificar este algoritmo para adaptarlo a conjuntos con millones de datos. El punto clave está en que la forma del algoritmo que hemos analizado reevalúa todo el conjunto de datos cada vez que se realiza una iteración o época. Si tenemos 1 millón de muestras, por cada época vamos a tener que calcular:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry></feed>
