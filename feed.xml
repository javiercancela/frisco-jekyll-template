<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="https://www.javiercancela.com/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="https://www.javiercancela.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.javiercancela.com/" rel="alternate" type="text/html" /><updated>2017-05-19T10:49:53+00:00</updated><id>https://www.javiercancela.com//</id><title type="html">Página de Javier Cancela</title><subtitle>Página personal de Javier Cancela</subtitle><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><entry><title type="html">Machine Learning con Python - Tema 3 - Regresión lineal</title><link href="https://www.javiercancela.com/2017/05/14/python-machine-learning-vi/" rel="alternate" type="text/html" title="Machine Learning con Python - Tema 3 - Regresión lineal" /><published>2017-05-14T01:00:00+00:00</published><updated>2017-05-14T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/05/14/python-machine-learning-vi</id><content type="html" xml:base="https://www.javiercancela.com/2017/05/14/python-machine-learning-vi/">&lt;p&gt;La regresión logística es un modelo de clasificación que permite superar algunos de los problemas del perceptrón, como su incapacidad para converger si las clases no son linearmente separables. Para estudiarlo, empezaremos con un pequeño recordatorio sobre los conceptos estadísticos involucrados.&lt;/p&gt;

&lt;h2 id=&quot;análisis-por-regresión&quot;&gt;Análisis por regresión&lt;/h2&gt;

&lt;p&gt;El análisis por regresión es un proceso que permite estimar la relación entre dos o más variables. En este proceso se parte de una o más variables conocidas (variables independientes o predictoras) y se busca construir un modelo que prediga el comportamiento de otra variable desconocida, llamada variable dependiente. Típicamente se busca calcular el valor medio de la variable dependiente para unos valores fijos de las variables independientes.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Un ejemplo de regresión lineal sacado de la wikipedia&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg&quot; /&gt;
        &lt;figcaption&gt;Un ejemplo de regresión lineal sacado de la wikipedia&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;regresión-lineal&quot;&gt;Regresión lineal&lt;/h3&gt;

&lt;p&gt;Dependiendo de la naturaleza de las variables involucradas se pueden llevar a cabo análisis por regresión de distintos tipos. Uno de los más habituales es la &lt;a href=&quot;https://es.wikipedia.org/wiki/Regresión_lineal&quot;&gt;regresión lineal&lt;/a&gt;. En este tipo de análisis se busca una relación lineal entre la variables:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y} = \mathbf{X}\beta + \epsilon&lt;/script&gt;

&lt;p&gt;Un ejemplo de regresión lineal sería el siguiente: imaginemos un curso universitario que tiene un examen final puntuado de 0 a 10. Para ese examen tenemos la lista de notas sacadas por los alumnos en los últimos años, así como el número de horas que cada alumno empleó preparando la asignatura:&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura&quot; src=&quot;/images/pml/3_reg_lineal_1.png&quot; /&gt;
        &lt;figcaption&gt;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;En este caso tenemos dos variables: las horas empleadas en el estudio (el predictor), y la nota conseguida en el alcance (la respuesta). Llamamos predictor a la primera variable porque la regresión lineal nos permitirá calcular una función (lineal) que prediga la nota de un estudiante a partir de las horas estudiadas, con un cierto margen de error. Este ejemplo constituye además una regresión lineal simple, ya que sólo tiene una variable predictora.&lt;/p&gt;

&lt;p&gt;El propósito de la regresión lineal es obtener una línea recta que, en cada punto del eje &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; (para cada valor del predictor) tenga un valor medio sobre los datos reales de la muestra. Como es lógico, esto no es posible para cada conjunto de datos. Existen una serie de condiciones que los datos deben cumplir para que esto sea posible:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;La media de las respuestas debe ser una función lineal de la variable predictora&lt;/li&gt;
  &lt;li&gt;Los errores de cada muestra (la cantidad que cada muestra se desvía sobre esta media) deben ser independientes&lt;/li&gt;
  &lt;li&gt;Los errores de las muestras con un mismo predictor deben seguir una distribución normal&lt;/li&gt;
  &lt;li&gt;Los errores de las muestras de cada predictor deben tener la misma varianza&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Calcular la regresión lineal consiste en calcular los valores &lt;script type=&quot;math/tex&quot;&gt;b_0&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; tales que&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_i=b_0+b_1x_i&lt;/script&gt;

&lt;p&gt;donde &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; es el predictor de la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; es el valor de la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, y &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; es el valor esperado (según la regresión calculada) de la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. El cálculo de &lt;script type=&quot;math/tex&quot;&gt;b_0&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; se realiza minimizando el error de cada error &lt;script type=&quot;math/tex&quot;&gt;e_i=y_i-\hat{y}_i&lt;/script&gt;. Una forma de conseguir esto, como ya vimos anteriormente, es minimizar la suma del cuadrado de los errores. Es decir, minimizamos&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q=\sum_{i=1}^{n}(y_i-(b_0+b_1x_i))^2,&lt;/script&gt;

&lt;p&gt;lo que nos da&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_0=\bar{y}-b_1\bar{x}&lt;/script&gt;

&lt;p&gt;y&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_1=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}.&lt;/script&gt;

&lt;p&gt;En estas ecuaciones &lt;script type=&quot;math/tex&quot;&gt;\bar{y}&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; representan la media de los valores &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;, respectivamente.&lt;/p&gt;

&lt;p&gt;Para nuestro ejemplo, la recta resultante es&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_i=2.37+0.39x_i&lt;/script&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura, y regresión calculada&quot; src=&quot;/images/pml/3_reg_lineal_2.png&quot; /&gt;
        &lt;figcaption&gt;Datos de horas estudiadas y notas conseguidas para 50 alumnos de una misma asignatura, y regresión calculada&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">La regresión logística es un modelo de clasificación que permite superar algunos de los problemas del perceptrón, como su incapacidad para converger si las clases no son linearmente separables. Para estudiarlo, empezaremos con un pequeño recordatorio sobre los conceptos estadísticos involucrados.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Notas rápidas de la semana</title><link href="https://www.javiercancela.com/2017/05/13/autenticacion-en-dos-pasos-y-otras-cosas/" rel="alternate" type="text/html" title="Notas rápidas de la semana" /><published>2017-05-13T01:00:00+00:00</published><updated>2017-05-13T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/05/13/autenticacion-en-dos-pasos-y-otras-cosas</id><content type="html" xml:base="https://www.javiercancela.com/2017/05/13/autenticacion-en-dos-pasos-y-otras-cosas/">&lt;p&gt;Algunas notas rápidas sobre cosas que he visto esta semana.&lt;/p&gt;

&lt;h2 id=&quot;nueva-autenticación-en-dos-pasos-de-google&quot;&gt;Nueva autenticación en dos pasos de Google&lt;/h2&gt;

&lt;p&gt;La autenticación en dos pasos con las cuentas de Google solicita en el primer paso usuario y contraseña, y después envía al móvil un código que se debe introducir en el segundo paso.&lt;/p&gt;

&lt;p&gt;Esta mañana se me ha activado la versión simplificada del mecanismo. Tras preguntarme, durante el login en el ordenador, si quería probarlo, el segundo paso se ha convertido en una simple pregunta en el móvil:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Sin necesidad de mensajes SMS&quot; src=&quot;https://2.bp.blogspot.com/-qFPA-JLCm9E/V5ujKPWvyzI/AAAAAAAAEyw/riCGK9YWu8UUGYl-9NElLIBPCidKTTJcACLcB/s1600/Prompt.png&quot; /&gt;
        &lt;figcaption&gt;Sin necesidad de mensajes SMS&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Nada de SMSs ni códigos adicionales: simplemente pulsando un botón en el móvil verificamos que el login es legítimo. No sé si es más seguro, pero desde luego es más cómodo.&lt;/p&gt;

&lt;h2 id=&quot;mashup&quot;&gt;¿&lt;em&gt;Mashup&lt;/em&gt;?&lt;/h2&gt;

&lt;p&gt;Buscando información sobre la api de Google Maps me vino a la cabeza la palabra &lt;a href=&quot;https://en.wikipedia.org/wiki/Mashup_(web_application_hybrid)&quot;&gt;&lt;em&gt;mashup&lt;/em&gt;&lt;/a&gt;, que se utilizaba hace unos años para describir las aplicaciones que mezclan en una aplicación datos obtenidos de otras aplicaciones distintas. El término se usó especialmente con aplicaciones que usaban Google Maps para mostrar todo tipo de información geolocalizada.&lt;/p&gt;

&lt;p&gt;Pero mi sensación era que este término dejó de usarse de forma bastante repentina. De hecho, creo que hace años que no me la encuentro es este contexto. Y con motivo, a juzgar por los datos que muestra Google Trends:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Evolución del término mashup en los últimos años&quot; src=&quot;/images/mashup-trend.jpg&quot; /&gt;
        &lt;figcaption&gt;Evolución del término mashup en los últimos años&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;zips-con-javascript-en-gmail&quot;&gt;Zips con javascript en GMail&lt;/h2&gt;

&lt;p&gt;Google no permite descargar desde GMail una zip que contenga javascript. El servidor detecta que el contenido puede ser peligroso y no permite descargarlo.&lt;/p&gt;

&lt;p&gt;Sin embargo, esta limitación parece sólo de la aplicación web. Usando un cliente de correo electrónico como Thunderbird (que abrí para esto por primera vez desde que instalé Ubuntu) el archivo se descarga sin problemas. Eso sí, conectaros vía IMAP si no queréis descargaros todo el correo que tengáis almacenado en el inbox de GMail (en mi caso, todo el personal correo desde el 2008)&lt;/p&gt;

&lt;h2 id=&quot;visual-studio-code&quot;&gt;Visual Studio Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.sublimetext.com/3&quot;&gt;Sublime Text 3&lt;/a&gt; es un editor estupendo: potente, flexible, multiplataforma y extremadamente rápido. Es superior a las otras opciones que he probado para desarrollo front y Python, creación de documentos Markdown, y gestión de archivos de texto en general.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Visual Studio Code, bueno, bonito y barato&quot; src=&quot;/images/vsc.png&quot; /&gt;
        &lt;figcaption&gt;Visual Studio Code, bueno, bonito y barato&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Sorprendentemente, la mejora alternativa viene de la mano de Microsoft: &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Visual Studio Code&lt;/a&gt;. Gratuito, &lt;em&gt;open source&lt;/em&gt;, multiplataforma y rápido. Pese al nombre  se parece más a Sublime Text que al &lt;a href=&quot;https://www.visualstudio.com/es/vs/whatsnew&quot;&gt;Visual Studio tradicional&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Está orientado al desarrollo desde el inicio, así que incluye de serie depurador para diversos lenguajes, autocompletado, integración con Git… Tiene también un sistema extensiones que permite adaptarlo a casi cualquier necesidad.&lt;/p&gt;

&lt;p&gt;Destaca como entorno de desarrollo &lt;em&gt;front&lt;/em&gt;, con depuración nativa de javascript, integración con Chrome, &lt;em&gt;linting&lt;/em&gt; para diversos sabores de javascript, … Pero también es una gran herramienta para Python, Go y &lt;em&gt;scripting&lt;/em&gt; en general. Totalmente recomendable.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Algunas notas rápidas sobre cosas que he visto esta semana.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/varios.jpg" /></entry><entry><title type="html">Machine Learning con Python - Tema 3 - Introducción a Scikit-learn</title><link href="https://www.javiercancela.com/2017/04/23/python-machine-learning-v/" rel="alternate" type="text/html" title="Machine Learning con Python - Tema 3 - Introducción a Scikit-learn" /><published>2017-04-23T01:00:00+00:00</published><updated>2017-04-23T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/04/23/python-machine-learning-v</id><content type="html" xml:base="https://www.javiercancela.com/2017/04/23/python-machine-learning-v/">&lt;p&gt;El propósito del tema 3 es revisar algoritmos comunes de aprendizaje máquina. Las diferencias existentes entre estos algoritmos hacen que cada uno sea más adecuado para unas tareas que para otras. En unos casos el criterio de elección será la naturaleza de nuestros datos (número de características, ruido, separabilidad lineal,…), mientras que en otros nos basaremos en la direfencia de rendimientos.&lt;/p&gt;

&lt;p&gt;Como paso previo el libro realiza una presentación de &lt;a href=&quot;http://scikit-learn.org/stable/index.html&quot;&gt;&lt;em&gt;scikit-learn&lt;/em&gt;&lt;/a&gt;, una librería para Python que incluye varios de los algoritmos más usados en &lt;em&gt;machine learning&lt;/em&gt;. El código de esta librería es abierto y &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn&quot;&gt;está disponible en GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;perceptrón-con-scikit-learn&quot;&gt;Perceptrón con &lt;em&gt;scikit-learn&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Para empezar con la librería, el libro vuelve al Perceptrón con el conjunto de datos Iris. La librería &lt;em&gt;scikit-learn&lt;/em&gt; incluye varios conjuntos de datos, así que usaremos el conjunto Iris incluido en la librería. Dividiremos el conjunto en un grupo de datos para el entrenamiento y otro para la validación.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Cargamos los datos de la librería y hacemos un &lt;em&gt;slice&lt;/em&gt; de dos de las columnas (dos caracterísiticas) para cargar los datos en la matriz &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;. La librería nos permite acceder directamente a las clases reales de cada muestra a través de &lt;code class=&quot;highlighter-rouge&quot;&gt;iris.target&lt;/code&gt;, donde los valores para &lt;em&gt;Iris-Setosa&lt;/em&gt;, &lt;em&gt;Iris-Versicolor&lt;/em&gt; e &lt;em&gt;Iris-Virginica&lt;/em&gt; están almacenados como 0, 1 y 2.&lt;/p&gt;

&lt;p&gt;El método &lt;code class=&quot;highlighter-rouge&quot;&gt;train_test_split&lt;/code&gt; reparte el 30% de los datos (&lt;code class=&quot;highlighter-rouge&quot;&gt;test_size=0.3&lt;/code&gt;) para las pruebas, de forma aleatoria (al fijar un &lt;em&gt;seed&lt;/em&gt; a través de &lt;code class=&quot;highlighter-rouge&quot;&gt;random_state=0&lt;/code&gt; garantizamos que el reparto pseudoaleatorio sea el mismo en cada ejecución). El resultado será una matriz &lt;code class=&quot;highlighter-rouge&quot;&gt;X_test&lt;/code&gt; con 45 elementos, y un array &lt;code class=&quot;highlighter-rouge&quot;&gt;y_test&lt;/code&gt; con las 45 clases correspondientes. &lt;code class=&quot;highlighter-rouge&quot;&gt;X_train&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;y_train&lt;/code&gt; contendrán el resto de los datos para el entrenamiento.&lt;/p&gt;

&lt;h3 id=&quot;escalado-de-características&quot;&gt;Escalado de características&lt;/h3&gt;

&lt;p&gt;Como mencionamos en la entrada anterior, el escalado de características es una técnica básica para la optimización del proceso de entrenamiento. El libro utiliza la clase &lt;code class=&quot;highlighter-rouge&quot;&gt;StandardScaler&lt;/code&gt; para ilustrar esta técnica:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;La clase &lt;code class=&quot;highlighter-rouge&quot;&gt;StandardScaler&lt;/code&gt; contiene un método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; que permite estimar los parámetros necesarios para la estandarización. A partir de los datos de entrenamiento proporcionados, el método estimará la media (&lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;) y la &lt;a href=&quot;https://es.wikipedia.org/wiki/Desviaci%C3%B3n_t%C3%ADpica&quot;&gt;desviación estándar&lt;/a&gt; (&lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;). Con ellas el método &lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt; generará nuestros nuevos conjuntos de datos de entrenamiento y de pruebas estandarizados. Tras la estandarización, las características pasan a tener valores positivos y negativos, siguiendo una distribución casi normal centrada en el cero:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.7101884052506424&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.6373128028016599&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.5192836530366176&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.4487217993375945&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;la-clase--con-one-vs-rest&quot;&gt;La clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; con &lt;em&gt;one-vs.-rest&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Una diferencia de la clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; de &lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt; con la implementada en el capítulo 2 del libro es que la que vamos a ver ahora soporta clasificaciones multiclase. En el capítulo 2 restringimos nuestro conjunto de datos a muestras de solo dos clases, ya que el algoritmo sólo era capaz de distinguir si una muestra pertenecía a una clase o no. Si la función de activación superaba un umbral se asignaba una clase, si no, se asignaba la otra.&lt;/p&gt;

&lt;p&gt;Esta función de activación es inherente al Perceptrón. Sin embargo, aun con ella es posible realizar clasificaciones multiclase. Para ello se utiliza una estrategia llamada ‘Uno contra los demás’ (&lt;em&gt;one-vs.-rest&lt;/em&gt;, &lt;em&gt;OvR&lt;/em&gt;). En esta estrategia se entrena un clasificador para cada clase. Es decir, en nuestro conjunto de datos con tres variantes de Iris, en vez de usar el Perceptrón para entrenar un clasificador que nos diga si una muestra es &lt;em&gt;Iris-setosa&lt;/em&gt; o &lt;em&gt;Iris-versicolor&lt;/em&gt;, entrenaremos un clasificador que nos diga si una muestra es &lt;em&gt;Iris-setosa&lt;/em&gt; o no, otro que nos diga si una muestra es &lt;em&gt;Iris-versicolor&lt;/em&gt; o no, y otro que haga lo mismo con &lt;em&gt;Iris-virginica&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;La clasificación se realiza evaluando cada muestra con cada clasificador. Estos nos dirán si la muestra pertenece a su clase o no. Para poder resolver ambigüedades (una muestra que esté en más de una clase, o que no esté en ninguna), los clasificadores no devuelven una etiqueta, sino una puntuación de confianza que indica cómo de seguro está el clasificador de que una muestra pertenece a la clase evaluada. Así, cuando queremos predecir la clase de una muestra nueva nos quedamos con la clase a la que corresponde la puntuación de confianza más alta.&lt;/p&gt;

&lt;p&gt;El código Python para hacer esto es muy simple:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;La clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; admite muchos parámetros en el constructor. Los parámetros &lt;code class=&quot;highlighter-rouge&quot;&gt;n_iter&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;eta0&lt;/code&gt; establecen el número de épocas y la tasa de aprendizaje, como ya vimos en entradas anteriores. Por defecto, la clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; baraja los datos de entrenamiento después de cada época. Para ello usa un generador de números pseudoaleatorios, cuya semilla se puede definir con el parámetro &lt;code class=&quot;highlighter-rouge&quot;&gt;random_state&lt;/code&gt;. Al fijar este parámetro provocamos que los números aleatorios generados sean siempre los mismos en cada ejecución del código, de forma que siempre que los clasificadores obtenidos sean siempre los mismos.&lt;/p&gt;

&lt;p&gt;El &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb&quot;&gt;código del libro&lt;/a&gt; prosigue mostrando los resultados del entrenamiento: de los datos de prueba se clasifican incorrectamente 4 muestras de 45, es decir, un porcentaje de acierto del 91%:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Misclassified samples: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Imprime: Misclassified samples: 4&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Accuracy: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Imprime: Accuracy: 0.91&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Para finalizar este apartado, el autor modifica el método &lt;code class=&quot;highlighter-rouge&quot;&gt;plot_decision_regions&lt;/code&gt;, visto en entradas anteriores, para que resalte los datos de prueba con círculos. El código se puede ver &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/ch03.ipynb&quot;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch03/images/03_01.png&quot;&gt;
        &lt;img alt=&quot;No es posible realizar una separación perfecta de los tres conjuntos de datos&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch03/images/03_01.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Como se ven en la gráfica, no es posible realizar una separación perfecta de los tres conjuntos de datos (no se puede dibujar una recta que separe todos los círculos verdes de todas las cruces azules)&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">El propósito del tema 3 es revisar algoritmos comunes de aprendizaje máquina. Las diferencias existentes entre estos algoritmos hacen que cada uno sea más adecuado para unas tareas que para otras. En unos casos el criterio de elección será la naturaleza de nuestros datos (número de características, ruido, separabilidad lineal,…), mientras que en otros nos basaremos en la direfencia de rendimientos.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Pausa por vacaciones</title><link href="https://www.javiercancela.com/2017/04/16/pausa-por-vacaciones/" rel="alternate" type="text/html" title="Pausa por vacaciones" /><published>2017-04-16T01:00:00+00:00</published><updated>2017-04-16T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/04/16/pausa-por-vacaciones</id><content type="html" xml:base="https://www.javiercancela.com/2017/04/16/pausa-por-vacaciones/">&lt;p&gt;Esta semana no hay entrada por vacaciones. Volvemos la semana que viene.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Esta semana no hay entrada por vacaciones. Volvemos la semana que viene.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/vacaciones.jpg" /></entry><entry><title type="html">Machine Learning con Python - Tema 2 - Gradiente descendente estocástico</title><link href="https://www.javiercancela.com/2017/04/09/python-machine-learning-iv/" rel="alternate" type="text/html" title="Machine Learning con Python - Tema 2 - Gradiente descendente estocástico" /><published>2017-04-09T01:00:00+00:00</published><updated>2017-04-09T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/04/09/python-machine-learning-iv</id><content type="html" xml:base="https://www.javiercancela.com/2017/04/09/python-machine-learning-iv/">&lt;p&gt;El siguiente punto del libro nos habla de cómo modificar este algoritmo para adaptarlo a conjuntos con millones de datos. El punto clave está en que la forma del algoritmo que hemos analizado reevalúa todo el conjunto de datos cada vez que se realiza una iteración o época. Si tenemos 1 millón de muestras, por cada época vamos a tener que calcular:
&lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{w} = \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Es decir, para cada época tenemos que realizar un millón de veces:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;el cálculo de la función de costos&lt;/li&gt;
  &lt;li&gt;el cálculo de la resta con la clase real&lt;/li&gt;
  &lt;li&gt;la multiplicación del resultado por la entrada de la muestra&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;y después sumar el millón de resultados.&lt;/p&gt;

&lt;h3 id=&quot;optimizando-el-proceso&quot;&gt;Optimizando el proceso&lt;/h3&gt;

&lt;p&gt;El gradiente descendente estocástico utiliza otra aproximación. En vez de recalcular los pesos usando todas las muestras, utilizamos sólo la muestra de la entrada para el cálculo:
&lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{w} = \eta \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Este cambio tiene dos consecuencias fundamentales Por un lado, los pesos se actualizan con más frecuencia, así que la convergencia suele ser más rápida. Por otro lado, usar una sola muestra da menos precisión a la actualización, así que se introduce más ruido en los pesos. Esto no es necesariamente malo, ya que facilita salir de mínimos locales que no se corresponden con la minimización de nuestra función de costos. Otra ventaja de este algoritmo es que resulta adecuado para el aprendizaje &lt;em&gt;on line&lt;/em&gt;, es decir, aquel en el que el modelo tiene que adaptarse rápidamente a cambios en los datos.&lt;/p&gt;

&lt;p&gt;Una variante mencionada en el libro pero no implementada en el código consiste en sustituir una tasa de aprendizaje fija &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; por una que disminuya con el tiempo:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{c_1}{[\textit {número de iteraciones}] + c_2}&lt;/script&gt;

&lt;h3 id=&quot;código-python&quot;&gt;Código Python&lt;/h3&gt;

&lt;p&gt;Vamos a adaptar la clase Adaline anterior a este nuevo algoritmo:&lt;/p&gt;

&lt;pre class=&quot;line-numbers&quot;&gt;
  &lt;code class=&quot;language-python&quot;&gt;
  from numpy.random import seed

  class AdalineSGD(object):
      def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
          self.eta = eta
          self.n_iter = n_iter
          self.w_initialized = False
          self.shuffle = shuffle
          if random_state:
              seed(random_state)
          
      def fit(self, X, y):
          self._initialize_weights(X.shape[1])
          self.cost_ = []
          for i in range(self.n_iter):
              if self.shuffle:
                  X, y = self._shuffle(X, y)
              cost = []
              for xi, target in zip(X, y):
                  cost.append(self._update_weights(xi, target))
              avg_cost = sum(cost) / len(y)
              self.cost_.append(avg_cost)
          return self

      def partial_fit(self, X, y):
          if not self.w_initialized:
              self._initialize_weights(X.shape[1])
          if y.ravel().shape[0] &amp;gt; 1:
              for xi, target in zip(X, y):
                  self._update_weights(xi, target)
          else:
              self._update_weights(X, y)
          return self

      def _shuffle(self, X, y):
          r = np.random.permutation(len(y))
          return X[r], y[r]
      
      def _initialize_weights(self, m):
          self.w_ = np.zeros(1 + m)
          self.w_initialized = True
          
      def _update_weights(self, xi, target):
          output = self.net_input(xi)
          error = (target - output)
          self.w_[1:] += self.eta * xi.dot(error)
          self.w_[0] += self.eta * error
          cost = 0.5 * error**2
          return cost
      
      def net_input(self, X):
          return np.dot(X, self.w_[1:]) + self.w_[0]

      def activation(self, X):
          return self.net_input(X)

      def predict(self, X):
          return np.where(self.activation(X) &amp;gt;= 0.0, 1, -1)
  &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;En primer lugar, hemos añadido un método &lt;code class=&quot;highlighter-rouge&quot;&gt;_shuffle&lt;/code&gt; para mezclar los datos de la entrada antes de cada iteración. El propósito de este paso es evitar ciclos al minimizar la función de costos, ya que este algoritmo es sensible a este problema.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;El constructor incluye parámetros para indicar si queremos mezclar al azar los datos de entrada y para inicializar una semilla para esta mezcla.&lt;/p&gt;

&lt;p&gt;El método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; es el que se encarga del entrenamiento. Invocamos primero al método que inicializa los pesos a cero. Este método existe para marcar los pesos como inicializados, información que necesitamos para el método de entrenamiento parcial &lt;code class=&quot;highlighter-rouge&quot;&gt;parcial_fit&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Dentro del método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; el código que se itera en cada época comienza comprobando si hay que barajar los datos de entrada, e inicializa el costo que evaluará la función de costos:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;A continuación iteramos los datos. La variable &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; es una matriz que contiene una fila por cada muestra, y una columna por cada característica. La variable &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; es un array que contiene un elemento, la clase real, para cada muestra de &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;. El método &lt;code class=&quot;highlighter-rouge&quot;&gt;zip&lt;/code&gt; nos devolverá un elemento de cada variable (&lt;em&gt;zip&lt;/em&gt; en inglés significa, entre otras cosas, cremallera), por lo que en cada iteración del &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; en &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; y en &lt;code class=&quot;highlighter-rouge&quot;&gt;target&lt;/code&gt; se almacenarán las características y la clase real de cada muestra.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Para cada muestra, actualizamos los pesos en el método &lt;code class=&quot;highlighter-rouge&quot;&gt;_update_weights&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_update_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Para la salida real seguimos utilizando la función identidad, con lo que la &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt; es igual a la entrada. La diferencia en este nuevo algoritmo está en cómo ajustamos los pesos. El array de pesos se ajusta con el producto del error con la muestra procesada, no con toda la matriz de entradas. Lógicamente, el costo calculado se obtendrá del error de esta muestra al cuadrado. El costo lo añadimos al array de costos para representarlo posteriormente.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;avg_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Para finalizar el método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; calculamos la media de los costos de cada muestra, y la añadimos al array de costos.&lt;/p&gt;

&lt;h3 id=&quot;resultados&quot;&gt;Resultados&lt;/h3&gt;

&lt;p&gt;Ahora toca usar esta clase. La instanciamos y entrenamos el modelo (el libro usa, en este ejemplo y en el de la entrada anterior, un conjunto de datos escalado para mejorar el rendimiento, y que nosotros revisaremos en el tema 3):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdalineSGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Pintamos los resultados, primero mostrando la clasificación de las muestras y después el costo por época.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plot_decision_regions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adaline - Gradiente descendiente estocástico'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'longitud sépalo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'longitud pétalo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ada&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Épocas'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Costo medio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Los datos obtenidos son estos (los resultados son distintos a los obtenidos en el libro, ya que nosotros no hemos estandarizado los valores de las muestras):&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados&quot; src=&quot;/images/pml/2_adaline.png&quot; /&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_14.png&quot;&gt;&lt;img alt=&quot;Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_14.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Nuestra convergencia es mucha más rápida, pero también muy variable, ya que a partir de la sexta época el costo empieza a oscilar.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">El siguiente punto del libro nos habla de cómo modificar este algoritmo para adaptarlo a conjuntos con millones de datos. El punto clave está en que la forma del algoritmo que hemos analizado reevalúa todo el conjunto de datos cada vez que se realiza una iteración o época. Si tenemos 1 millón de muestras, por cada época vamos a tener que calcular:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Machine Learning con Python - Tema 2 - Adaline</title><link href="https://www.javiercancela.com/2017/04/02/python-machine-learning-iii/" rel="alternate" type="text/html" title="Machine Learning con Python - Tema 2 - Adaline" /><published>2017-04-02T01:00:00+00:00</published><updated>2017-04-02T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/04/02/python-machine-learning-iii</id><content type="html" xml:base="https://www.javiercancela.com/2017/04/02/python-machine-learning-iii/">&lt;p&gt;El perceptrón visto en el post anterior es una red neuronal de una sola capa. Otro ejemplo de este tipo de redes es la llamada &lt;strong&gt;neurona lineal adaptativa&lt;/strong&gt;, en inglés &lt;em&gt;ADAptative LIneal NEuron&lt;/em&gt;, o &lt;a href=&quot;https://es.wikipedia.org/wiki/Adaline&quot;&gt;&lt;em&gt;ADALINE&lt;/em&gt;&lt;/a&gt;. Nos dice el libro que la importancia de este algoritmo se debe a que muestra con claridad la definición y minimización de las funciones de costos (&lt;em&gt;cost functions&lt;/em&gt;).&lt;/p&gt;

&lt;h2 id=&quot;función-de-costos&quot;&gt;Función de costos&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Función de costos&lt;/strong&gt; es un término que procede de la economía, y hace referencia a la función que expresa los costes de producción en términos de la cantidad producida. En el campo de las redes neuronales la función de costos devuelve un número que determina indica lo bien que el algoritmo genera las salidas correctas para las muestras de entrenamiento. Este número es mejor cuanto más bajo, por eso se busca minimizar el valor de la función. Esta minimización de la función de costos supone la base de muchas técnicas de clasificación.&lt;/p&gt;

&lt;p&gt;La diferencia básica entre Adaline y el perceptrón está en la función de activación, que pasa de ser una función escalón a ser una función lineal.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_09.png&quot;&gt;&lt;img alt=&quot;Actualizamos los pesos con una función lineal de la entrada&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_09.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Actualizamos los pesos con una función lineal de la entrada&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;En concreto, la función lineal de activación de Adaline es la función identidad &lt;script type=&quot;math/tex&quot;&gt;\phi(z) = z&lt;/script&gt;, con lo que &lt;script type=&quot;math/tex&quot;&gt;\phi(\mathbf{w}^T\bf x) = \mathbf{w}^T\bf x&lt;/script&gt;. Como vemos en la gráfica superior, se sigue usando un cuantizador para clasificar la muestra, aunque no se use ya para corregir los pesos.&lt;/p&gt;

&lt;h2 id=&quot;minimización-de-funciones-de-costos&quot;&gt;Minimización de funciones de costos&lt;/h2&gt;

&lt;p&gt;Igual que en el caso del perceptrón, la actualización de pesos se realiza calculando &lt;script type=&quot;math/tex&quot;&gt;w_j := w_j + \eta(y^{(i)} - \hat y^{(i)})x_j^{(i)}&lt;/script&gt;. Para el perceptrón tanto &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; como &lt;script type=&quot;math/tex&quot;&gt;\hat y^{(i)}&lt;/script&gt; son etiquetas de clases con solo dos posibles valores. En el caso de Adaline se usan valores continuos.&lt;/p&gt;

&lt;p&gt;La idea de la función de costos es definir una función que se pueda optimizar durante el proceso de aprendizaje. Para ello utilizamos la suma de los errores al cuadrado (&lt;em&gt;Sum of Squared Errors&lt;/em&gt;, o &lt;em&gt;SSE&lt;/em&gt;, también llamado en inglés &lt;a href=&quot;https://en.wikipedia.org/wiki/Residual_sum_of_squares&quot;&gt;&lt;em&gt;Residual Sum of Squareds&lt;/em&gt;&lt;/a&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2,&lt;/script&gt;

&lt;p&gt;donde &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2}&lt;/script&gt; se incluye para simplificar cálculos posteriores. Como buscamos el &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt; que minimiza el valor de esa función, multiplicarla por una constante no afecta al resultado. Como siempre, &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; es el resultado esperado para la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;\phi(z)_{A}^{(i)}&lt;/script&gt; es la salida de la función de activación (&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;) para la muestra &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Esta función es diferenciable y convexa, lo que nos permite calcular un mínimo. Como nuestra función de costos sólo depende de los pesos &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt;, minimizarla nos dará el peso que tenga una menor diferencia entre la salida esperada y la obtenida (&lt;script type=&quot;math/tex&quot;&gt;y^{(i)} - \phi(z)_{A}^{(i)}&lt;/script&gt;). Para calcular el mínimo se usa un algoritmo llamado &lt;a href=&quot;http://alejandrosanchezyali.blogspot.co.uk/2016/01/algoritmo-del-gradiente-descendente-y.html&quot;&gt;gradiente descendente&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Nuestro objetivo, como siempre, es ajustar el peso &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt; en incrementos &lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{w}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w} := \mathbf{w} + \Delta \mathbf{w}.&lt;/script&gt;

&lt;p&gt;Para calcular &lt;script type=&quot;math/tex&quot;&gt;\Delta \mathbf{w}&lt;/script&gt; vamos a necesitar conocer el gradiente de nuestra función de costos: &lt;script type=&quot;math/tex&quot;&gt;\nabla J(\mathbf{w})&lt;/script&gt;. &lt;a href=&quot;https://es.wikipedia.org/wiki/Gradiente&quot;&gt;Gradiente&lt;/a&gt; es un término matemático que hace referencia al vector que indica la dirección en la que un campo varía con más rapidez. Para una función de &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; variables el gradiente es el vector de &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; dimensiones que marca la dirección en la que la función se incrementa más rápidamente.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_10.png&quot;&gt;&lt;img alt=&quot;Usamos el gradiente en cada punto para calcular los decrementos que nos lleven al mínimo&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_10.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Usamos el gradiente en cada punto para calcular los decrementos que nos lleven al mínimo&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Nosotros vamos a usar el gradiente para buscar el mínimo de &lt;script type=&quot;math/tex&quot;&gt;J(\mathbf{w})&lt;/script&gt;, así que en vez de sumarlo lo restaremos (de ahí lo de gradiente descendente). Además usaremos nuestra tasa de aprendizaje para modular esta iteración:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta \mathbf{w} = - \eta \nabla J(\mathbf{w}).&lt;/script&gt;

&lt;p&gt;Dado que&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2,&lt;/script&gt;

&lt;p&gt;tendremos que cada uno de los pesos se actualizará con este incremento:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_{j} := \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x_{j}^{(i)}.&lt;/script&gt;

&lt;p&gt;Para una explicación del cálculo de este gradiente podemos leer &lt;a href=&quot;http://rasbt.github.io/mlxtend/user_guide/general_concepts/linear-gradient-derivative/&quot;&gt;este artículo del propio Sebastian Raschka&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;adaline-en-python&quot;&gt;Adaline en Python&lt;/h2&gt;

&lt;p&gt;Como en la entrada anterior, el código está &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb&quot;&gt;sacado de GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;La implementación de Adaline es muy similar a la del Perceptrón. La única diferencia está en el método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt;, donde los pesos se actualizan con el gradiente descendente&lt;/p&gt;

&lt;pre class=&quot;line-numbers&quot;&gt;
  &lt;code class=&quot;language-python&quot;&gt;
	class AdalineGD(object):
	    def __init__(self, eta=0.01, n_iter=50):
	        self.eta = eta
	        self.n_iter = n_iter

	    def fit(self, X, y):
	        self.w_ = np.zeros(1 + X.shape[1])
	        self.cost_ = []

	        for i in range(self.n_iter):
	            output = self.activation(X)
	            errors = (y - output)
	            self.w_[1:] += self.eta * X.T.dot(errors)
	            self.w_[0] += self.eta * errors.sum()
	            cost = (errors**2).sum() / 2.0
	            self.cost_.append(cost)
	        return self

	    def net_input(self, X):
	        return np.dot(X, self.w_[1:]) + self.w_[0]

	    def activation(self, X):
	        return self.net_input(X)

	    def predict(self, X):
	        return np.where(self.activation(X) &amp;gt;= 0.0, 1, -1)
  &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Recordemos cómo funciona el método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt;. Se le pasan dos parámetros: una matriz &lt;script type=&quot;math/tex&quot;&gt;\bf X&lt;/script&gt;, que contiene una fila por cada muestra y una columna por cada característica, y que constituye nuestro conjunto de datos de entrenamiento; y un array &lt;script type=&quot;math/tex&quot;&gt;\bf y&lt;/script&gt;, que contiene el resultado objetivo para cada muestra.&lt;/p&gt;

&lt;p&gt;Lo primero que hace el método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; es inicializar los pesos y los costos:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Después iteramos el proceso tantas veces como épocas se hayan definido. La entrada neta (la función &lt;code class=&quot;highlighter-rouge&quot;&gt;net_input&lt;/code&gt;) es el producto escalar de los atributos de las muestras y sus pesos. Para la salida utilizamos una función &lt;code class=&quot;highlighter-rouge&quot;&gt;activation&lt;/code&gt; que es igual a la entrada neta. La existencia de este método se justifica para hacer el código más general, y poder reutilizarlo con otros algoritmos. En nuestro caso, esta función es la función identidad, por lo que podríamos haber usado directamente &lt;code class=&quot;highlighter-rouge&quot;&gt;net_input&lt;/code&gt;. El error será la difencia entre la salida esperada, &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;, y la real, &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Si recordamos las entradas anteriores, la salida esperada &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; es un array con las clases reales de 100 muestras, las 50 primeras con valor -1 (Iris-setosa), y las 50 siguientes con valor 1 (Iris-versicolor). El valor de output es la salida de la función identidad, pero en este caso vamos a considerar todas las muestras a la vez, con lo que tendremos un array de 100 elementos en vez de un escalar. El método &lt;code class=&quot;highlighter-rouge&quot;&gt;net_input&lt;/code&gt; calcula el producto escalar de la matriz de muestras &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; por el array de pesos &lt;code class=&quot;highlighter-rouge&quot;&gt;w_&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;net_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;cálculo-de-errores&quot;&gt;Cálculo de errores&lt;/h3&gt;

&lt;p&gt;El resultado es un array de errores (&lt;code class=&quot;highlighter-rouge&quot;&gt;errors&lt;/code&gt;) de 100 elementos, uno para cada muestra. Con ellos se actualizan los pesos:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;En la primera línea se aplica un incremento &lt;code class=&quot;highlighter-rouge&quot;&gt;self.eta * X.T.dot(errors)&lt;/code&gt; al array de pesos &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt;, salvo al elemento &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; (peso asociado a una entrada ficticia &lt;script type=&quot;math/tex&quot;&gt;x_0 = 1&lt;/script&gt; para simplificar los cálculos). Recordando la fórmula de incremento de pesos vista más arriba:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_{j} := \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x_{j}^{(i)},&lt;/script&gt;

&lt;p&gt;tenemos que los elementos &lt;script type=&quot;math/tex&quot;&gt;error^{(i)}&lt;/script&gt; del array &lt;code class=&quot;highlighter-rouge&quot;&gt;errors&lt;/code&gt; se corresponden con &lt;script type=&quot;math/tex&quot;&gt;error^{(i)} = y^{(i)} - \phi(z)_{A}^{(i)}&lt;/script&gt;. Por lo tanto, el producto escalar entre la traspuesta de la matriz de atributos de cada muestra &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; y el array de errores &lt;code class=&quot;highlighter-rouge&quot;&gt;errors&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;X.T.dot(errors)&lt;/code&gt;) se corresponden con el sumatorio &lt;script type=&quot;math/tex&quot;&gt;\sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x_{j}^{(i)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Para el caso del elemento &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;, al ser el valor de los atributos de esa muestra ficticia igual a uno, basta con sumar los errores para obtener el incremento de peso (también podríamos haber añadido una fila con valores unitarios a la matriz &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Aunque el algoritmo no usa la función de costos directamente, ya que el ajuste de pesos se realiza mediante el gradiente descendente que acabamos de aplicar y que se explica más arriba, vamos a almacenar también el valor de la función para dibujar después su evolución.&lt;/p&gt;

&lt;h3 id=&quot;cálculo-de-costos&quot;&gt;Cálculo de costos&lt;/h3&gt;

&lt;p&gt;Recordemos la función de costos:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J({\mathbf{w}}) = \frac{1}{2} \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big)^2.&lt;/script&gt;

&lt;p&gt;El programa almacena un arrary &lt;code class=&quot;highlighter-rouge&quot;&gt;errors = (y - output)&lt;/code&gt; que se corresponde con &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} - \phi(z)_{A}^{(i)}&lt;/script&gt;, así que para calcular en valor de la función sólo tenemos que sumar el cuadrado de los valores del array y dividir entre dos. Esto nos dará el valor calculado por la función de costos para los pesos actuales. Como estamos iterando mediante el gradiente descendente vamos a almacenar los resultados de la función obtenidos para cada conjunto de pesos:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Una vez definida la clase, el libro pasa a utilizarla con dos tasas de aprendizaje distintas. Esta tasa va a determinar la convergencia del algoritmo, y encontrar el valor que nos de la mejor convergencia posible requiere muchas pruebas. Vamos a probar con &lt;script type=&quot;math/tex&quot;&gt;\eta = 0.1&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;\eta = 0.0001&lt;/script&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ada1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdalineGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Epochs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'log(Sum-squared-error)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adaline - Learning rate 0.01'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ada2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdalineGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ada2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ada2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Epochs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sum-squared-error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Adaline - Learning rate 0.0001'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Pintamos una gráfica con cada tasa, representado la evolución del valor de la función de costos para cada iteración (época). Para la tasa de 0.1 representamos en realidad el logaritmo del valor, ya que el valor real diverge exponencialmente. En cambio, podemos ver como la tasa de 0.0001 converge, pero lentamente.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_11.png&quot;&gt;&lt;img alt=&quot;Una tasa de aprendizaje muy alta provoca que el algoritmo no converja, una tasa muy baja provoca que converja lentamente&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_11.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Una tasa de aprendizaje muy alta provoca que el algoritmo no converja, una tasa muy baja provoca que converja lentamente&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Llevado al ejemplo visual del gradiente descendente, vemos que la tasa de aprendizaje influye en la longitud del “paso” que se da hacia el mínimo. Si este paso es muy grande el algoritmo “se sale”:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_12.png&quot;&gt;&lt;img alt=&quot;Visualización gráfica de los pasos dados con el algoritmo&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_12.png&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;Visualización gráfica de los pasos dados con el algoritmo&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;El libro acaba esta parte con una referencia al escalado de características que se verá en el tema 3, y que nosotros dejaremos hasta entonces.&lt;/p&gt;

&lt;p&gt;En la próxima entrada veremos cómo modificar este algoritmo para adaptarlo a grandes grupos de datos.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">El perceptrón visto en el post anterior es una red neuronal de una sola capa. Otro ejemplo de este tipo de redes es la llamada neurona lineal adaptativa, en inglés ADAptative LIneal NEuron, o ADALINE. Nos dice el libro que la importancia de este algoritmo se debe a que muestra con claridad la definición y minimización de las funciones de costos (cost functions).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Materia oscura</title><link href="https://www.javiercancela.com/2017/03/26/materia-oscura-blake-crouch/" rel="alternate" type="text/html" title="Materia oscura" /><published>2017-03-26T01:00:00+00:00</published><updated>2017-03-26T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/03/26/materia-oscura-blake-crouch</id><content type="html" xml:base="https://www.javiercancela.com/2017/03/26/materia-oscura-blake-crouch/">&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Materia-oscura-Spanish-Blake-Crouch/dp/6075270477&quot;&gt;&lt;em&gt;Materia oscura&lt;/em&gt;, de Blake Crouch&lt;/a&gt; es una novela de ciencia ficción protagonizada por Jason Dessen, un profesor de física que vive felizmente con su mujer y su hijo en Chicago. El libro nos sumerge en una trama que comienza cuando Jason es secuestrado por un misterioso hombre encapuchado.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img style=&quot;width:400px&quot; alt=&quot;La portada de la edición en castellano es mucho más chula que la inglesa&quot; src=&quot;https://images-na.ssl-images-amazon.com/images/I/61ToEf-IziL.jpg&quot; /&gt;  
        &lt;figcaption&gt;La portada de la edición en castellano es mucho más chula que la inglesa&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Materia oscura&lt;/em&gt; es una historia sobre universos paralelos. Fundamentalmente es un &lt;em&gt;thriller&lt;/em&gt; con ingredientes de ciencia ficción, toques románticos, y reflexiones sobre aquello que nos hace únicos, sobre la esencia de nuestra identidad. Todo se mezcla en una historia que avanza a un ritmo muy alto, a veces atropellado. Este ritmo permite que no nos entretengamos demasiado demasiado profundos para ser tratados en un libro de este tipo, pero a cambio le da a la historia un tono superficial en varios puntos.&lt;/p&gt;

&lt;p&gt;El libro se lee de un tirón. Al estar protagonizado por un físico y tener el viaje entre universos paralelos como eje, contiene las inevitables descripciones de fenómenos cuánticos que justifiquen la trama. Pero estas descripciones son las justas para dar un poco de sentido a la trama, y no entorpecen la narración.&lt;/p&gt;

&lt;p&gt;Quizás me lo perdí por haberlo escuchado (en audiolibro) en vez de leído, pero no acabé de entender bien él por qué del título. El término ‘materia oscura’ se utiliza en física para denominar a un tipo de materia que no emite radiación electromagnética ni interactúa con ella, pero cuya existencia se puede deducir por el efecto gravitatorio que ejerce sobre la materia convencional. Sin embargo esta materia no juega ningún papel en la trama, así que supongo que se menciona en algún momento durante las explicaciones, y se me pasó la mención.&lt;/p&gt;

&lt;p&gt;Escuchar el libro tiene otro efecto: he leido algunas quejas sobre el hábito del autor de incluir párrafos compuestos de una frase corta, o incluso una única palabra, para dar ritmo a algunas escenas, provocando un efecto en el texto que no agrada a todo el mundo. En audiolibro este efecto no se aprecia, por el buen hacer del actor encargado de la lectura.&lt;/p&gt;

&lt;p&gt;La única traducción al español que he encontrado es &lt;a href=&quot;http://www.oceano.com.mx/obras/materia-oscura-blake-crouch-13955.aspx&quot;&gt;la de la editorial mexicana Oceano&lt;/a&gt;. Curiosamente, esta edición &lt;a href=&quot;https://www.amazon.com/Materia-oscura-Spanish-Blake-Crouch/dp/6075270477&quot;&gt;está disponible en la tienda americana de Amazon&lt;/a&gt;, mientras que en la española sólo están disponibles &lt;a href=&quot;https://www.amazon.es/Dark-Matter-Blake-Crouch/dp/0451496418&quot;&gt;la edición inglesa&lt;/a&gt; y &lt;a href=&quot;https://www.amazon.es/Mat%C3%A9ria-escura-Portuguese-Blake-Crouch-ebook/dp/B01MZCVP6L&quot;&gt;la portuguesa&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;En resumen, &lt;em&gt;Materia oscura&lt;/em&gt; es un libro adecuado para aficionados a los &lt;em&gt;thriller&lt;/em&gt; a los que no les importe un toque fantástico en la trama.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img style=&quot;width:400px&quot; alt=&quot;&quot; src=&quot;&quot; /&gt;  
        &lt;figcaption&gt;&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Materia oscura, de Blake Crouch es una novela de ciencia ficción protagonizada por Jason Dessen, un profesor de física que vive felizmente con su mujer y su hijo en Chicago. El libro nos sumerge en una trama que comienza cuando Jason es secuestrado por un misterioso hombre encapuchado.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://i.imgur.com/q2ZEHxF.jpg" /></entry><entry><title type="html">Machine Learning con Python - Tema 2 - El perceptrón</title><link href="https://www.javiercancela.com/2017/03/19/python-machine-learning-ii/" rel="alternate" type="text/html" title="Machine Learning con Python - Tema 2 - El perceptrón" /><published>2017-03-19T01:00:00+00:00</published><updated>2017-03-19T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/03/19/python-machine-learning-ii</id><content type="html" xml:base="https://www.javiercancela.com/2017/03/19/python-machine-learning-ii/">&lt;p&gt;Empezamos con los algoritmos. Vamos a implementar un perceptrón en Python, y entrenarlo para que sepa clasificar las diferentes especies de flores en Iris. El libro comienza mostrando el &lt;a href=&quot;https://es.wikipedia.org/wiki/Neurona_de_McCulloch-Pitts&quot;&gt;modelo de neurona de McCulloch-Pitts&lt;/a&gt;:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;En este modelo se emite una señal de salida si el resultado de aplicar una función a la suma de las entradas supera cierto umbral&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_01.png&quot; /&gt;
        &lt;figcaption&gt;En este modelo se emite una señal de salida si el resultado de aplicar una función a la suma de las entradas supera cierto umbral&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Tomando este modelo como base, Frank Rosenblatt diseñó un dispositivo electrónico con capacidad para aprender. El algoritmo usado por ese dispositivo se conoce como Perceptrón.&lt;/p&gt;

&lt;h2 id=&quot;perceptrón&quot;&gt;Perceptrón&lt;/h2&gt;

&lt;p&gt;El &lt;a href=&quot;https://es.wikipedia.org/wiki/Perceptr%C3%B3n&quot;&gt;perceptrón&lt;/a&gt; es un algoritmo de aprendizaje supervisado para realizar tareas de clasificación binaria, es decir, para determinar si una muestra pertenece o no a una clase.&lt;/p&gt;

&lt;div style=&quot;background-color: #EEEEEE; padding: 1em&quot;&gt;
El libro asume cierta familiaridad trabajando con vectores y matrices. Para refrescar conceptos recomiendo revisar los &lt;a href=&quot;https://es.wikipedia.org/wiki/%C3%81lgebra_lineal#Enlaces_externos&quot;&gt;enlaces externos referenciados en la página de la Wikipedia dedicada al Álgebra Lineal&lt;/a&gt;. También tiene buena pinta [el curso de la Khan Academy](https://es.khanacademy.org/math/linear-algebra). &lt;br /&gt;
En inglés hay varios libros gratuitos disponibles, como [este](https://www.math.ucdavis.edu/~linear/linear-guest.pdf) y [este](http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf).
&lt;/div&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Perceptr%C3%B3n_5_unidades.svg#/media/File:Perceptr%C3%B3n_5_unidades.svg&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/1200px-Perceptr%C3%B3n_5_unidades.svg.png&quot; alt=&quot;Perceptrón 5 unidades.svg&quot; /&gt;&lt;/a&gt;
        &lt;figcaption&gt;
            De &lt;a href=&quot;//commons.wikimedia.org/w/index.php?title=User:Alejandro.cartas&amp;amp;action=edit&amp;amp;redlink=1&quot; class=&quot;new&quot; title=&quot;User:Alejandro.cartas (page does not exist)&quot;&gt;Alejandro Cartas&lt;/a&gt; - &lt;span class=&quot;int-own-work&quot; lang=&quot;es&quot;&gt;Trabajo propio&lt;/span&gt;, &lt;a href=&quot;http://creativecommons.org/licenses/by-sa/4.0&quot; title=&quot;Creative Commons Attribution-Share Alike 4.0&quot;&gt;CC BY-SA 4.0&lt;/a&gt;, &lt;a href=&quot;https://commons.wikimedia.org/w/index.php?curid=41534843&quot;&gt;Enlace&lt;/a&gt;
        &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Como podemos ver en la figura anterior, la señal de entrada está compuesta por un array &lt;script type=&quot;math/tex&quot;&gt;\bf x&lt;/script&gt; al que se aplica una serie de pesos definidos por un array &lt;script type=&quot;math/tex&quot;&gt;\bf w&lt;/script&gt;. Es decir, cada elemento &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; se multiplica por &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;, y se suma el resultado de cada uno de estos productos. De una manera más formal, definimos&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \begin{bmatrix}
x_1 \cr
\vdots \cr
x_m \cr
\end{bmatrix}, w = \begin{bmatrix}
w_1 \cr
\vdots \cr
w_m \cr
\end{bmatrix}
\tag{2.1}&lt;/script&gt;

&lt;p&gt;Y la entrada resultante &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; será la suma de cada entrada ponderada:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = \sum{w_ix_i} = w_1x_1 + \cdots + w_mx_m
\tag{2.2}&lt;/script&gt;

&lt;p&gt;Ya tenemos la entrada &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; a la función de activación &lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt;. Para nuestro caso vamos a usar como función de activación la &lt;a href=&quot;https://es.wikipedia.org/wiki/Funci%C3%B3n_escal%C3%B3n_de_Heaviside&quot;&gt;función escalón&lt;/a&gt;. Esta función se define de la siguiente forma:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(z) = \begin{cases}
\text{1 si } z \ge \theta \cr
\text{-1 en otro caso}
\end{cases}
\tag{2.3}&lt;/script&gt;

&lt;p&gt;Ese valor &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; es el umbral que tiene que superar &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; para disparar la señal. Para simplificar los cálculos, definimos un peso 0 con &lt;script type=&quot;math/tex&quot;&gt;w_0 = -\theta&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;x_0 = 1&lt;/script&gt;, de esta forma podemos escribir:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = w_0x_0 + w_1x_1 + \cdots + w_mx_m
\tag{2.4}
\label{2.4}&lt;/script&gt;

&lt;p&gt;y&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(z) = \begin{cases}
\text{1 si } z \ge 0 \cr
\text{-1 en otro caso}
\end{cases}
\tag{2.5}
\label{2.5}&lt;/script&gt;

&lt;p&gt;El valor de &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; se puede escribir también como &lt;script type=&quot;math/tex&quot;&gt;z = \mathbf{w}^T\bf x&lt;/script&gt;. El superíndice T indica que la matriz (en este caso un array, que tratamos como una matriz de 1 columna y n filas) está traspuesta, o lo que es lo mismo, se han convertidos sus columnas en filas. Por lo tanto:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{w}^T\bf x = \begin{bmatrix} w_0 &amp; w_1 &amp; \cdots &amp; w_m \end{bmatrix} \begin{bmatrix}
x_1 \cr
\vdots \cr
x_m \cr
\end{bmatrix}

\tag{2.6} %]]&gt;&lt;/script&gt;

&lt;p&gt;Esto nos permite usar la operación de producto escalar entre matrices. En una operación entre matrices, si la primera tiene dimensiones &lt;script type=&quot;math/tex&quot;&gt;n \times m&lt;/script&gt;, la segunda debe tener &lt;script type=&quot;math/tex&quot;&gt;m \times p&lt;/script&gt;, dando como resultado una matriz &lt;script type=&quot;math/tex&quot;&gt;n \times p&lt;/script&gt;. En nuestro caso, la dimensiones serían &lt;script type=&quot;math/tex&quot;&gt;1 \times n&lt;/script&gt; y &lt;script type=&quot;math/tex&quot;&gt;n \times 1&lt;/script&gt;, dando como resultado una matriz &lt;script type=&quot;math/tex&quot;&gt;1 \times 1&lt;/script&gt;, es decir, un escalar. Por ello esta operación es realmente un producto escalar entre vectores, cuyo resultado es la ya conocida expresión \eqref{2.4}&lt;/p&gt;

&lt;h2 id=&quot;entrenando-el-modelo&quot;&gt;Entrenando el modelo&lt;/h2&gt;

&lt;p&gt;Pretendemos modificar los valores de los pesos (&lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt;) de forma que las salidas del preceptrón se vayan acercando a los valores esperados. Para ello se inicializan los pesos a cero, o a un valor muy pequeño, se obtiene la salida, y se actualizan los pesos.&lt;/p&gt;

&lt;p&gt;¿Cómo se actualizan los pesos? Sumándoles una cantidad (positiva o negativa) que depende de la entrada, de la diferencia entre la salida real y la salida esperada, y de la tasa de aprendizaje (&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;), que es una constante que vale entre 0 y 1:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_j = \eta(y^{(i)} - \hat y^{(i)})x_j^{(i)}
\tag{2.7}&lt;/script&gt;

&lt;p&gt;En esta fórmula &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; es la clase a la que pertenece la muestra (i), &lt;script type=&quot;math/tex&quot;&gt;\hat y^{(i)}&lt;/script&gt; es la clase predicha por el perceptrón, y &lt;script type=&quot;math/tex&quot;&gt;x_j^{(i)}&lt;/script&gt; es la entrada de la característica j para la muestra (i). Todo esto se multiplica para calcular la variación en la característica j: &lt;script type=&quot;math/tex&quot;&gt;\Delta w_j&lt;/script&gt;. Con cada muestra se recalculan los pesos:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_j := w_j + \Delta w_j
\tag{2.8}&lt;/script&gt;

&lt;h3 id=&quot;condiciones-de-convergencia&quot;&gt;Condiciones de convergencia&lt;/h3&gt;

&lt;p&gt;Este proceso iterativo puede converger o no. Para que el perceptrón clasifique correctamente las muestras , el valor de &lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt; debe ir acercándose cada vez más a un valor final, o lo que es lo mismo, &lt;script type=&quot;math/tex&quot;&gt;\Delta w_j&lt;/script&gt; debe tender a cero.&lt;/p&gt;

&lt;p&gt;Para que se de esta situación deben cumplirse dos requisitos. En primer lugar, la tasa de aprendizaje debe ser suficientemente pequeña. En general, cuanto más grande sea &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; más rápido será el aprendizaje, pero pasado cierto valor el proceso de aprendizaje nunca convergerá. En segundo lugar, las dos clases deben ser linealmente separables. Es decir, si representamos los elementos de ambas clases en un diagrama de 2 dimensiones (suponeniendo que las muestras tienen dos características), debe existir una línea que separe a todos los de una clase a un lado, y a todos los de la otra clase al otro lado.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Ejemplos con muestras de dos clases. Sólo el primero es linearmente separable.&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_03.png&quot; /&gt;
        &lt;figcaption&gt;Ejemplos con muestras de dos clases. Sólo el primero es linearmente separable.&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;perceptrón-en-python&quot;&gt;Perceptrón en Python&lt;/h2&gt;

&lt;p&gt;El código del capítulo 2 (y de los demás capítulos) está, como comentamos, &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb&quot;&gt;subido a GitHub&lt;/a&gt; en formato &lt;a href=&quot;http://jupyter.org/&quot;&gt;Jupyter&lt;/a&gt;. Jupyter (originalmente iPython) es una aplicación web que permite crear y compartir documentos que contengan código, gráficas, animaciones… de un forma muy sencilla. Jupyter viene incluido en Anaconda, que es el entorno Python que recomendamos en &lt;a href=&quot;/2017/03/12/python-machine-learning-i/&quot;&gt;una entrada anterior&lt;/a&gt;. También se puede ver el código completo de cada capítulo como archivo Python &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book/tree/master/code/optional-py-scripts&quot;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;El código comienza declarando una clase &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron&lt;/code&gt; en la que se definen en constructor y tres métodos:&lt;/p&gt;
&lt;pre class=&quot;line-numbers&quot;&gt;
  &lt;code class=&quot;language-python&quot;&gt;
    import numpy as np

    class Perceptron(object):
      def __init__(self, eta=0.01, n_iter=10):
        self.eta = eta
        self.n_iter = n_iter

      def fit(self, X, y):
        self.w_ = np.zeros(1 + X.shape[1])
        self.errors_ = []

        for _ in range(self.n_iter):
          errors = 0
          for xi, target in zip(X, y):
            update = self.eta * (target - self.predict(xi))
            self.w_[1:] += update * xi
            self.w_[0] += update
            errors += int(update != 0.0)
          self.errors_.append(errors)
        return self

      def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

      def predict(self, X):
        return np.where(self.net_input(X) &amp;gt;= 0.0, 1, -1)
  &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Inicializamos el perceptrón con dos parámetros: &lt;code class=&quot;highlighter-rouge&quot;&gt;eta&lt;/code&gt;, que es la tasa de aprendizaje, y &lt;code class=&quot;highlighter-rouge&quot;&gt;n_iter&lt;/code&gt;, que es el número de veces que vamos a recorrer el conjunto de datos de entrenamiento. A cada una de estas veces se les llama “épocas”.&lt;/p&gt;

&lt;p&gt;Una vez instanciado el perceptrón llamaremos al método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; para entrenar nuestro modelo. Se le pasan dos parámetros: una matriz &lt;script type=&quot;math/tex&quot;&gt;\bf X&lt;/script&gt;, que contiene una fila por cada muestra y una columna por cada característica, y que constituye nuestro conjunto de datos de entrenamiento; y un array &lt;script type=&quot;math/tex&quot;&gt;\bf y&lt;/script&gt;, que contiene el resultado objetivo para cada muestra.&lt;/p&gt;

&lt;p&gt;Lo primero que hace el método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; es inicializar los pesos y los errores:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Los pesos se inicializan a un array con el mismo número de elementos que muestras tiene la matriz &lt;script type=&quot;math/tex&quot;&gt;\bf X&lt;/script&gt; más uno. Ese más uno es debido a que vamos a almacenar también el parámetro &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; corresondiente al umbral &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; que pasamos a la izquiera en la equación \eqref{2.4}. Como estamos inicializando todo a cero consideramos el umbral inicial de activación como cero.&lt;/p&gt;

&lt;p&gt;A continuación comienza un precose que se ejecutará &lt;code class=&quot;highlighter-rouge&quot;&gt;n_iter&lt;/code&gt; veces. En el se itera sobre los datos proporcionados mediante esta instrucción:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Lo que va a hacer la instrucción &lt;code class=&quot;highlighter-rouge&quot;&gt;zip&lt;/code&gt; es devolver un elemento por cada lista que se le pase de parámetro. Así, si &lt;script type=&quot;math/tex&quot;&gt;\bf X&lt;/script&gt; es una matriz de 100 filas (muestras) y 2 columnas (características), e &lt;script type=&quot;math/tex&quot;&gt;\bf y&lt;/script&gt; es un array de 100 elementos (las clases reales de cada muestra), la instrucción anterior iterará para cada muestra almacenando en &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; las características y en &lt;code class=&quot;highlighter-rouge&quot;&gt;target&lt;/code&gt; su clase real.&lt;/p&gt;

&lt;p&gt;Con esos datos vamos a calcular los nuevos pesos. La fórmula era la siguiente:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_j = \eta(y^{(i)} - \hat y^{(i)})x_j^{(i)}
w_j := w_j + \Delta w_j&lt;/script&gt;

&lt;p&gt;En nuestro caso hacemos esto:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Calculamos primero el factor &lt;script type=&quot;math/tex&quot;&gt;\eta(y^{(i)} - \hat y^{(i)})&lt;/script&gt;, y después lo multiplicamos por &lt;script type=&quot;math/tex&quot;&gt;\bf{x^{(i)}}&lt;/script&gt; para sumárselo al array de pesos, salvo el peso 0. Al ser tanto ‘self.w_[1:]’ como ‘xi’ arrays con un elemento por cada característica, estamos aplicando la corrección a todos los pesos. En el peso &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; no multiplicamos por &lt;script type=&quot;math/tex&quot;&gt;\bf{x_0^{(i)}}&lt;/script&gt;, ya que este por definición vale uno. Finalmente acumulamos el error de cada muestra, y al final de cada época lo añadimos a un array. Esto nos permitirá ver la evolución del error por épocas.&lt;/p&gt;

&lt;p&gt;Veamos ahora cómo calcular el término &lt;script type=&quot;math/tex&quot;&gt;\hat y^{(i)}&lt;/script&gt;, que en código se obtiene llamando a &lt;code class=&quot;highlighter-rouge&quot;&gt;self.predict(xi)&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;net_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Lo primero que hace &lt;code class=&quot;highlighter-rouge&quot;&gt;predict&lt;/code&gt; con &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; es usarlo para invocar &lt;code class=&quot;highlighter-rouge&quot;&gt;net_input&lt;/code&gt;. En esta función usamos &lt;code class=&quot;highlighter-rouge&quot;&gt;np.dot&lt;/code&gt; para realizar un producto escalar entre &lt;code class=&quot;highlighter-rouge&quot;&gt;xi&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;self.w_[1:]&lt;/code&gt;. Es decir, suponiendo una muestra con dos características, y por tanto dos entradas y dos pesos, &lt;code class=&quot;highlighter-rouge&quot;&gt;np.dot(xi, self.w_[1:])&lt;/code&gt; equivale a &lt;script type=&quot;math/tex&quot;&gt;w_1x_1 + w_2x_2&lt;/script&gt;. A esto le sumamos &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; (ya que &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt; es igual a uno), y tenemnos que &lt;code class=&quot;highlighter-rouge&quot;&gt;net_input&lt;/code&gt; devuelve el valor &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, tal como se define en \eqref{2.4}. La última parte, &lt;code class=&quot;highlighter-rouge&quot;&gt;np.where(self.net_input(xi) &amp;gt;= 0.0, 1, -1)&lt;/code&gt; devuelve 1 si &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; es mayor o igual que 0, y -1 en otro caso. En resumen, el método &lt;code class=&quot;highlighter-rouge&quot;&gt;predict&lt;/code&gt; es la función &lt;script type=&quot;math/tex&quot;&gt;\phi(z)&lt;/script&gt; (\eqref{2.5}).&lt;/p&gt;

&lt;h3 id=&quot;perceptrón-con-el-conjunto-de-datos-iris&quot;&gt;Perceptrón con el conjunto de datos Iris&lt;/h3&gt;

&lt;p&gt;Toda la lógica del perceptrón está implementada en la clase anterior. Vamos a utilizarla con el conjunto de datos Iris ya mencionado anteriormente. El ejemplo del libro utilizan la librería &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas&lt;/code&gt;, especializada en análisis de datos y estructuras, para cargar los datos de Iris. Estos datos se almacenan en un tipo de datos de la librería llamado &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe&quot;&gt;DataFrame&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://archive.ics.uci.edu/ml/'&lt;/span&gt;
                 &lt;span class=&quot;s&quot;&gt;'machine-learning-databases/iris/iris.data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;                
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;El archivo de datos lo estamos descargando de internet, pero también está disponible en el código de ejemplo para usar en local.&lt;/p&gt;

&lt;p&gt;El código de ejemplo imprime las últimas líneas de la estructura de datos cargada:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;145&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;6.7&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;5.2&lt;/td&gt;
      &lt;td&gt;2.3&lt;/td&gt;
      &lt;td&gt;Iris-virginica&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;146&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;6.3&lt;/td&gt;
      &lt;td&gt;2.5&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;1.9&lt;/td&gt;
      &lt;td&gt;Iris-virginica&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;147&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;6.5&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;5.2&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;Iris-virginica&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;148&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;6.2&lt;/td&gt;
      &lt;td&gt;3.4&lt;/td&gt;
      &lt;td&gt;5.4&lt;/td&gt;
      &lt;td&gt;2.3&lt;/td&gt;
      &lt;td&gt;Iris-virginica&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;149&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;5.9&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;1.8&lt;/td&gt;
      &lt;td&gt;Iris-virginica&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A continuación preprocesamos los datos, para dejarlos en el formato adecuado para trabajar con ellos. Usaremos las librerías &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt;, para tratar los datos, y &lt;code class=&quot;highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt;, para dibujarlos en una gráfica&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;En primer lugar hemos seleccionado los 100 primeros registros del DataFrame, pero cogiendo sólo la quinta columna (la 4 por comenzar el índice en 0). Esto nos devuelve un array con 100 valores, cada uno de los cuales es el nombre de la especie. Es decir, cogemos las etiquetas de la clase de cada muestra.&lt;/p&gt;

&lt;p&gt;En el conjunto de datos Iris los datos están ordenados de tal forma que los 50 primeros son Iris-setosa, y los 50 siguientes Iris-versicolor. De esta forma limitarnos a los 100 primeros registros nos permite realizar una clasificación binaria.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Iris-setosa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;El siguiente paso consiste en sustituir las etiquetas de texto por valores numéricos. El valor objetivo pasará a ser -1 para las Iris-setosa y 1 para las Iris-versicolor. Almacenamos los datos en un array &lt;script type=&quot;math/tex&quot;&gt;\bf y&lt;/script&gt; de 100 elementos.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;De las cuatro características posibles nos vamos a quedar con dos: la longitud del sépalo y la longitud del pétalo. Esto nos permitirá dibujar los datos en una gráfica de dos dimensiones. Al igual que hicimos con el valor objetivo, cogemos los cien primeros valores del conjunto de datos, quedándonos con la primera y la tercera columna. Esto nos devuelve una matriz &lt;script type=&quot;math/tex&quot;&gt;\bf X \in \mathbb R^{100x2}&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'setosa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'versicolor'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sepal length [cm]'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'petal length [cm]'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Dibujamos los datos almacenados. Marcamos con una ‘o’ roja las Iris-setosa (las 50 primeras), y con una ‘x’ azul las Iris-versicolor. El resultado es este:&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Datos de entrenamiento&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_06.png&quot; /&gt;
        &lt;figcaption&gt;Datos de entrenamiento&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Con los datos listos podemos realizar el entrenamiento.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Instanciamos el perceptrón con un &lt;script type=&quot;math/tex&quot;&gt;\eta = 0.1&lt;/script&gt; y 10 épocas. Para entrenar  basta con llamar al método &lt;code class=&quot;highlighter-rouge&quot;&gt;fit&lt;/code&gt; pasando &lt;script type=&quot;math/tex&quot;&gt;\bf X&lt;/script&gt;  e &lt;script type=&quot;math/tex&quot;&gt;\bf y&lt;/script&gt; como parámetros.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ppn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Epochs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Number of updates'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# plt.savefig('./perceptron_1.png', dpi=300)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Una vez entrenado el modelo, pintamos en un gráfico los errores encontrados en cada generación. En  el eje X pintamos las épocas (la longitud del array de errores), y en el Y mostramos el número de errores.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Errores por época&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_07.png&quot; /&gt;
        &lt;figcaption&gt;Errores por época&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;A partir de la 6ª época el modelo ya no comete errores. Ahora vamos a dibujar los datos de entrenamiento, pero marcando las zonas que el modelo ya entrenado considera como de una clase u otra.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.colors&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListedColormap&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;plot_decision_regions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;markers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'^'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'v'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'lightgreen'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'gray'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cyan'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ListedColormap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Definimos una nueva función que nos permita dibujar en distintos colores las zonas que serían de Iris-setosa e Iris-versicolor si una muestra tuviese unas características que estuviesen en ese punto.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;plot_decision_regions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ... Continuación&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x1_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x2_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contourf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xx2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# plot class samples&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;markers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Definimos los valores mínimo y máximo del sépalo (&lt;code class=&quot;highlighter-rouge&quot;&gt;x1_min&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;x1_max&lt;/code&gt;) y del pétalo (&lt;code class=&quot;highlighter-rouge&quot;&gt;x2_min&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;x2_max&lt;/code&gt;), restando uno al mínimo y sumándolo al máximo para tener un rango con margen. La función &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;np.arange&lt;/code&gt;&lt;/a&gt; devuelve una lista de valores que van del primer parámetro al segundo en intervalos marcados por el tercer parámetro. Las dos llamadas a &lt;code class=&quot;highlighter-rouge&quot;&gt;np.arange&lt;/code&gt; devolverán un array de valores separados por 0.02. En el primer caso van del 3.30 al 7.98, en el segundo del 0 al 6.08.  Con ambos arrays construimos una matriz que pasamos al predictor, el cual nos devolverá el valor predicho para todos los puntos del plano definido.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Regiones predichas para cada clase&quot; src=&quot;https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_08.png&quot; /&gt;
        &lt;figcaption&gt;Regiones predichas para cada clase&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Empezamos con los algoritmos. Vamos a implementar un perceptrón en Python, y entrenarlo para que sepa clasificar las diferentes especies de flores en Iris. El libro comienza mostrando el modelo de neurona de McCulloch-Pitts:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Machine Learning con Python - Tema 1</title><link href="https://www.javiercancela.com/2017/03/12/python-machine-learning-i/" rel="alternate" type="text/html" title="Machine Learning con Python - Tema 1" /><published>2017-03-12T01:00:00+00:00</published><updated>2017-03-12T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/03/12/python-machine-learning-i</id><content type="html" xml:base="https://www.javiercancela.com/2017/03/12/python-machine-learning-i/">&lt;p&gt;El foco de estas entradas se pondrá en el &lt;a href=&quot;https://github.com/rasbt/python-machine-learning-book&quot;&gt;código Python publicado en GitHub&lt;/a&gt;, pero el primer tema no incluye código, tan sólo presenta una serie de conceptos que resumo en esta entrada.&lt;/p&gt;

&lt;h2 id=&quot;tipos-de-machine-learning&quot;&gt;Tipos de &lt;em&gt;machine learning&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Existen tres estrategias distintas de aprendizaje automático: supervisado, no supervisado y reforzado&lt;/p&gt;

&lt;h3 id=&quot;supervisado&quot;&gt;Supervisado&lt;/h3&gt;

&lt;p&gt;En el aprendizaje supervisado se entrena el modelo con datos para los que ya se conoce el resultado correcto. En función del tipo de resultado que esperemos obtener, podemos encontrarnos con que los datos de entrenamiento llevan asociada una etiqueta que indica a qué categoría pertenece, o un valor numérico. En el primer caso hablamos de &lt;strong&gt;clasificación&lt;/strong&gt;, y en el segundo de &lt;strong&gt;regresión&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Un ejemplo de problema de clasificación es el mencionado en la entrada anterior: la clasificación de spam. Al modelo se le presenta con una serie de correos que ya han sido identificados con una etiqueta ‘spam’ o ‘no spam’. El modelo utiliza esos correos para calibrar ciertos valores internos, de forma que pueda identificar correctamente futuros correos sin intervención humana.&lt;/p&gt;

&lt;p&gt;En los problemas de regresión el valor esperado pertenece a un rango continuo. Un ejemplo, &lt;a href=&quot;http://setosa.io/ev/ordinary-least-squares-regression/&quot;&gt;sacado de este post (en inglés)&lt;/a&gt;, sería un modelo para predecir la altura de una persona a partir del tamaño de su mano. Para ello entrenaríamos el modelo con una muestra aleatoria de personas, para las cuales tendríamos tanto el tamaño de su mano como su altura real. El resultado sería una ecuación del tipo &lt;script type=&quot;math/tex&quot;&gt;y = A + Bx&lt;/script&gt;
donde &lt;em&gt;x&lt;/em&gt; sería el tamaño de la mano, &lt;em&gt;A&lt;/em&gt; y &lt;em&gt;B&lt;/em&gt; valores definidos tras el proceso de aprendizaje, e &lt;em&gt;y&lt;/em&gt; la altura predicha para el valor x.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Ejemplo de regresión lineal&quot; src=&quot;/images/pml/1_regression.png&quot; /&gt;
        &lt;figcaption&gt;Ejemplo de regresión lineal&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;reforzado&quot;&gt;Reforzado&lt;/h3&gt;

&lt;p&gt;El aprendizaje reforzado entrena un modelo para que consiga un objetivo. Para ello se alimenta el modelo con el resultado de una función que evalúa cómo de cerca estamos del objetivo. Se establece un ciclo en el que el modelo genera una salida, la función evalúa cómo de satisfactoria ha sido esta salida, y la evaluación se utiliza para ajustar el modelo.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;El modelo ('Agent') cambia el estado a través de una acción. Este cambio nos acerca más o menos al objetivo. Realimentamos el modelo con una recompensa, así como con la acción que generó dicha recompensa&quot; src=&quot;/images/pml/1_refuerzo.png&quot; /&gt;
        &lt;figcaption&gt;El modelo ('Agent') cambia el estado a través de una acción. Este cambio nos acerca más o menos al objetivo. Realimentamos el modelo con una recompensa, así como con la acción que generó dicha recompensa&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Los ejemplos habituales de aplicación de esta técnica son los juegos: ajedrez, tres en raya, juegos de cartas… En ellos el programa va aprendiendo según juega partidas y comprueba qué acciones llevan a la victoria.&lt;/p&gt;

&lt;h3 id=&quot;no-supervisado&quot;&gt;No supervisado&lt;/h3&gt;

&lt;p&gt;En algunas ocasiones nos encontramos con datos de los que no sabemos qué información extraer. Para estos casos se usan técnicas de aprendizaje no supervisado, en las que se analiza la estructura de los datos para intentar extraer información útil. El libro menciona dos aproximaciones a esta técnica: &lt;strong&gt;agrupación (&lt;em&gt;clustering&lt;/em&gt;)&lt;/strong&gt; y &lt;strong&gt;reducción de dimensionalidad&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;En la agrupación se buscan elementos que, según algún criterio, son algo similares entre sí y algo distintos a los demás. Por ejemplo, si tenemos un conjunto de animales con gatos, caballos, palomas y águilas, y proporcionamos al modelo sus tamaños y el número de patas de cada animal, el aprendizaje no supervisado le permitiría identificar correctamente cuatro tipos de animales: grande con dos patas (águila), grande con cuatro patas (caballo), pequeño con dos patas (paloma) y pequeño con cuatro patas (gato).&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Un ejemplo un poco más real de clustering&quot; src=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/media/machine-learning-algorithm-choice/image9.png&quot; /&gt;
        &lt;figcaption&gt;Un ejemplo un poco más real de clustering&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;En cuanto a la reducción de dimensionalidad, consiste en reducir los datos de forma que se retenga la información relevante. La idea es disminuir tanto el espacio ocupado por los datos como el tiempo necesario para ejecutar los algoritmos. Usando un ejemplo sacado de &lt;a href=&quot;http://stackoverflow.com/a/1994481&quot;&gt;esta respuesta de stackoverflow&lt;/a&gt;, supongamos un conjunto de datos en el que 1000 personas indican si les gusta o no cada una de 100 películas seleccionadas. Podríamos almacenar esta información en 1000 vectores de 100 elementos cada uno, donde cada elemento fuese un 1 o un 0 según la película hubiese gustado o no. Una forma de reducir la dimensionalidad del conjunto de datos sería asignar a cada película uno de cinco géneros (comedia, terror, … los que fuesen más representativos), y usar los datos originales para construir un nuevo vector de cinco elementos para cada persona, donde se indicase si el género es no de su agrado. Pasaríamos así de 100.000 a 5.000 elementos. Como contrapartida, perderíamos información (a una persona le pueden gustar un par de películas de un género que no le gusta).&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;El ejemplo del libro reduce una muestra de datos en 3 dimensionaes a otra equivalente en 2 dimensiones&quot; src=&quot;/images/pml/1_compression.png&quot; /&gt;
        &lt;figcaption&gt;El ejemplo del libro reduce una muestra de datos en 3 dimensiones a otra equivalente en 2 dimensiones&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;terminología-y-notación&quot;&gt;Terminología y notación&lt;/h2&gt;

&lt;p&gt;Como es de esperar en cualquier disciplina técnica que se precie, el &lt;em&gt;machine learning&lt;/em&gt; incluye un montón de terminología propia que es preciso conocer. El siguiente punto del tema 1 presenta algunos de estos términos.&lt;/p&gt;

&lt;p&gt;En aprendizaje automático existe un conjunto de datos muy conocido que se usa en todos los cursos para principiantes, algo así como el “Hola Mundo” de los conjuntos de datos. Se llama &lt;a href=&quot;https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos&quot;&gt;conjunto de datos Iris&lt;/a&gt;, e incluye datos de 50 ejemplares de cada una de las 3 especies de la flor Iris. Para cada ejemplar se da el largo y ancho del pétalo y el sépalo, así como la especie a la que pertenece (&lt;em&gt;setosa&lt;/em&gt;, &lt;em&gt;versicolor&lt;/em&gt; o &lt;em&gt;virginica&lt;/em&gt;).&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Algunos datos de Iris y una ilustración de la flor&quot; src=&quot;/images/pml/1_iris.png&quot; /&gt;
        &lt;figcaption&gt;Algunos datos de Iris y una ilustración de la flor&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Como se puede apreciar en la imagen superior, cada fila de datos es una &lt;em&gt;muestra&lt;/em&gt;, &lt;em&gt;instancia&lt;/em&gt; u &lt;em&gt;observación&lt;/em&gt;. Para cada una de estas muestras se proporciona una serie de datos, a los que nos solemos referir como &lt;em&gt;características&lt;/em&gt;, aunque también &lt;em&gt;atributos&lt;/em&gt;, &lt;em&gt;medidas&lt;/em&gt; o &lt;em&gt;dimensiones&lt;/em&gt;. Finalmente, la especie de la flor sería la &lt;em&gt;etiqueta&lt;/em&gt; u &lt;em&gt;objetivo&lt;/em&gt;. La consistencia en la terminología no es un fuerte del &lt;em&gt;machine learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;En este punto el libro también nos habla de notación algebraica. El trabajo con matrices es fundamental en el &lt;em&gt;machine learning&lt;/em&gt;, así que este el autor nos cuenta qué notación utiliza para representar matices, vectores y elementos dentro de los mismos.&lt;/p&gt;

&lt;p&gt;Por ejemplo, el conjunto de datos Iris se puede representar como una matriz de 150 filas (una por muestra) y 4 columnas (una por característica):&lt;/p&gt;

&lt;p&gt;\begin{bmatrix}
x_1^{(1)} &amp;amp; x_2^{(1)} &amp;amp; x_3^{(1)} &amp;amp; x_4^{(1)} \cr
x_1^{(2)} &amp;amp; x_2^{(2)} &amp;amp; x_3^{(2)} &amp;amp; x_4^{(2)} \cr
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \cr
x_1^{(150)} &amp;amp; x_2^{(150)} &amp;amp; x_3^{(150)} &amp;amp; x_4^{(150)} \cr
\end{bmatrix}&lt;/p&gt;

&lt;p&gt;En general, un elemento &lt;script type=&quot;math/tex&quot;&gt;x_n^{(m)}&lt;/script&gt; se refiere al elemento de la fila m y la columna n de una matriz. Usaremos negritas en minúsculas para los vectores (&lt;script type=&quot;math/tex&quot;&gt;\bf x \in \mathbb R^{nx1}&lt;/script&gt;), y negritas en mayúsculas para las matrices (&lt;script type=&quot;math/tex&quot;&gt;\bf X \in \mathbb R^{nxm}&lt;/script&gt;).&lt;/p&gt;

&lt;h2 id=&quot;otros-aspectos-importantes-del-aprendizaje-automático&quot;&gt;Otros aspectos importantes del aprendizaje automático&lt;/h2&gt;

&lt;p&gt;Además del algoritmo de aprendizaje, el libro nos habla en este tema de otros aspectos importantes del &lt;em&gt;machine learning&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;preprocesado&quot;&gt;Preprocesado&lt;/h3&gt;

&lt;p&gt;El preprocesado es la fase inicial del proceso de aprendizaje. Consiste en disponer los datos existentes en un formato adecuado para su procesamiento: se normalizan valores, se pasan textos a números, se separan datos de entrenamiento y datos de prueba,…&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Ejemplo de flujo de modelado predictivo&quot; src=&quot;/images/pml/1_flujo.png&quot; /&gt;
        &lt;figcaption&gt;Ejemplo de flujo de modelado predictivo&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;entrenamiento-y-selección-del-proceso-predictivo&quot;&gt;Entrenamiento y selección del proceso predictivo&lt;/h3&gt;

&lt;p&gt;La otra tarea fundamental a realizar al empezar es la correcta selección del algoritmo a utilizar. Queremos obtener el modelo que presente un mejor rendimiento, pero para ello necesitamos definir una métrica que mida este rendimiento. La más común es la proporción de muestras correctamente clasificadas.&lt;/p&gt;

&lt;h3 id=&quot;evaluación-de-modelos-y-predcción-de-instancias-de-datos-no-vistas&quot;&gt;Evaluación de modelos y predcción de instancias de datos no vistas&lt;/h3&gt;

&lt;p&gt;El modelo se entrena con los datos de entrenamiento, y se evalúa con la parte del conjunto de datos reservada para ello. En función de la tasa de error encontrada en la evaluación decidiremos si el modelo es válido.&lt;/p&gt;

&lt;h2 id=&quot;python&quot;&gt;Python&lt;/h2&gt;

&lt;p&gt;El tema finaliza hablando de Python y de cómo instalarlo. Python es uno de los lenguajes más populares en el mundo del &lt;em&gt;machine learning&lt;/em&gt;, en parte por su versatilidad, en parte por la gran cantidad de librerías especialidadas de las que dispone. Especialmente relevantes para el aprendizaje máquina son &lt;a href=&quot;http://www.numpy.org/&quot;&gt;NumPy&lt;/a&gt; y &lt;a href=&quot;https://www.scipy.org/&quot;&gt;SciPy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;La opción más cómoda para instalar Python con todo lo necesario para este libro es descargar &lt;a href=&quot;https://docs.continuum.io/anaconda/install#&quot;&gt;Anaconda&lt;/a&gt;, un gestor de instalaciones de Python.&lt;/p&gt;

&lt;p&gt;El libro no habla del entorno de desarrollo, pero yo uso desde hace unos meses el &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Visual Studio Code&lt;/a&gt;. Con la &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=donjayamanne.python&quot;&gt;extensión de Python&lt;/a&gt; proporcionando autocompletado, &lt;a href=&quot;https://es.wikipedia.org/wiki/Lint&quot;&gt;&lt;em&gt;linting&lt;/em&gt;&lt;/a&gt;, formateado de código y depuración, el Visual Studio Code supera al resto de entornos Python disponibles de forma gratuita.&lt;/p&gt;

&lt;h2 id=&quot;conclusión&quot;&gt;Conclusión&lt;/h2&gt;

&lt;p&gt;El tema 1 nos familiariza con los conceptos más básicos del aprendizaje máquina. En el tema 2 empezaremos ya a ver código para avanzar en la materia.&lt;/p&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">El foco de estas entradas se pondrá en el código Python publicado en GitHub, pero el primer tema no incluye código, tan sólo presenta una serie de conceptos que resumo en esta entrada.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/pml/pml.jpg" /></entry><entry><title type="html">Ready Player One</title><link href="https://www.javiercancela.com/2017/03/05/ready-player-one/" rel="alternate" type="text/html" title="Ready Player One" /><published>2017-03-05T01:00:00+00:00</published><updated>2017-03-05T01:00:00+00:00</updated><id>https://www.javiercancela.com/2017/03/05/ready-player-one</id><content type="html" xml:base="https://www.javiercancela.com/2017/03/05/ready-player-one/">&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.artstation.com/artwork/bg49G&quot;&gt;Imagen de la cabecera&lt;/a&gt; de &lt;a href=&quot;https://www.artstation.com/artist/degesart&quot;&gt;Florian de Gesincourt&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.es/Ready-player-one-Grandes-novelas/dp/8466649174/&quot;&gt;&lt;em&gt;Ready Player One&lt;/em&gt;, de Ernest Cline&lt;/a&gt; es un homenaje a la cultura pop de los años 80 en su vertiente más friki: máquinas recreativas, videoconsolas, juegos de rol, y novelas, comics, películas de componente más o menos fantástico. Conmigo la fórmula funcionó muy bien, y lo devoré como se devora una hamburguesa un viernes a las 7 de la mañana antes de irte a dormir.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img style=&quot;width:400px&quot; alt=&quot;Cuando devoré mi última hamburguesa a las 7 de la mañana aun había maquinitas de Invaders en los bares&quot; src=&quot;https://images-na.ssl-images-amazon.com/images/I/71teOCvL2TL.jpg&quot; /&gt;  
        &lt;figcaption&gt;Cuando devoré mi última hamburguesa a las 7 de la mañana aun había maquinitas de Invaders en los bares&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Estamos en el año 2044. Wade Watts es una adolescente huérfano que vive con una tía que le odia en un asentamiento donde las caravanas se apilan a varias alturas, y cuya única ilusión en la vida es evadirse a través de OASIS, un híbrido de internet y MMORPG que sirve lo mismo para jugar al rol que para ir a clase. Ernest Cline aprovecha este entorno para sumergirnos en una fantasía casi sin límites, a partir de un concurso que se convierte en una aventura repleta de guiños nostálgicos al pasado.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img style=&quot;width:400px&quot; alt=&quot;Aquí las caravanas apiladas&quot; src=&quot;http://t2.gstatic.com/images?q=tbn:ANd9GcQWpxNZupWTxKEoVaD-U0C_wC7cIkfUFtRRj12M4an8tl1rwj9p&quot; /&gt;  
        &lt;figcaption&gt;Aquí las caravanas apiladas&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;El libro está bien escrito. Tiene ritmo, sobre todo al principio, y los personajes son estereotipos bien desarrollados, al menos lo suficiente como para que te encariñes con ellos. El libro está lleno de tópicos, con malos muy malos, buenos muy buenos, chico, chica, amigo del chico, figura paterna,… pero todo encaja en el juego que propone el autor.&lt;/p&gt;

&lt;p&gt;¿Cómo reciben el libro los que no entren en este juego? Con cierta dificultad, por lo que se lee en las críticas. Los más jóvenes, y los que pasaron por los 80 haciendo una vida normal (o siendo parte de la Movida, o ligando, o vete a saber haciendo qué) puede que no le encuentren interés a un libro que rinde culto a un mundo que nunca fue el suyo.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;Perspectiva isométrica, robots japoneses, llaves y castillos. Todo esto y mucho más en Ready Player One&quot; src=&quot;/images/rpo.jpg&quot; /&gt;  
        &lt;figcaption&gt;Perspectiva isométrica, robots japoneses, llaves y castillos. Todo esto y mucho más en Ready Player One&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Tras leer el libro me encontré con que &lt;a href=&quot;https://es.wikipedia.org/wiki/Ready_Player_One_(pel%C3%ADcula)&quot;&gt;Steven Spielberg está rodando una versión para el cine&lt;/a&gt;. Imagino que la idea es apuntar al &lt;em&gt;target&lt;/em&gt; natural del libro (cuarentones nostálgicos) y añadir los cambios necesarios para asegurarse el interés del público joven. Veremos cómo resulta.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
    &lt;figure&gt;
        &lt;img alt=&quot;El chico y la chica&quot; src=&quot;https://images-na.ssl-images-amazon.com/images/M/MV5BMTY1NDA5MTY0M15BMl5BanBnXkFtZTgwMzA3MDExOTE@._V1_UY317_CR14,0,214,317_AL_.jpg&quot; /&gt;  
        &lt;img alt=&quot;El chico y la chica&quot; src=&quot;https://s-media-cache-ak0.pinimg.com/236x/38/14/30/3814303a211c6f94afbd4d977e0ff803.jpg&quot; /&gt;  
        &lt;figcaption&gt;El chico y la chica, más guapos en la película que en el libro, as usual&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;</content><author><name>Javier Cancela</name><email>javier.cancela.vicente@gmail.com</email></author><summary type="html">Imagen de la cabecera de Florian de Gesincourt</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.javiercancela.com/images/florian-de-gesincourt-degesart-sk0010-playerone-j-deges1500.jpg" /></entry></feed>
