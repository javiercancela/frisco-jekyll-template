--- layout: page ---

<section>
    <div class="blog-post text-container">
        <p class="post-details">
	
		
		<span class="blog-filter">
			<a href="/category/python/">Python</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/libros/">Libros</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/machine-learning/">Machine Learning</a>
		</span>
	
		
		<span class="blog-filter">
			<a href="/category/inteligencia-artificial/">Inteligencia Artificial</a>
		</span>
	
	
	
	
	
	
	
	<span class="post-date">09 de abril de 2017</span>
</p>


        <div class="post-content">
            <p>El siguiente punto del libro nos habla de cómo modificar este algoritmo para adaptarlo a conjuntos con millones de datos. El punto clave está en que la forma del algoritmo que hemos analizado reevalúa todo el conjunto de datos cada vez que se realiza una iteración o época. Si tenemos 1 millón de muestras, por cada época vamos a tener que calcular:
\(\Delta \mathbf{w} = \eta \sum_i \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}\)</p>

<p>Es decir, para cada época tenemos que realizar un millón de veces:</p>

<ul>
  <li>el cálculo de la función de costos</li>
  <li>el cálculo de la resta con la clase real</li>
  <li>la multiplicación del resultado por la entrada de la muestra</li>
</ul>

<p>y después sumar el millón de resultados.</p>

<h3 id="optimizando-el-proceso">Optimizando el proceso</h3>

<p>El gradiente descendente estocástico utiliza otra aproximación. En vez de recalcular los pesos usando todas las muestras, utilizamos sólo la muestra de la entrada para el cálculo:
\(\Delta \mathbf{w} = \eta \big(y^{(i)} - \phi(z)_{A}^{(i)}\big) x^{(i)}\)</p>

<p>Este cambio tiene dos consecuencias fundamentales Por un lado, los pesos se actualizan con más frecuencia, así que la convergencia suele ser más rápida. Por otro lado, usar una sola muestra da menos precisión a la actualización, así que se introduce más ruido en los pesos. Esto no es necesariamente malo, ya que facilita salir de mínimos locales que no se corresponden con la minimización de nuestra función de costos. Otra ventaja de este algoritmo es que resulta adecuado para el aprendizaje <em>on line</em>, es decir, aquel en el que el modelo tiene que adaptarse rápidamente a cambios en los datos.</p>

<p>Una variante mencionada en el libro pero no implementada en el código consiste en sustituir una tasa de aprendizaje fija \(\eta\) por una que disminuya con el tiempo:</p>

\[\frac{c_1}{[\textit {número de iteraciones}] + c_2}\]

<h3 id="código-python">Código Python</h3>

<p>Vamos a adaptar la clase Adaline anterior a este nuevo algoritmo:</p>

<pre class="line-numbers">
  <code class="language-python">
  from numpy.random import seed

  class AdalineSGD(object):
      def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
          self.eta = eta
          self.n_iter = n_iter
          self.w_initialized = False
          self.shuffle = shuffle
          if random_state:
              seed(random_state)
          
      def fit(self, X, y):
          self._initialize_weights(X.shape[1])
          self.cost_ = []
          for i in range(self.n_iter):
              if self.shuffle:
                  X, y = self._shuffle(X, y)
              cost = []
              for xi, target in zip(X, y):
                  cost.append(self._update_weights(xi, target))
              avg_cost = sum(cost) / len(y)
              self.cost_.append(avg_cost)
          return self

      def partial_fit(self, X, y):
          if not self.w_initialized:
              self._initialize_weights(X.shape[1])
          if y.ravel().shape[0] &gt; 1:
              for xi, target in zip(X, y):
                  self._update_weights(xi, target)
          else:
              self._update_weights(X, y)
          return self

      def _shuffle(self, X, y):
          r = np.random.permutation(len(y))
          return X[r], y[r]
      
      def _initialize_weights(self, m):
          self.w_ = np.zeros(1 + m)
          self.w_initialized = True
          
      def _update_weights(self, xi, target):
          output = self.net_input(xi)
          error = (target - output)
          self.w_[1:] += self.eta * xi.dot(error)
          self.w_[0] += self.eta * error
          cost = 0.5 * error**2
          return cost
      
      def net_input(self, X):
          return np.dot(X, self.w_[1:]) + self.w_[0]

      def activation(self, X):
          return self.net_input(X)

      def predict(self, X):
          return np.where(self.activation(X) &gt;= 0.0, 1, -1)
  </code>
</pre>

<p>En primer lugar, hemos añadido un método <code class="language-plaintext highlighter-rouge">_shuffle</code> para mezclar los datos de la entrada antes de cada iteración. El propósito de este paso es evitar ciclos al minimizar la función de costos, ya que este algoritmo es sensible a este problema.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">[</span><span class="n">r</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">r</span><span class="p">]</span>
</code></pre></div></div>

<p>El constructor incluye parámetros para indicar si queremos mezclar al azar los datos de entrada y para inicializar una semilla para esta mezcla.</p>

<p>El método <code class="language-plaintext highlighter-rouge">fit</code> es el que se encarga del entrenamiento. Invocamos primero al método que inicializa los pesos a cero. Este método existe para marcar los pesos como inicializados, información que necesitamos para el método de entrenamiento parcial <code class="language-plaintext highlighter-rouge">parcial_fit</code>.</p>

<p>Dentro del método <code class="language-plaintext highlighter-rouge">fit</code> el código que se itera en cada época comienza comprobando si hay que barajar los datos de entrada, e inicializa el costo que evaluará la función de costos:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span><span class="p">:</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_shuffle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>A continuación iteramos los datos. La variable <code class="language-plaintext highlighter-rouge">X</code> es una matriz que contiene una fila por cada muestra, y una columna por cada característica. La variable <code class="language-plaintext highlighter-rouge">y</code> es un array que contiene un elemento, la clase real, para cada muestra de <code class="language-plaintext highlighter-rouge">X</code>. El método <code class="language-plaintext highlighter-rouge">zip</code> nos devolverá un elemento de cada variable (<em>zip</em> en inglés significa, entre otras cosas, cremallera), por lo que en cada iteración del <code class="language-plaintext highlighter-rouge">for</code> en <code class="language-plaintext highlighter-rouge">xi</code> y en <code class="language-plaintext highlighter-rouge">target</code> se almacenarán las características y la clase real de cada muestra.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">cost</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_update_weights</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>

</code></pre></div></div>

<p>Para cada muestra, actualizamos los pesos en el método <code class="language-plaintext highlighter-rouge">_update_weights</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">net_input</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">xi</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">error</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">error</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">cost</span>

</code></pre></div></div>

<p>Para la salida real seguimos utilizando la función identidad, con lo que la <code class="language-plaintext highlighter-rouge">output</code> es igual a la entrada. La diferencia en este nuevo algoritmo está en cómo ajustamos los pesos. El array de pesos se ajusta con el producto del error con la muestra procesada, no con toda la matriz de entradas. Lógicamente, el costo calculado se obtendrá del error de esta muestra al cuadrado. El costo lo añadimos al array de costos para representarlo posteriormente.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">avg_cost</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">cost_</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_cost</span><span class="p">)</span>
</code></pre></div></div>

<p>Para finalizar el método <code class="language-plaintext highlighter-rouge">fit</code> calculamos la media de los costos de cada muestra, y la añadimos al array de costos.</p>

<h3 id="resultados">Resultados</h3>

<p>Ahora toca usar esta clase. La instanciamos y entrenamos el modelo (el libro usa, en este ejemplo y en el de la entrada anterior, un conjunto de datos escalado para mejorar el rendimiento, y que nosotros revisaremos en el tema 3):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ada</span> <span class="o">=</span> <span class="n">AdalineSGD</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ada</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Pintamos los resultados, primero mostrando la clasificación de las muestras y después el costo por época.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">ada</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Adaline - Gradiente descendiente estocástico'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'longitud sépalo'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'longitud pétalo'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ada</span><span class="p">.</span><span class="n">cost_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ada</span><span class="p">.</span><span class="n">cost_</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Épocas'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Costo medio'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>Los datos obtenidos son estos (los resultados son distintos a los obtenidos en el libro, ya que nosotros no hemos estandarizado los valores de las muestras):</p>

<div style="text-align:center">
    <figure>
        <img alt="Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados" src="/images/pml/2_adaline.png" />
        <a href="https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch02/images/02_14.png"><img alt="Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados" src="https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/ch02/images/02_14.png" /></a>
        <figcaption>Arriba, nuestros resultados. Abajo, los resultados del libro con los valores estandarizados</figcaption>
    </figure>
</div>

<p>Nuestra convergencia es mucha más rápida, pero también muy variable, ya que a partir de la sexta época el costo empieza a oscilar.</p>


            <div class="blog-navigation">
                
                <a class="prev" href="/2017/04/02/python-machine-learning-iii/">&laquo; PML T2 - ADALINE</a>  
                <a class="next" href="/2017/04/16/pausa-por-vacaciones/">Pausa por vacaciones &raquo;</a> 
            </div>
        </div>
    </div>
</section>